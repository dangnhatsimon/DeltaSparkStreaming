{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02cf1f22-d484-4897-a5af-0a6e7df6aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split, trim, lower\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "import logging\n",
    "from os.path import abspath\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2496d8f7-17c5-4ace-a768-6dc32c4067f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%y-%m-%d %H:%M:%S\",\n",
    "    level=logging.DEBUG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a8f4fb7-5957-47e8-89f6-38a79d7708f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWordCount():\n",
    "    def __init__(\n",
    "        self,\n",
    "        spark: SparkSession\n",
    "    ):\n",
    "        self.spark = spark\n",
    "        logging.info(\"Initiated SparkSession.\")\n",
    "\n",
    "    def read_text(\n",
    "        self,\n",
    "        path: str,\n",
    "        format: str = \"text\",\n",
    "        line_sep: str = \".\"\n",
    "    ) -> DataFrame:\n",
    "        lines = (\n",
    "            self.spark.read\n",
    "            .format(format)\n",
    "            .option(\"lineSep\", line_sep)\n",
    "            .load(path)\n",
    "        )\n",
    "        raw_sdf = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "        logging.info(f\"Read Text Files as DataFrame, {raw_sdf.count()} rows, {len(raw_sdf.columns)} columns, schema: {raw_sdf.schema}\")\n",
    "        return raw_sdf\n",
    "\n",
    "    def process_text(\n",
    "        self,\n",
    "        raw_sdf: DataFrame\n",
    "    ) -> DataFrame:\n",
    "        processed_sdf = (\n",
    "            raw_sdf.select(lower(trim(raw_sdf.word)).alias(\"word\"))\n",
    "            .where(\"word is not null\")\n",
    "            .where(\"word rlike '[a-z]'\")\n",
    "        )\n",
    "        logging.info(f\"Processed DataFrame, {processed_sdf.count()} rows, {len(processed_sdf.columns)} columns, schema: {processed_sdf.schema}\")\n",
    "        return processed_sdf\n",
    "\n",
    "    def count_words(\n",
    "        self,\n",
    "        processed_sdf: DataFrame\n",
    "    ) -> DataFrame:\n",
    "        sdf = processed_sdf.groupBy(\"word\").count()\n",
    "        logging.info(f\"Grouped DataFrame by word, {sdf.count()} rows, {len(sdf.columns)} columns, schema: {sdf.schema}\")\n",
    "        return sdf\n",
    "\n",
    "    def write_table(\n",
    "        self,\n",
    "        sdf: DataFrame,\n",
    "        format: str,\n",
    "        mode: str,\n",
    "        table_name: str\n",
    "    ):\n",
    "        (\n",
    "            sdf.write\n",
    "            .format(format)\n",
    "            .mode(mode)\n",
    "            .saveAsTable(table_name)\n",
    "        )\n",
    "        logging.info(f\"Wrote {sdf.count()} rows, {len(sdf.columns)} columns, schema: {sdf.schema}  to table: {table_name}, format: {format}, mode: {mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89bbc940-cad8-4a66-99c1-e66c0c89ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamWordCount():\n",
    "    def __init__(\n",
    "        self,\n",
    "        spark: SparkSession\n",
    "    ):\n",
    "        self.spark = spark\n",
    "\n",
    "    def read_text(\n",
    "        self,\n",
    "        path: str,\n",
    "        format: str = \"text\",\n",
    "        line_sep: str = \".\"\n",
    "    ):\n",
    "        lines = (\n",
    "            self.spark.readStream\n",
    "            .format(format)\n",
    "            .option(\"lineSep\", line_sep)\n",
    "            .load(path)\n",
    "        )\n",
    "        raw_sdf = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "        logging.info(f\"Read Text Files Streaming as DataFrame, {raw_sdf.count()} rows, {len(raw_sdf.columns)} columns, schema: {raw_sdf.schema}\")\n",
    "        return raw_sdf\n",
    "\n",
    "    def process_text(\n",
    "        self,\n",
    "        raw_sdf: DataFrame\n",
    "    ) -> DataFrame:\n",
    "        processed_sdf = (\n",
    "            raw_sdf.select(lower(trim(raw_sdf.word)).alias(\"word\"))\n",
    "            .where(\"word is not null\")\n",
    "            .where(\"word rlike '[a-z]'\")\n",
    "        )\n",
    "        logging.info(f\"Processed Streaming DataFrame, {processed_sdf.count()} rows, {len(processed_sdf.columns)} columns, schema: {processed_sdf.schema}\")\n",
    "        return processed_sdf\n",
    "\n",
    "    def count_words(\n",
    "        self,\n",
    "        processed_sdf: DataFrame\n",
    "    ) -> DataFrame:\n",
    "        sdf = processed_sdf.groupBy(\"word\").count()\n",
    "        logging.info(f\"Grouped Streaming DataFrame by word, {sdf.count()} rows, {len(sdf.columns)} columns, schema: {sdf.schema}\")\n",
    "        return sdf\n",
    "\n",
    "    def write_table(\n",
    "        self,\n",
    "        sdf: DataFrame,\n",
    "        format: str,\n",
    "        output_mode: str,\n",
    "        table_name: str,\n",
    "        checkpoint_location: str\n",
    "    ):\n",
    "        squery = (\n",
    "            sdf.writeStream\n",
    "            .format(format)\n",
    "            .option(\"truncate\", value=False)\n",
    "            .option(\"checkpointLocation\", checkpoint_location)\n",
    "            .outputMode(output_mode)\n",
    "            # .toTable(table_name)\n",
    "            .start()\n",
    "            .awaitTermination()\n",
    "        )\n",
    "        logging.info(f\"Wrote streaming {sdf.count()} rows, {len(sdf.columns)} columns, schema: {sdf.schema}  to table: {table_name}, format: {format}, outputMode: {output_mode}\")\n",
    "        return squery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "910c4581-6783-4e11-aedb-82c94ccb0159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-04-08 08:24:16 - INFO - spark-warehouse: /home/jovyan/work/spark-warehouse\n"
     ]
    }
   ],
   "source": [
    "table_name = \"word_count_table\"\n",
    "warehouse_location = abspath(\"spark-warehouse\")\n",
    "logging.info(f\"spark-warehouse: {warehouse_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "132df697-58c7-428c-82eb-caff60d9ed50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-04-08 08:27:10 - DEBUG - GatewayClient.address is deprecated and will be removed in version 1.0. Use GatewayParameters instead.\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: A\n",
      "ec90b042d81623381b8ac78c0fdbec1de70505406db0d49a0ed7902b1f33ddc2\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.SparkConf\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.api.java.*\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.api.python.*\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.ml.python.*\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.mllib.api.python.*\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.resource.*\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.sql.*\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.sql.api.python.*\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.sql.hive.*\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: j\n",
      "i\n",
      "rj\n",
      "scala.Tuple2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: r\n",
      "u\n",
      "SparkConf\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ycorg.apache.spark.SparkConf\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: i\n",
      "org.apache.spark.SparkConf\n",
      "bTrue\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro0\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.app.name\n",
      "sstreaming_word_count\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro1\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.sql.warehouse.dir\n",
      "s/home/jovyan/work/spark-warehouse\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro2\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.sql.extensions\n",
      "sio.delta.sql.DeltaSparkSessionExtension\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro3\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.sql.catalog.spark_catalog\n",
      "sorg.apache.spark.sql.delta.catalog.DeltaCatalog\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro4\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.jars.packages\n",
      "sio.delta:delta-spark_2.13:3.3.1,org.apache.spark:spark-sql_2.12:3.5.3\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro5\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.driver.cores\n",
      "s2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro6\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.driver.memory\n",
      "s2g\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro7\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.executor.memory\n",
      "s1g\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro8\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.submit.deployMode\n",
      "sclient\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro9\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.log.level\n",
      "sALL\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro10\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.sql.catalogImplementation\n",
      "shive\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro11\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "contains\n",
      "sspark.serializer.objectStreamReset\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ybfalse\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.serializer.objectStreamReset\n",
      "s100\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro12\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "contains\n",
      "sspark.rdd.compress\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ybfalse\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.rdd.compress\n",
      "sTrue\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro13\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "contains\n",
      "sspark.master\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ybtrue\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "contains\n",
      "sspark.app.name\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ybtrue\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "contains\n",
      "sspark.master\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ybtrue\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "get\n",
      "sspark.master\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark://spark-master:7077\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "contains\n",
      "sspark.app.name\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ybtrue\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "get\n",
      "sspark.app.name\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysstreaming_word_count\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "contains\n",
      "sspark.home\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ybfalse\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o0\n",
      "getAll\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yto14\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i0\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro15\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o15\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.eventLog.enabled\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o15\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ystrue\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro16\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o16\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.submit.pyFiles\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o16\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ys/home/jovyan/.ivy2/jars/io.delta_delta-spark_2.13-3.3.1.jar,/home/jovyan/.ivy2/jars/io.delta_delta-storage-3.3.1.jar,/home/jovyan/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro17\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o17\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.eventLog.dir\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o17\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ys/opt/bitnami/spark/spark-events\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i3\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro18\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o18\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.driver.cores\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o18\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ys2\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i4\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro19\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o19\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.app.submitTime\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o19\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ys1744100830072\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i5\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro20\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o20\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.log.level\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o20\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysALL\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i6\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro21\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o21\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.files\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o21\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysfile:///home/jovyan/.ivy2/jars/io.delta_delta-spark_2.13-3.3.1.jar,file:///home/jovyan/.ivy2/jars/io.delta_delta-storage-3.3.1.jar,file:///home/jovyan/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i7\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro22\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o22\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.jars\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o22\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysfile:///home/jovyan/.ivy2/jars/io.delta_delta-spark_2.13-3.3.1.jar,file:///home/jovyan/.ivy2/jars/io.delta_delta-storage-3.3.1.jar,file:///home/jovyan/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i8\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o23\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.sql.catalogImplementation\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o23\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yshive\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i9\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro24\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o24\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.sql.extensions\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o24\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysio.delta.sql.DeltaSparkSessionExtension\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i10\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro25\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o25\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.executor.memory\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o25\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ys1g\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i11\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro26\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o26\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.rdd.compress\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o26\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysTrue\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i12\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro27\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o27\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.history.fs.logDirectory\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o27\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ys/opt/bitnami/spark/spark-events\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i13\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro28\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o28\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.master\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o28\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark://spark-master:7077\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro29\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o29\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.driver.memory\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o29\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ys2g\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i15\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro30\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o30\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.serializer.objectStreamReset\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o30\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ys100\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i16\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro31\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o31\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.sql.warehouse.dir\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o31\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ys/home/jovyan/work/spark-warehouse\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i17\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro32\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o32\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.app.name\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o32\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysstreaming_word_count\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i18\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro33\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o33\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.submit.deployMode\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o33\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysclient\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i19\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro34\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o34\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.ui.showConsoleProgress\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o34\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ystrue\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i20\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro35\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o35\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.jars.packages\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o35\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysio.delta:delta-spark_2.13:3.3.1,org.apache.spark:spark-sql_2.12:3.5.3\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i21\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro36\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o36\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.repl.local.jars\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o36\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysfile:///home/jovyan/.ivy2/jars/io.delta_delta-spark_2.13-3.3.1.jar,file:///home/jovyan/.ivy2/jars/io.delta_delta-storage-3.3.1.jar,file:///home/jovyan/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "g\n",
      "o14\n",
      "i22\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yro37\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o37\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysspark.sql.catalog.spark_catalog\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: c\n",
      "o37\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ysorg.apache.spark.sql.delta.catalog.DeltaCatalog\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: a\n",
      "e\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !yi23\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: r\n",
      "u\n",
      "JavaSparkContext\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-08 08:27:10 - DEBUG - Answer received: !ycorg.apache.spark.api.java.JavaSparkContext\n",
      "25-04-08 08:27:10 - DEBUG - Command to send: i\n",
      "org.apache.spark.api.java.JavaSparkContext\n",
      "ro0\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: A\n",
      "ec90b042d81623381b8ac78c0fdbec1de70505406db0d49a0ed7902b1f33ddc2\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: m\n",
      "d\n",
      "o1\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: m\n",
      "d\n",
      "o2\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: m\n",
      "d\n",
      "o3\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: m\n",
      "d\n",
      "o4\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: m\n",
      "d\n",
      "o5\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: m\n",
      "d\n",
      "o6\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: m\n",
      "d\n",
      "o7\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: m\n",
      "d\n",
      "o8\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: m\n",
      "d\n",
      "o9\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: m\n",
      "d\n",
      "o10\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: m\n",
      "d\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: m\n",
      "d\n",
      "o12\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: m\n",
      "d\n",
      "o13\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: m\n",
      "d\n",
      "o15\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: m\n",
      "d\n",
      "o16\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: m\n",
      "d\n",
      "o17\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: m\n",
      "d\n",
      "o18\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: m\n",
      "d\n",
      "o19\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: m\n",
      "d\n",
      "o20\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: m\n",
      "d\n",
      "o21\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: m\n",
      "d\n",
      "o22\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:11 - DEBUG - Command to send: m\n",
      "d\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-08 08:27:11 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:13 - DEBUG - Answer received: !xro38\n",
      "25-04-08 08:27:14 - DEBUG - Command to send: p\n",
      "ro38\n",
      "e\n",
      "\n",
      "25-04-08 08:27:14 - DEBUG - Answer received: !ysjava.io.FileNotFoundException: File file:/opt/bitnami/spark/spark-events does not exist\\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\\n\tat org.apache.spark.deploy.history.EventLogFileWriter.requireLogBaseDirAsDirectory(EventLogFileWriters.scala:77)\\n\tat org.apache.spark.deploy.history.SingleEventLogFileWriter.start(EventLogFileWriters.scala:221)\\n\tat org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:81)\\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:632)\\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\tat py4j.Gateway.invoke(Gateway.java:238)\\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\\n\tat java.base/java.lang.Thread.run(Thread.java:833)\\n\n",
      "25-04-08 08:27:14 - DEBUG - Command to send: p\n",
      "ro38\n",
      "e\n",
      "\n",
      "25-04-08 08:27:14 - DEBUG - Answer received: !ysjava.io.FileNotFoundException: File file:/opt/bitnami/spark/spark-events does not exist\\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\\n\tat org.apache.spark.deploy.history.EventLogFileWriter.requireLogBaseDirAsDirectory(EventLogFileWriters.scala:77)\\n\tat org.apache.spark.deploy.history.SingleEventLogFileWriter.start(EventLogFileWriters.scala:221)\\n\tat org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:81)\\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:632)\\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\tat py4j.Gateway.invoke(Gateway.java:238)\\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\\n\tat java.base/java.lang.Thread.run(Thread.java:833)\\n\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.io.FileNotFoundException: File file:/opt/bitnami/spark/spark-events does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.spark.deploy.history.EventLogFileWriter.requireLogBaseDirAsDirectory(EventLogFileWriters.scala:77)\n\tat org.apache.spark.deploy.history.SingleEventLogFileWriter.start(EventLogFileWriters.scala:221)\n\tat org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:81)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:632)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      2\u001b[0m     \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstreaming_word_count\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.warehouse.dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarehouse_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.extensions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mio.delta.sql.DeltaSparkSessionExtension\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.spark_catalog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.sql.delta.catalog.DeltaCatalog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.jars.packages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mio.delta:delta-spark_2.13:3.3.1,org.apache.spark:spark-sql_2.12:3.5.3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.cores\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.submit.deployMode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclient\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.log.level\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mALL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# .config(conf=conf)\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menableHiveSupport\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.io.FileNotFoundException: File file:/opt/bitnami/spark/spark-events does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.spark.deploy.history.EventLogFileWriter.requireLogBaseDirAsDirectory(EventLogFileWriters.scala:77)\n\tat org.apache.spark.deploy.history.SingleEventLogFileWriter.start(EventLogFileWriters.scala:221)\n\tat org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:81)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:632)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-04-08 08:27:14 - DEBUG - Command to send: p\n",
      "ro38\n",
      "e\n",
      "\n",
      "25-04-08 08:27:14 - DEBUG - Answer received: !ysjava.io.FileNotFoundException: File file:/opt/bitnami/spark/spark-events does not exist\\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\\n\tat org.apache.spark.deploy.history.EventLogFileWriter.requireLogBaseDirAsDirectory(EventLogFileWriters.scala:77)\\n\tat org.apache.spark.deploy.history.SingleEventLogFileWriter.start(EventLogFileWriters.scala:221)\\n\tat org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:81)\\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:632)\\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\tat py4j.Gateway.invoke(Gateway.java:238)\\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\\n\tat java.base/java.lang.Thread.run(Thread.java:833)\\n\n",
      "25-04-08 08:27:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o24\n",
      "e\n",
      "\n",
      "25-04-08 08:27:14 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o25\n",
      "e\n",
      "\n",
      "25-04-08 08:27:14 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o26\n",
      "e\n",
      "\n",
      "25-04-08 08:27:14 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o27\n",
      "e\n",
      "\n",
      "25-04-08 08:27:14 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o28\n",
      "e\n",
      "\n",
      "25-04-08 08:27:14 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o29\n",
      "e\n",
      "\n",
      "25-04-08 08:27:14 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o30\n",
      "e\n",
      "\n",
      "25-04-08 08:27:14 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o31\n",
      "e\n",
      "\n",
      "25-04-08 08:27:14 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o32\n",
      "e\n",
      "\n",
      "25-04-08 08:27:14 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o33\n",
      "e\n",
      "\n",
      "25-04-08 08:27:14 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o34\n",
      "e\n",
      "\n",
      "25-04-08 08:27:14 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o35\n",
      "e\n",
      "\n",
      "25-04-08 08:27:14 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o36\n",
      "e\n",
      "\n",
      "25-04-08 08:27:14 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o37\n",
      "e\n",
      "\n",
      "25-04-08 08:27:14 - DEBUG - Answer received: !yv\n",
      "25-04-08 08:27:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o23\n",
      "e\n",
      "\n",
      "25-04-08 08:27:14 - DEBUG - Answer received: !yv\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"streaming_word_count\")\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location)\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.13:3.3.1,org.apache.spark:spark-sql_2.12:3.5.3\")\n",
    "    .config(\"spark.driver.cores\", 2)\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.executor.memory\", \"1g\")\n",
    "    .config(\"spark.submit.deployMode\", \"client\")\n",
    "    .config(\"spark.log.level\", \"ALL\")\n",
    "    # .config(conf=conf)\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
