{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be1361a-0140-444f-ac62-b0380c55c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from pyspark.sql.connect.session import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name, broadcast, to_date, col\n",
    "%run ./config.ipynb"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Bronze:\n",
    "    def __init__(self, spark: SparkSession, env: str):\n",
    "        self.spark = spark\n",
    "        self.conf = Config(self.spark)\n",
    "        self.landing_zone = self.conf.base_dir_data + \"/raw\"\n",
    "        self.checkpoint_base = self.conf.base_dir_checkpoint + \"/checkpoint\"\n",
    "        self.catalog = env\n",
    "        self.db_name = self.conf.db_name\n",
    "        self.spark.sql(f\"USE {self.catalog}.{self.db_name}\")\n",
    "\n",
    "    def consume_user_registration(\n",
    "            self,\n",
    "            once: bool = True,\n",
    "            processing_time: str = \"5 seconds\",\n",
    "    ):\n",
    "        schema = \"user_id long, device_id long, mac_address string, registration_timestamp double\"\n",
    "        df = (\n",
    "            self.spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .schema(schema)\n",
    "            .option(\"maxFilePerTrigger\", 1)\n",
    "            .option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .load(self.landing_zone + \"/registered_users_bz\")\n",
    "            .withColumn(\"load_time\", current_timestamp())\n",
    "            .withColumn(\"source_file\", input_file_name())\n",
    "        )\n",
    "        writer = (\n",
    "            df.writeStream\n",
    "            .format(\"delta\")\n",
    "            .option(\"checkpointLocation\", self.checkpoint_base + \"/registered_users_bz\")\n",
    "            .outputMode(\"append\")\n",
    "            .queryName(\"registered_users_bz_ingestion_stream\")\n",
    "        )\n",
    "\n",
    "        self.spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"bronze_p2\")\n",
    "\n",
    "        if once:\n",
    "            return writer.trigger(availableNow=True).toTable(f\"{self.catalog}.{self.db_name}.registered_users_bz\")\n",
    "        else:\n",
    "            return writer.trigger(processingTime=processing_time).toTable(\n",
    "                f\"{self.catalog}.{self.db_name}.registered_users_bz\")\n",
    "\n",
    "    def consume_gym_logins(\n",
    "            self,\n",
    "            once: bool = True,\n",
    "            processing_time: str = \"5 seconds\",\n",
    "    ):\n",
    "        schema = \"mac_address string, gym bigint, login double, logout double\"\n",
    "        df = (\n",
    "            self.spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .schema(schema)\n",
    "            .option(\"maxFilePerTrigger\", 1)\n",
    "            .option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"header\", )\n",
    "            .load(self.landing_zone + \"/gym_logins_bz\")\n",
    "            .withColumn(\"load_time\", current_timestamp())\n",
    "            .withColumn(\"source_file\", input_file_name())\n",
    "        )\n",
    "        writer = (\n",
    "            df.writeStream\n",
    "            .format(\"delta\")\n",
    "            .option(\"checkpointLocation\", self.checkpoint_base + \"/gym_logins_bz\")\n",
    "            .outputMode(\"append\")\n",
    "            .queryName(\"gym_logins_bz_ingestion_stream\")\n",
    "        )\n",
    "\n",
    "        self.spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"bronze_p2\")\n",
    "\n",
    "        if once:\n",
    "            return writer.trigger(availableNow=True).toTable(f\"{self.catalog}.{self.db_name}.gym_logins_bz\")\n",
    "        else:\n",
    "            return writer.trigger(processingTime=processing_time).toTable(\n",
    "                f\"{self.catalog}.{self.db_name}.gym_logins_bz\")\n",
    "\n",
    "    def consume_kafka_multiplex(\n",
    "        self,\n",
    "        once: bool = True,\n",
    "        processing_time: str = \"5 seconds\",\n",
    "    ):\n",
    "        schema = \"key string, value string, topic string, partition bigint, offset bigint, timestamp bigint\"\n",
    "        df_date_lookup = self.spark.table(f\"{self.catalog}.{self.db_name}.date_lookup\").select(\"date\", \"week_part\")\n",
    "\n",
    "        df = (\n",
    "            self.spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .schema(schema)\n",
    "            .option(\"maxFilePerTrigger\", 1)\n",
    "            .option(\"cloudFiles.format\", \"json\")\n",
    "            .load(self.landing_zone + \"/kafka_multiplex_bz\")\n",
    "            .withColumn(\"load_time\", current_timestamp())\n",
    "            .withColumn(\"source_file\", input_file_name())\n",
    "            .join(\n",
    "                broadcast(df_date_lookup),\n",
    "                on=[to_date(col(\"timestamp\") / 1000).cast(\"timestamp\") == col(\"date\")],\n",
    "                how=\"left\"\n",
    "            )\n",
    "        )\n",
    "        writer = (\n",
    "            df.writeStream\n",
    "            .format(\"delta\")\n",
    "            .option(\"checkpointLocation\", self.checkpoint_base + \"/kafka_multiplex_bz\")\n",
    "            .outputMode(\"append\")\n",
    "            .queryName(\"kafka_multiplex_bz_ingestion_stream\")\n",
    "        )\n",
    "        self.spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"bronze_p1\")\n",
    "\n",
    "        if once:\n",
    "            return writer.trigger(availableNow=True).toTable(f\"{self.catalog}.{self.db_name}.kafka_multiplex_bz\")\n",
    "        else:\n",
    "            return writer.trigger(processingTime=processing_time).toTable(\n",
    "                f\"{self.catalog}.{self.db_name}.kafka_multiplex_bz\")\n",
    "\n",
    "    def consume(\n",
    "        self,\n",
    "        once: bool = True,\n",
    "        processing_time: str = \"5 seconds\",\n",
    "    ):\n",
    "        self.consume_user_registration(once, processing_time)\n",
    "        self.consume_gym_logins(once, processing_time)\n",
    "        self.consume_kafka_multiplex(once, processing_time)\n",
    "        if once:\n",
    "            for stream in self.spark.streams.active:\n",
    "                stream.awaitTermination()"
   ],
   "id": "d19a6aaa6fe95eb2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
