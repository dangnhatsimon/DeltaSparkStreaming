{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be1361a-0140-444f-ac62-b0380c55c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.connect.session import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name, broadcast, to_date, col, rank, from_json, expr, when, floor, months_between, current_date\n",
    "from pyspark.sql.window import Window\n",
    "%run ./config.ipynb"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Upserter:\n",
    "    def __init__(self, merge_query, temp_view):\n",
    "        self.merge_query = merge_query\n",
    "        self.temp_view = temp_view\n",
    "\n",
    "    def upsert(\n",
    "        self,\n",
    "        df_micro_batch: DataFrame,\n",
    "        batch_id: str,\n",
    "    ):\n",
    "        df_micro_batch.createOrReplaceTempView(self.temp_view)\n",
    "        df_micro_batch._jdk.sparkSession().sql(self.merge_query)"
   ],
   "id": "d19a6aaa6fe95eb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CDCUpserter:\n",
    "    def __init__(self, merge_query, temp_view, id_column, sort_by):\n",
    "        self.merge_query = merge_query\n",
    "        self.temp_view = temp_view\n",
    "        self.id_column = id_column\n",
    "        self.sort_by = sort_by\n",
    "\n",
    "    def upsert(\n",
    "            self,\n",
    "            df_micro_batch: DataFrame,\n",
    "            batch_id: str,\n",
    "    ):\n",
    "        window = Window.partitionBy(self.id_column).orderBy(col(self.sort_by).desc())\n",
    "        (\n",
    "            df_micro_batch\n",
    "            .filter(col(\"update_type\").isin([\"new\", \"update\"]))\n",
    "            .withColumn(\"rank\", rank().over(window)).filter(\"rank == 1\")\n",
    "            .drop(\"rank\")\n",
    "            .createOrReplaceTempView(self.temp_view)\n",
    "        )\n",
    "        df_micro_batch._jdk.sparkSession().sql(self.merge_query)"
   ],
   "id": "bd10a786543d67eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Silver:\n",
    "    def __init__(self, spark: SparkSession, env: str):\n",
    "        self.spark = spark\n",
    "        self.conf = Config(self.spark)\n",
    "        self.checkpoint_base = self.conf.base_dir_checkpoint + \"/checkpoint\"\n",
    "        self.catalog = env\n",
    "        self.db_name = self.conf.db_name\n",
    "        self.spark.sql(f\"USE {self.catalog}.{self.db_name}\")\n",
    "\n",
    "    def upsert_users(self, once: bool = True, processing_time: str = \"15 seconds\", startingVersion: int = 0):\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.users a\n",
    "            USING users_delta b\n",
    "            ON a.user_id = b.user_id\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "        upserter = Upserter(query, \"users_delta\")\n",
    "\n",
    "        # Spark Structured Streaming accepts append only sources.\n",
    "        #      - This is not a problem for silver layer streams because bronze layer is insert only\n",
    "        #      - However, you may want to allow bronze layer deletes due to regulatory compliance\n",
    "        # Spark Structured Streaming throws an exception if any modifications occur on the table being used as a source\n",
    "        #      - This is a problem for silver layer streaming jobs.\n",
    "        #      - ignoreDeletes allows to delete records on partition column in the bronze layer without exception on silver layer streams\n",
    "        # Starting version is to allow you to restart your stream from a given version just in case you need it\n",
    "        #      - startingVersion is only applied for an empty checkpoint\n",
    "        # Limiting your input stream size is critical for running on limited capacity\n",
    "        #      - maxFilesPerTrigger/maxBytesPerTrigger can be used with the readStream\n",
    "        #      - Default value is 1000 for maxFilesPerTrigger and maxBytesPerTrigger has no default value\n",
    "        #      - The recomended maxFilesPerTrigger is equal to #executors assuming auto optimize file size to 128 MB\n",
    "\n",
    "        df = (\n",
    "            self.spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            # .option(\"withEventTimeOrder\", \"true\")\n",
    "            # .option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "            .table(f\"{self.catalog}.{self.db_name}.registered_users_bz\")\n",
    "            .selectExpr(\"user_id\", \"device_id\", \"mac_address\", \"cast(registration_timestamp as timestamp)\")\n",
    "            .withWatermark(\"registration_timestamp\", \"30 seconds\")\n",
    "            .dropDuplicates([\"user_id\", \"device_id\"])\n",
    "        )\n",
    "\n",
    "        # We read new records in bronze layer and insert them to silver. So, the silver layer is insert only in a typical case.\n",
    "        # However, we want to ignoreDeletes, remove duplicates and also merge with an update statement depending upon the scenarios\n",
    "        # Hence, it is recomended to se the update mode\n",
    "        stream_writer = (\n",
    "            df.writeStream\n",
    "            .foreachBatch(upserter.upsert)\n",
    "            .outputMode(\"update\")\n",
    "            .option(\"checkpointLocation\", f\"{self.checkpoint_base}/users\")\n",
    "            .queryName(\"users_upsert_stream\")\n",
    "        )\n",
    "\n",
    "        self.spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p2\")\n",
    "\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "    def upsert_gym_logs(self, once: bool = True, processing_time: str = \"15 seconds\", startingVersion: int = 0):\n",
    "        # Idempotent - Insert new login records\n",
    "        #           - Update logout time when\n",
    "        #                   1. It is greater than login time\n",
    "        #                   2. It is greater than earlier logout\n",
    "        #                   3. It is not NULL (This is also satisfied by above conditions)\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.gym_logs a\n",
    "            USING gym_logs_delta b\n",
    "            ON a.mac_address = b.mac_address AND a.gym = b.gym AND a.login = b.login\n",
    "            WHEN MATCHED AND b.logout > a.login AND b.logout > a.logout\n",
    "              THEN UPDATE SET logout = b.logout\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "            \"\"\"\n",
    "\n",
    "        data_upserter = Upserter(query, \"gym_logs_delta\")\n",
    "\n",
    "        df_delta = (self.spark.readStream\n",
    "                    .option(\"startingVersion\", startingVersion)\n",
    "                    .option(\"ignoreDeletes\", True)\n",
    "                    # .option(\"withEventTimeOrder\", \"true\")\n",
    "                    # .option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "                    .table(f\"{self.catalog}.{self.db_name}.gym_logins_bz\")\n",
    "                    .selectExpr(\"mac_address\", \"gym\", \"cast(login as timestamp)\", \"cast(logout as timestamp)\")\n",
    "                    .withWatermark(\"login\", \"30 seconds\")\n",
    "                    .dropDuplicates([\"mac_address\", \"gym\", \"login\"])\n",
    "                    )\n",
    "\n",
    "        stream_writer = (\n",
    "            df_delta.writeStream\n",
    "            .foreachBatch(data_upserter.upsert)\n",
    "            .outputMode(\"update\")\n",
    "            .option(\"checkpointLocation\", f\"{self.checkpoint_base}/gym_logs\")\n",
    "            .queryName(\"gym_logs_upsert_stream\")\n",
    "        )\n",
    "\n",
    "        self.spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p3\")\n",
    "\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "    def upsert_user_profile(self, once: bool = False, processing_time: str = \"15 seconds\", startingVersion: int = 0):\n",
    "\n",
    "        # Idempotent - Insert new record\n",
    "        #           - Ignore deletes\n",
    "        #           - Update user details when\n",
    "        #               1. update_type in (\"new\", \"append\")\n",
    "        #               2. current update is newer than the earlier\n",
    "        schema = \"\"\"\n",
    "            user_id bigint, update_type STRING, timestamp FLOAT,\n",
    "            dob STRING, sex STRING, gender STRING, first_name STRING, last_name STRING,\n",
    "            address STRUCT<street_address: STRING, city: STRING, state: STRING, zip: INT>\n",
    "        \"\"\"\n",
    "\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.user_profile a\n",
    "            USING user_profile_cdc b\n",
    "            ON a.user_id = b.user_id\n",
    "            WHEN MATCHED AND a.updated < b.updated\n",
    "              THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED\n",
    "              THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = CDCUpserter(query, \"user_profile_cdc\", \"user_id\", \"updated\")\n",
    "\n",
    "        df_cdc = (\n",
    "            self.spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            # .option(\"withEventTimeOrder\", \"true\")\n",
    "            # .option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "            .table(f\"{self.catalog}.{self.db_name}.kafka_multiplex_bz\")\n",
    "            .filter(\"topic = 'user_info'\")\n",
    "            .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "            .select(\"v.*\")\n",
    "            .select(\n",
    "                \"user_id\", to_date('dob', 'MM/dd/yyyy').alias('dob'),\n",
    "                \"sex\", \"gender\", \"first_name\", \"last_name\", \"address.*\",\n",
    "                col(\"timestamp\").cast(\"timestamp\").alias(\"updated\"),\n",
    "                \"update_type\"\n",
    "            )\n",
    "            .withWatermark(\"updated\", \"30 seconds\")\n",
    "            .dropDuplicates([\"user_id\", \"updated\"])\n",
    "        )\n",
    "\n",
    "        stream_writer = (\n",
    "            df_cdc.writeStream\n",
    "            .foreachBatch(data_upserter.upsert)\n",
    "            .outputMode(\"update\")\n",
    "            .option(\"checkpointLocation\", f\"{self.checkpoint_base}/user_profile\")\n",
    "            .queryName(\"user_profile_stream\")\n",
    "        )\n",
    "\n",
    "        self.spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p3\")\n",
    "\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "    def upsert_workouts(self, once: bool = False, processing_time: str = \"10 seconds\", startingVersion: int = 0):\n",
    "        schema = \"user_id INT, workout_id INT, timestamp FLOAT, action STRING, session_id INT\"\n",
    "\n",
    "        # Idempotent - User cannot have two workout sessions at the same time. So ignore the duplicates and insert the new records\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.workouts a\n",
    "            USING workouts_delta b\n",
    "            ON a.user_id = b.user_id AND a.time = b.time\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = Upserter(query, \"workouts_delta\")\n",
    "\n",
    "        df_delta = (\n",
    "            self.spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            # .option(\"withEventTimeOrder\", \"true\")\n",
    "            # .option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "            .table(f\"{self.catalog}.{self.db_name}.kafka_multiplex_bz\")\n",
    "            .filter(\"topic = 'workout'\")\n",
    "            .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "            .select(\"v.*\")\n",
    "            .select(\n",
    "                \"user_id\", \"workout_id\",\n",
    "                col(\"timestamp\").cast(\"timestamp\").alias(\"time\"),\n",
    "                \"action\", \"session_id\"\n",
    "            )\n",
    "            .withWatermark(\"time\", \"30 seconds\")\n",
    "            .dropDuplicates([\"user_id\", \"time\"])\n",
    "        )\n",
    "\n",
    "        stream_writer = (\n",
    "            df_delta.writeStream\n",
    "            .foreachBatch(data_upserter.upsert)\n",
    "            .outputMode(\"update\")\n",
    "            .option(\"checkpointLocation\", f\"{self.checkpoint_base}/workouts\")\n",
    "            .queryName(\"workouts_upsert_stream\")\n",
    "        )\n",
    "\n",
    "        self.spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p3\")\n",
    "\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "    def upsert_heart_rate(self, once: bool = False, processing_time: str = \"10 seconds\", startingVersion: int = 0):\n",
    "        schema = \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\"\n",
    "\n",
    "        # Idempotent - Only one BPM signal is allowed at a timestamp. So ignore the duplicates and insert the new records\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.heart_rate a\n",
    "            USING heart_rate_delta b\n",
    "            ON a.device_id = b.device_id AND a.time = b.time\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = Upserter(query, \"heart_rate_delta\")\n",
    "\n",
    "        df_delta = (\n",
    "            self.spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            # .option(\"withEventTimeOrder\", \"true\")\n",
    "            # .option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "            .table(f\"{self.catalog}.{self.db_name}.kafka_multiplex_bz\")\n",
    "            .filter(\"topic = 'bpm'\")\n",
    "            .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "            .select(\"v.*\", when(col(\"v.heartrate\") <= 0, False).otherwise(True).alias(\"valid\"))\n",
    "            .withWatermark(\"time\", \"30 seconds\")\n",
    "            .dropDuplicates([\"device_id\", \"time\"])\n",
    "        )\n",
    "\n",
    "        stream_writer = (\n",
    "            df_delta.writeStream\n",
    "            .foreachBatch(data_upserter.upsert)\n",
    "            .outputMode(\"update\")\n",
    "            .option(\"checkpointLocation\", f\"{self.checkpoint_base}/heart_rate\")\n",
    "            .queryName(\"heart_rate_upsert_stream\")\n",
    "        )\n",
    "\n",
    "        self.spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p2\")\n",
    "\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "    def age_bins(self, dob_col):\n",
    "        age_col = floor(months_between(current_date(), dob_col) / 12).alias(\"age\")\n",
    "        return (when((age_col < 18), \"under 18\")\n",
    "                .when((age_col >= 18) & (age_col < 25), \"18-25\")\n",
    "                .when((age_col >= 25) & (age_col < 35), \"25-35\")\n",
    "                .when((age_col >= 35) & (age_col < 45), \"35-45\")\n",
    "                .when((age_col >= 45) & (age_col < 55), \"45-55\")\n",
    "                .when((age_col >= 55) & (age_col < 65), \"55-65\")\n",
    "                .when((age_col >= 65) & (age_col < 75), \"65-75\")\n",
    "                .when((age_col >= 75) & (age_col < 85), \"75-85\")\n",
    "                .when((age_col >= 85) & (age_col < 95), \"85-95\")\n",
    "                .when((age_col >= 95), \"95+\")\n",
    "                .otherwise(\"invalid age\").alias(\"age\"))\n",
    "\n",
    "    def upsert_user_bins(self, once: bool = True, processing_time: str = \"15 seconds\", startingVersion: int = 0):\n",
    "        # Idempotent - This table is maintained as SCD Type 1 dimension\n",
    "        #            - Insert new user_id records\n",
    "        #            - Update old records using the user_id\n",
    "\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.user_bins a\n",
    "            USING user_bins_delta b\n",
    "            ON a.user_id=b.user_id\n",
    "            WHEN MATCHED\n",
    "              THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = Upserter(query, \"user_bins_delta\")\n",
    "\n",
    "        df_user = self.spark.table(f\"{self.catalog}.{self.db_name}.users\").select(\"user_id\")\n",
    "\n",
    "        # Running stream on silver table requires ignoreChanges\n",
    "        # No watermark required - Stream to staic join is stateless\n",
    "        df_delta = (\n",
    "            self.spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreChanges\", True)\n",
    "            # .option(\"withEventTimeOrder\", \"true\")\n",
    "            # .option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "            .table(f\"{self.catalog}.{self.db_name}.user_profile\")\n",
    "            .join(df_user, [\"user_id\"], \"left\")\n",
    "            .select(\"user_id\", self.age_bins(col(\"dob\")), \"gender\", \"city\", \"state\")\n",
    "        )\n",
    "\n",
    "        stream_writer = (\n",
    "            df_delta.writeStream\n",
    "            .foreachBatch(data_upserter.upsert)\n",
    "            .outputMode(\"update\")\n",
    "            .option(\"checkpointLocation\", f\"{self.checkpoint_base}/user_bins\")\n",
    "            .queryName(\"user_bins_upsert_stream\")\n",
    "        )\n",
    "\n",
    "        self.spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p3\")\n",
    "\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "    def upsert_completed_workouts(self, once: bool = True, processing_time: str = \"15 seconds\",\n",
    "                                  startingVersion: int = 0):\n",
    "\n",
    "        # Idempotent - Only one user workout session completes. So ignore the duplicates and insert the new records\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.completed_workouts a\n",
    "            USING completed_workouts_delta b\n",
    "            ON a.user_id=b.user_id AND a.workout_id = b.workout_id AND a.session_id=b.session_id\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = Upserter(query, \"completed_workouts_delta\")\n",
    "\n",
    "        df_start = (\n",
    "            self.spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            # .option(\"withEventTimeOrder\", \"true\")\n",
    "            # .option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "            .table(f\"{self.catalog}.{self.db_name}.workouts\")\n",
    "            .filter(\"action = 'start'\")\n",
    "            .selectExpr(\"user_id\", \"workout_id\", \"session_id\", \"time as start_time\")\n",
    "            .withWatermark(\"start_time\", \"30 seconds\")\n",
    "            # .dropDuplicates([\"user_id\", \"workout_id\", \"session_id\", \"start_time\"])\n",
    "        )\n",
    "\n",
    "        df_stop = (\n",
    "            self.spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            # .option(\"withEventTimeOrder\", \"true\")\n",
    "            # .option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "            .table(f\"{self.catalog}.{self.db_name}.workouts\")\n",
    "            .filter(\"action = 'stop'\")\n",
    "            .selectExpr(\"user_id\", \"workout_id\", \"session_id\", \"time as end_time\")\n",
    "            .withWatermark(\"end_time\", \"30 seconds\")\n",
    "            # .dropDuplicates([\"user_id\", \"workout_id\", \"session_id\", \"end_time\"])\n",
    "        )\n",
    "\n",
    "        # State cleanup - Define a condition to clean the state\n",
    "        #               - stop must occur within 3 hours of start\n",
    "        #               - stop < start + 3 hours\n",
    "        join_condition = [\n",
    "            df_start.user_id == df_stop.user_id,\n",
    "            df_start.workout_id == df_stop.workout_id,\n",
    "            df_start.session_id == df_stop.session_id,\n",
    "            df_stop.end_time < df_start.start_time + expr('interval 3 hour')\n",
    "        ]\n",
    "\n",
    "        df_delta = (\n",
    "            df_start.join(df_stop, join_condition)\n",
    "            .select(df_start.user_id, df_start.workout_id, df_start.session_id, df_start.start_time, df_stop.end_time)\n",
    "        )\n",
    "\n",
    "        stream_writer = (\n",
    "            df_delta.writeStream\n",
    "            .foreachBatch(data_upserter.upsert)\n",
    "            .outputMode(\"append\")\n",
    "            .option(\"checkpointLocation\", f\"{self.checkpoint_base}/completed_workouts\")\n",
    "            .queryName(\"completed_workouts_upsert_stream\")\n",
    "        )\n",
    "\n",
    "        self.spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p1\")\n",
    "\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "    def upsert_workout_bpm(self, once: bool = True, processing_time: str = \"15 seconds\", startingVersion: int = 0):\n",
    "\n",
    "        # Idempotent - Only one user workout session completes. So ignore the duplicates and insert the new records\n",
    "        query = f\"\"\"\n",
    "            MERGE INTO {self.catalog}.{self.db_name}.workout_bpm a\n",
    "            USING workout_bpm_delta b\n",
    "            ON a.user_id=b.user_id AND a.workout_id = b.workout_id AND a.session_id=b.session_id AND a.time=b.time\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = Upserter(query, \"workout_bpm_delta\")\n",
    "\n",
    "        df_users = self.spark.read.table(\"users\")\n",
    "\n",
    "        df_completed_workouts = (\n",
    "            self.spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            # .option(\"withEventTimeOrder\", \"true\")\n",
    "            # .option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "            .table(f\"{self.catalog}.{self.db_name}.completed_workouts\")\n",
    "            .join(df_users, \"user_id\")\n",
    "            .selectExpr(\"user_id\", \"device_id\", \"workout_id\", \"session_id\", \"start_time\", \"end_time\")\n",
    "            .withWatermark(\"end_time\", \"30 seconds\")\n",
    "        )\n",
    "\n",
    "        df_bpm = (\n",
    "            self.spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            # .option(\"withEventTimeOrder\", \"true\")\n",
    "            # .option(\"maxFilesPerTrigger\", self.maxFilesPerTrigger)\n",
    "            .table(f\"{self.catalog}.{self.db_name}.heart_rate\")\n",
    "            .filter(\"valid = True\")\n",
    "            .selectExpr(\"device_id\", \"time\", \"heartrate\")\n",
    "            .withWatermark(\"time\", \"30 seconds\")\n",
    "        )\n",
    "\n",
    "        # State cleanup - Define a condition to clean the state\n",
    "        #               - Workout could be a maximum of three hours\n",
    "        #               - workout must end within 3 hours of bpm\n",
    "        #               - workout.end < bpm.time + 3 hours\n",
    "        join_condition = [\n",
    "            df_completed_workouts.device_id == df_bpm.device_id,\n",
    "            df_bpm.time > df_completed_workouts.start_time, df_bpm.time <= df_completed_workouts.end_time,\n",
    "            df_completed_workouts.end_time < df_bpm.time + expr('interval 3 hour')\n",
    "        ]\n",
    "\n",
    "        df_delta = (df_bpm.join(df_completed_workouts, join_condition)\n",
    "                    .select(\"user_id\", \"workout_id\", \"session_id\", \"start_time\", \"end_time\", \"time\", \"heartrate\")\n",
    "                    )\n",
    "\n",
    "        stream_writer = (\n",
    "            df_delta.writeStream\n",
    "            .foreachBatch(data_upserter.upsert)\n",
    "            .outputMode(\"append\")\n",
    "            .option(\"checkpointLocation\", f\"{self.checkpoint_base}/workout_bpm\")\n",
    "            .queryName(\"workout_bpm_upsert_stream\")\n",
    "        )\n",
    "\n",
    "        self.spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_p2\")\n",
    "\n",
    "        if once:\n",
    "            return stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "    def await_queries(self, once):\n",
    "        if once:\n",
    "            for stream in self.spark.streams.active:\n",
    "                stream.awaitTermination()\n",
    "\n",
    "    def upsert(\n",
    "        self,\n",
    "        once: bool = True,\n",
    "        processing_time: str = \"5 seconds\"\n",
    "    ):\n",
    "        self.upsert_users(once, processing_time)\n",
    "        self.upsert_gym_logs(once, processing_time)\n",
    "        self.upsert_user_profile(once, processing_time)\n",
    "        self.upsert_workouts(once, processing_time)\n",
    "        self.upsert_heart_rate(once, processing_time)\n",
    "        self.await_queries(once)\n",
    "        self.upsert_user_bins(once, processing_time)\n",
    "        self.upsert_completed_workouts(once, processing_time)\n",
    "        self.await_queries(once)\n",
    "        self.upsert_workout_bpm(once, processing_time)\n",
    "        self.await_queries(once)"
   ],
   "id": "72643316a7d576fe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
