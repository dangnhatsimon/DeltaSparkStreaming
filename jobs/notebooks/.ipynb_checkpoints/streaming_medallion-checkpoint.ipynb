{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb4f7c6a-cd66-4081-aa08-e64e7106b431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/18 03:52:06 INFO  SparkContext:60 SparkContext is stopping with exitCode 0.\n",
      "25/05/18 03:52:06 INFO  SparkUI:60 Stopped Spark web UI at http://7db080dfe9d2:4040\n",
      "25/05/18 03:52:06 INFO  StandaloneSchedulerBackend:60 Shutting down all executors\n",
      "25/05/18 03:52:06 INFO  StandaloneSchedulerBackend$StandaloneDriverEndpoint:60 Asking each executor to shut down\n",
      "25/05/18 03:52:06 INFO  MapOutputTrackerMasterEndpoint:60 MapOutputTrackerMasterEndpoint stopped!\n",
      "25/05/18 03:52:06 INFO  MemoryStore:60 MemoryStore cleared\n",
      "25/05/18 03:52:06 INFO  BlockManager:60 BlockManager stopped\n",
      "25/05/18 03:52:06 INFO  BlockManagerMaster:60 BlockManagerMaster stopped\n",
      "25/05/18 03:52:06 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:60 OutputCommitCoordinator stopped!\n",
      "25/05/18 03:52:06 INFO  SparkContext:60 Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73822cca-3af0-496e-a52c-5ecdd9f7e583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split, trim, lower, expr\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "import logging\n",
    "from os.path import abspath\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, List, Tuple, Any, Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c085da-3212-4eee-b90b-1f108d16f231",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%y-%m-%d %H:%M:%S\",\n",
    "    level=logging.DEBUG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af23095-7408-41c6-b3cc-992886c1da6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-05-18 03:52:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o38\n",
      "e\n",
      "\n",
      "25-05-18 03:52:14 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o39\n",
      "e\n",
      "\n",
      "25-05-18 03:52:14 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o40\n",
      "e\n",
      "\n",
      "25-05-18 03:52:14 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o43\n",
      "e\n",
      "\n",
      "25-05-18 03:52:14 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o44\n",
      "e\n",
      "\n",
      "25-05-18 03:52:14 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o45\n",
      "e\n",
      "\n",
      "25-05-18 03:52:14 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o46\n",
      "e\n",
      "\n",
      "25-05-18 03:52:14 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o49\n",
      "e\n",
      "\n",
      "25-05-18 03:52:14 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o50\n",
      "e\n",
      "\n",
      "25-05-18 03:52:14 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:14 - DEBUG - Command to send: m\n",
      "d\n",
      "o51\n",
      "e\n",
      "\n",
      "25-05-18 03:52:14 - DEBUG - Answer received: !yv\n"
     ]
    }
   ],
   "source": [
    "class InvoiceStreamBronze:\n",
    "    def __init__(\n",
    "        self,\n",
    "        spark: SparkSession\n",
    "    ):\n",
    "        self.spark = spark\n",
    "\n",
    "    def read_invoices(\n",
    "        self,\n",
    "        format: str,\n",
    "        path: Union[str, Path],\n",
    "        schema: Union[str, Any],\n",
    "        clean_source: Literal[\"delete\", \"archive\"],\n",
    "        archive_dir: Optional[str],\n",
    "    ) -> DataFrame:\n",
    "        if isinstance(path, str):\n",
    "            path = Path(path).as_posix()\n",
    "        if clean_source == \"archive\":\n",
    "            return (\n",
    "                self.spark.readStream\n",
    "                .format(format)\n",
    "                .schema(schema)\n",
    "                .option(\"cleanSource\", \"archive\")\n",
    "                .option(\"sourceArchiveDir\", archive_dir)\n",
    "                .load(path)\n",
    "            )\n",
    "        return (\n",
    "            self.spark.readStream\n",
    "            .format(format)\n",
    "            .schema(schema)\n",
    "            .option(\"cleanSource\", clean_source)\n",
    "            .load(path)\n",
    "        )\n",
    "\n",
    "    def write_invoices(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        format: str,\n",
    "        checkpoint_location: str,\n",
    "        output_mode: str,\n",
    "        table: str,\n",
    "        query_name: str\n",
    "    ):\n",
    "        return (\n",
    "            df.writeStream\n",
    "            .queryName(query_name)\n",
    "            .format(format)\n",
    "            .option(\"checkpointLocation\", checkpoint_location)\n",
    "            .outputMode(output_mode)\n",
    "            .toTable(table)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5156e85-1b04-46fa-ba8f-760448ab5c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvoiceStreamSilver:\n",
    "    def __init__(\n",
    "        self,\n",
    "        spark: SparkSession\n",
    "    ):\n",
    "        self.spark = spark\n",
    "\n",
    "    def read_invoices(self, table_name: str) -> DataFrame:\n",
    "        return (\n",
    "            self.spark.readStream\n",
    "            .table(table_name)\n",
    "        )\n",
    "\n",
    "    def explode_invoices(self, df: DataFrame) -> DataFrame:\n",
    "        return (\n",
    "            df.selectExpr(\n",
    "                \"InvoiceNumber\",\n",
    "                \"CreatedTime\",\n",
    "                \"StoreID\",\n",
    "                \"PosID\",\n",
    "                \"CustomerType\",\n",
    "                \"PaymentMethod\",\n",
    "                \"DeliveryType\",\n",
    "                \"DeliveryAddress.City\",\n",
    "                \"DeliveryAddress.PinCode\",\n",
    "                \"DeliveryAddress.State\",\n",
    "                \"explode(InvoiceLineItems) AS LineItem\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def flatten_invoices(self, df: DataFrame) -> DataFrame:\n",
    "        return (\n",
    "            df.withColumn(\"ItemCode\", expr(\"LineItem.ItemCode\"))\n",
    "            .withColumn(\"ItemDescription\", expr(\"LineItem.ItemDescription\"))\n",
    "            .withColumn(\"ItemPrice\", expr(\"LineItem.ItemPrice\"))\n",
    "            .withColumn(\"ItemQty\", expr(\"LineItem.ItemQty\"))\n",
    "            .withColumn(\"TotalValue\", expr(\"LineItem.TotalValue\"))\n",
    "            .drop(\"LineItem\")\n",
    "        )\n",
    "\n",
    "    def write_invoices(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        format: str,\n",
    "        checkpoint_location: str,\n",
    "        output_mode: str,\n",
    "        table: str,\n",
    "        query_name: str,\n",
    "    ):\n",
    "        return (\n",
    "            df.writeStream\n",
    "            .queryName(query_name)\n",
    "            .format(format)\n",
    "            .option(\"checkpointLocation\", checkpoint_location)\n",
    "            .outputMode(output_mode)\n",
    "            .toTable(table)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "554f53f6-1274-4260-af37-7fdc1cf86df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "SparkConf\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ycorg.apache.spark.SparkConf\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: i\n",
      "org.apache.spark.SparkConf\n",
      "bTrue\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro52\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o52\n",
      "set\n",
      "sspark.app.name\n",
      "sInvoicesStreamMedallion\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro53\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o52\n",
      "set\n",
      "sspark.sql.catalogImplementation\n",
      "shive\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro54\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o52\n",
      "get\n",
      "sspark.executor.allowSparkContext\n",
      "sfalse\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysfalse\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o52\n",
      "contains\n",
      "sspark.serializer.objectStreamReset\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ybfalse\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o52\n",
      "set\n",
      "sspark.serializer.objectStreamReset\n",
      "s100\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro55\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o52\n",
      "contains\n",
      "sspark.rdd.compress\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ybfalse\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o52\n",
      "set\n",
      "sspark.rdd.compress\n",
      "sTrue\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro56\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o52\n",
      "contains\n",
      "sspark.master\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o52\n",
      "contains\n",
      "sspark.app.name\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o52\n",
      "contains\n",
      "sspark.master\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o52\n",
      "get\n",
      "sspark.master\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark://delta-streaming-spark-master:7077\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o52\n",
      "contains\n",
      "sspark.app.name\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o52\n",
      "get\n",
      "sspark.app.name\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysInvoicesStreamMedallion\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o52\n",
      "contains\n",
      "sspark.home\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ybfalse\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o52\n",
      "getAll\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yto57\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i0\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro58\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o58\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.eventLog.enabled\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o58\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ystrue\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro59\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o59\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.executor.memory\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o59\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ys2g\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro60\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o60\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.log.level\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o60\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysINFO\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i3\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro61\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o61\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.hadoop.fs.s3a.aws.credentials.provider\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o61\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysorg.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i4\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro62\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o62\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.driver.memoryOverheadFactor\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o62\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ys0.1\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i5\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro63\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o63\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.executor.cores\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o63\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ys4\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i6\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro64\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o64\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.serializer\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o64\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysorg.apache.spark.serializer.KryoSerializer\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i7\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro65\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o65\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.hadoop.fs.s3a.impl\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o65\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysorg.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i8\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro66\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o66\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.history.fs.logDirectory\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o66\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ys/opt/spark/spark-events\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i9\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro67\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o67\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.hadoop.fs.s3a.access.key\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o67\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysaccess_key\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i10\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro68\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o68\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.hadoop.fs.s3a.secret.key\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o68\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yssecret_key\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i11\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro69\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o69\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.app.name\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o69\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysInvoicesStreamMedallion\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i12\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro70\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o70\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.sql.catalogImplementation\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o70\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yshive\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i13\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro71\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o71\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.sql.extensions\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o71\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysio.delta.sql.DeltaSparkSessionExtension\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i14\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro72\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o72\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.rdd.compress\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o72\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysTrue\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i15\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro73\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o73\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.app.submitTime\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o73\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ys1747539851840\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i16\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro74\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o74\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.serializer.objectStreamReset\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o74\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ys100\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i17\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro75\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o75\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.master\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o75\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark://delta-streaming-spark-master:7077\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i18\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro76\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o76\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.submit.pyFiles\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o76\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ys\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i19\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro77\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o77\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.submit.deployMode\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o77\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysclient\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i20\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro78\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o78\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.ui.showConsoleProgress\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o78\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ystrue\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i21\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro79\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o79\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.sql.warehouse.dir\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o79\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ys/opt/spark/warehouse\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i22\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro80\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o80\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.sql.catalog.spark_catalog\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o80\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysorg.apache.spark.sql.delta.catalog.DeltaCatalog\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "g\n",
      "o57\n",
      "i23\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro81\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o81\n",
      "_1\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysspark.eventLog.dir\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o81\n",
      "_2\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ys/opt/spark/spark-events\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: a\n",
      "e\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi24\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "JavaSparkContext\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ycorg.apache.spark.api.java.JavaSparkContext\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: i\n",
      "org.apache.spark.api.java.JavaSparkContext\n",
      "ro52\n",
      "e\n",
      "\n",
      "25/05/18 03:52:24 INFO  SparkContext:60 Running Spark version 3.5.5\n",
      "25/05/18 03:52:24 INFO  SparkContext:60 OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64\n",
      "25/05/18 03:52:24 INFO  SparkContext:60 Java version 17.0.14\n",
      "Setting Spark log level to \"INFO\".\n",
      "25/05/18 03:52:24 INFO  ResourceUtils:60 ==============================================================\n",
      "25/05/18 03:52:24 INFO  ResourceUtils:60 No custom resources configured for spark.driver.\n",
      "25/05/18 03:52:24 INFO  ResourceUtils:60 ==============================================================\n",
      "25/05/18 03:52:24 INFO  SparkContext:60 Submitted application: InvoicesStreamMedallion\n",
      "25/05/18 03:52:24 INFO  ResourceProfile:60 Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/05/18 03:52:24 INFO  ResourceProfile:60 Limiting resource is cpus at 4 tasks per executor\n",
      "25/05/18 03:52:24 INFO  ResourceProfileManager:60 Added ResourceProfile id: 0\n",
      "25/05/18 03:52:24 INFO  SecurityManager:60 Changing view acls to: root\n",
      "25/05/18 03:52:24 INFO  SecurityManager:60 Changing modify acls to: root\n",
      "25/05/18 03:52:24 INFO  SecurityManager:60 Changing view acls groups to: \n",
      "25/05/18 03:52:24 INFO  SecurityManager:60 Changing modify acls groups to: \n",
      "25/05/18 03:52:24 INFO  SecurityManager:60 SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
      "25/05/18 03:52:24 INFO  Utils:60 Successfully started service 'sparkDriver' on port 35237.\n",
      "25/05/18 03:52:24 INFO  SparkEnv:60 Registering MapOutputTracker\n",
      "25/05/18 03:52:24 INFO  SparkEnv:60 Registering BlockManagerMaster\n",
      "25/05/18 03:52:24 INFO  BlockManagerMasterEndpoint:60 Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/05/18 03:52:24 INFO  BlockManagerMasterEndpoint:60 BlockManagerMasterEndpoint up\n",
      "25/05/18 03:52:24 INFO  SparkEnv:60 Registering BlockManagerMasterHeartbeat\n",
      "25/05/18 03:52:24 INFO  DiskBlockManager:60 Created local directory at /tmp/blockmgr-fdab611f-1564-4bfd-8716-ebde1e7f313f\n",
      "25/05/18 03:52:24 INFO  MemoryStore:60 MemoryStore started with capacity 434.4 MiB\n",
      "25/05/18 03:52:24 INFO  SparkEnv:60 Registering OutputCommitCoordinator\n",
      "25/05/18 03:52:24 INFO  JettyUtils:60 Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/05/18 03:52:24 INFO  Utils:60 Successfully started service 'SparkUI' on port 4040.\n",
      "25/05/18 03:52:24 INFO  StandaloneAppClient$ClientEndpoint:60 Connecting to master spark://delta-streaming-spark-master:7077...\n",
      "25/05/18 03:52:24 INFO  TransportClientFactory:316 Successfully created connection to delta-streaming-spark-master/172.18.0.2:7077 after 1 ms (0 ms spent in bootstraps)\n",
      "25/05/18 03:52:24 INFO  StandaloneSchedulerBackend:60 Connected to Spark cluster with app ID app-20250518035224-0004\n",
      "25/05/18 03:52:24 INFO  Utils:60 Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43279.\n",
      "25/05/18 03:52:24 INFO  NettyBlockTransferService:84 Server created on 7db080dfe9d2:43279\n",
      "25/05/18 03:52:24 INFO  BlockManager:60 Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/05/18 03:52:24 INFO  BlockManagerMaster:60 Registering BlockManager BlockManagerId(driver, 7db080dfe9d2, 43279, None)\n",
      "25/05/18 03:52:24 INFO  BlockManagerMasterEndpoint:60 Registering block manager 7db080dfe9d2:43279 with 434.4 MiB RAM, BlockManagerId(driver, 7db080dfe9d2, 43279, None)\n",
      "25/05/18 03:52:24 INFO  BlockManagerMaster:60 Registered BlockManager BlockManagerId(driver, 7db080dfe9d2, 43279, None)\n",
      "25/05/18 03:52:24 INFO  BlockManager:60 Initialized BlockManager: BlockManagerId(driver, 7db080dfe9d2, 43279, None)\n",
      "25/05/18 03:52:24 INFO  SingleEventLogFileWriter:60 Logging events to file:/opt/spark/spark-events/app-20250518035224-0004.inprogress\n",
      "25/05/18 03:52:24 INFO  StandaloneSchedulerBackend:60 SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro82\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o82\n",
      "sc\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro83\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o83\n",
      "conf\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro84\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "PythonAccumulatorV2\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonAccumulatorV2\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: i\n",
      "org.apache.spark.api.python.PythonAccumulatorV2\n",
      "s127.0.0.1\n",
      "i59799\n",
      "se26c433c0cb127b6c796158364acd89320210c61e4915586ab9de041566fd8ad\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro85\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o82\n",
      "sc\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro86\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o86\n",
      "register\n",
      "ro85\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "PythonUtils\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.api.python.PythonUtils\n",
      "isEncryptionEnabled\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.api.python.PythonUtils\n",
      "isEncryptionEnabled\n",
      "ro82\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ybfalse\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "PythonUtils\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.api.python.PythonUtils\n",
      "getPythonAuthSocketTimeout\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.api.python.PythonUtils\n",
      "getPythonAuthSocketTimeout\n",
      "ro82\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yL15\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "PythonUtils\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.api.python.PythonUtils\n",
      "getSparkBufferSize\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.api.python.PythonUtils\n",
      "getSparkBufferSize\n",
      "ro82\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yi65536\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "org\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yp\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "org.apache\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yp\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "org.apache.spark\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yp\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "org.apache.spark.SparkFiles\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.SparkFiles\n",
      "getRootDirectory\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.SparkFiles\n",
      "getRootDirectory\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ys/tmp/spark-0ebd8959-f139-4424-af36-52213e74340e/userFiles-1c7fc857-6c30-4ed4-9e0b-b88e930389c1\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o84\n",
      "get\n",
      "sspark.submit.pyFiles\n",
      "s\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ys\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "org\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yp\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "org.apache\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yp\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "org.apache.spark\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yp\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "org.apache.spark.util\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yp\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "org.apache.spark.util.Utils\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ycorg.apache.spark.util.Utils\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.util.Utils\n",
      "getLocalDir\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o82\n",
      "sc\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro87\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o87\n",
      "conf\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro88\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.util.Utils\n",
      "getLocalDir\n",
      "ro88\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ys/tmp/spark-0ebd8959-f139-4424-af36-52213e74340e\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "org\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yp\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "org.apache\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yp\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "org.apache.spark\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yp\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "org.apache.spark.util\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yp\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "org.apache.spark.util.Utils\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ycorg.apache.spark.util.Utils\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.util.Utils\n",
      "createTempDir\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.util.Utils\n",
      "createTempDir\n",
      "s/tmp/spark-0ebd8959-f139-4424-af36-52213e74340e\n",
      "spyspark\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro89\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o89\n",
      "getAbsolutePath\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ys/tmp/spark-0ebd8959-f139-4424-af36-52213e74340e/pyspark-f7879a2e-8662-4164-b83b-1b2ba0f5dbcc\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o84\n",
      "get\n",
      "sspark.python.profile\n",
      "sfalse\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysfalse\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o84\n",
      "get\n",
      "sspark.python.profile.memory\n",
      "sfalse\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ysfalse\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "SparkSession\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.SparkSession\n",
      "getDefaultSession\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.SparkSession\n",
      "getDefaultSession\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro90\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o90\n",
      "isDefined\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ybfalse\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "SparkSession\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o82\n",
      "sc\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro91\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: i\n",
      "java.util.HashMap\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yao92\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o92\n",
      "put\n",
      "sspark.app.name\n",
      "sInvoicesStreamMedallion\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yn\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o92\n",
      "put\n",
      "sspark.sql.catalogImplementation\n",
      "shive\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yn\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: i\n",
      "org.apache.spark.sql.SparkSession\n",
      "ro91\n",
      "ro92\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yro93\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "SparkSession\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.SparkSession\n",
      "setDefaultSession\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.SparkSession\n",
      "setDefaultSession\n",
      "ro93\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "u\n",
      "SparkSession\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.SparkSession\n",
      "setActiveSession\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.SparkSession\n",
      "setActiveSession\n",
      "ro93\n",
      "e\n",
      "\n",
      "25-05-18 03:52:24 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:24 - DEBUG - Command to send: c\n",
      "o93\n",
      "readStream\n",
      "e\n",
      "\n",
      "25/05/18 03:52:24 INFO  SharedState:60 Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/05/18 03:52:24 INFO  SharedState:60 Warehouse path is 'file:/opt/spark/warehouse'.\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o53\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o54\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o55\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o56\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o58\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o59\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o60\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o61\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o62\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o63\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o64\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o65\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o66\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o67\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o69\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o70\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o71\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o72\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o73\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o74\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o75\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o76\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o77\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o78\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o57\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: m\n",
      "d\n",
      "o92\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yro94\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: c\n",
      "o94\n",
      "format\n",
      "sjson\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yro95\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: r\n",
      "u\n",
      "SparkSession\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.SparkSession\n",
      "getActiveSession\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.SparkSession\n",
      "getActiveSession\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yro96\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: c\n",
      "o96\n",
      "isDefined\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: r\n",
      "u\n",
      "SparkSession\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.SparkSession\n",
      "getActiveSession\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.SparkSession\n",
      "getActiveSession\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yro97\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: c\n",
      "o97\n",
      "get\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yro98\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: r\n",
      "u\n",
      "SparkSession$\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession$\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.SparkSession$\n",
      "MODULE$\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yro99\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: i\n",
      "java.util.HashMap\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yao100\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: c\n",
      "o99\n",
      "applyModifiableSettings\n",
      "ro98\n",
      "ro100\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: c\n",
      "o95\n",
      "schema\n",
      "s\\n        InvoiceNumber string,\\n        CreatedTime bigint,\\n        StoreID string,\\n        PosID string,\\n        CashierID string,\\n        CustomerType string,\\n        CustomerCardNo string,\\n        TotalAmount double,\\n        NumberOfItems bigint,\\n        PaymentMethod string,\\n        TaxableAmount double,\\n        CGST double,\\n        SGST double,\\n        CESS double,\\n        DeliveryType string,\\n        DeliveryAddress struct<\\n            AddressLine string,\\n            City string,\\n            ContactNumber string,\\n            PinCode string,\\n            State string\\n        >,\\n        InvoiceLineItems array<\\n            struct<\\n                ItemCode string,\\n                ItemDescription string,\\n                ItemPrice double,\\n                ItemQty bigint,\\n                TotalValue double\\n            >\\n        >\\n    \n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yro101\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: c\n",
      "o101\n",
      "option\n",
      "scleanSource\n",
      "sarchive\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yro102\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: c\n",
      "o102\n",
      "option\n",
      "ssourceArchiveDir\n",
      "s/opt/spark/datasets/archive/invoices\n",
      "e\n",
      "\n",
      "25-05-18 03:52:25 - DEBUG - Answer received: !yro103\n",
      "25-05-18 03:52:25 - DEBUG - Command to send: c\n",
      "o103\n",
      "load\n",
      "s/opt/spark/datasets/invoices/*.json\n",
      "e\n",
      "\n",
      "25/05/18 03:52:26 INFO  InMemoryFileIndex:60 It took 46 ms to list leaf files for 4 paths.\n",
      "25-05-18 03:52:26 - DEBUG - Command to send: m\n",
      "d\n",
      "o100\n",
      "e\n",
      "\n",
      "25-05-18 03:52:26 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:26 - DEBUG - Answer received: !yro104\n",
      "25-05-18 03:52:26 - DEBUG - Command to send: c\n",
      "o104\n",
      "writeStream\n",
      "e\n",
      "\n",
      "25-05-18 03:52:26 - DEBUG - Answer received: !yro105\n",
      "25-05-18 03:52:26 - DEBUG - Command to send: c\n",
      "o105\n",
      "queryName\n",
      "singestion_bronze\n",
      "e\n",
      "\n",
      "25-05-18 03:52:26 - DEBUG - Answer received: !yro106\n",
      "25-05-18 03:52:26 - DEBUG - Command to send: c\n",
      "o106\n",
      "format\n",
      "sdelta\n",
      "e\n",
      "\n",
      "25-05-18 03:52:26 - DEBUG - Answer received: !yro107\n",
      "25-05-18 03:52:26 - DEBUG - Command to send: c\n",
      "o107\n",
      "option\n",
      "scheckpointLocation\n",
      "s/opt/spark/datasets/checkpoint/invoices/bronze\n",
      "e\n",
      "\n",
      "25-05-18 03:52:26 - DEBUG - Answer received: !yro108\n",
      "25-05-18 03:52:26 - DEBUG - Command to send: c\n",
      "o108\n",
      "outputMode\n",
      "sappend\n",
      "e\n",
      "\n",
      "25-05-18 03:52:26 - DEBUG - Answer received: !yro109\n",
      "25-05-18 03:52:26 - DEBUG - Command to send: c\n",
      "o109\n",
      "toTable\n",
      "sinvoices_bronze\n",
      "e\n",
      "\n",
      "25/05/18 03:52:26 INFO  HiveUtils:60 Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.\n",
      "25/05/18 03:52:26 INFO  HiveClientImpl:60 Warehouse location for Hive client (version 2.3.9) is file:/opt/spark/warehouse\n",
      "25/05/18 03:52:27 WARN  HiveConf:4122 HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/05/18 03:52:27 WARN  HiveConf:4122 HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/05/18 03:52:27 INFO  HiveMetaStore:614 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore\n",
      "25/05/18 03:52:27 INFO  ObjectStore:403 ObjectStore, initialize called\n",
      "25/05/18 03:52:27 INFO  Persistence:77 Property hive.metastore.integral.jdo.pushdown unknown - will be ignored\n",
      "25/05/18 03:52:27 INFO  Persistence:77 Property datanucleus.cache.level2 unknown - will be ignored\n",
      "25/05/18 03:52:41 INFO  ObjectStore:526 Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\"\n",
      "25/05/18 03:52:43 INFO  MetaStoreDirectSql:146 Using direct SQL, underlying DB is DERBY\n",
      "25/05/18 03:52:43 INFO  ObjectStore:317 Initialized ObjectStore\n",
      "25/05/18 03:52:43 WARN  ObjectStore:7812 Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "25/05/18 03:52:43 WARN  ObjectStore:7900 setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.18.0.2\n",
      "25/05/18 03:52:44 INFO  HiveMetaStore:698 Added admin role in metastore\n",
      "25/05/18 03:52:44 INFO  HiveMetaStore:707 Added public role in metastore\n",
      "25/05/18 03:52:44 INFO  HiveMetaStore:747 No user is added in admin role, since config is empty\n",
      "25/05/18 03:52:44 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:52:44 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25/05/18 03:52:44 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:52:44 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25/05/18 03:52:44 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_bronze\n",
      "25/05/18 03:52:44 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_bronze\t\n",
      "25/05/18 03:52:44 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:52:44 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25/05/18 03:52:44 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_bronze\n",
      "25/05/18 03:52:44 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_bronze\t\n",
      "25/05/18 03:52:45 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_bronze\n",
      "25/05/18 03:52:45 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_bronze\t\n",
      "25/05/18 03:52:45 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:52:45 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25/05/18 03:52:45 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:52:45 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25/05/18 03:52:45 INFO  DelegatingLogStore:95 LogStore LogStoreAdapter(io.delta.storage.HDFSLogStore) is used for scheme file\n",
      "25/05/18 03:52:45 INFO  DeltaLog:95 Creating initial snapshot without metadata, because the directory is empty\n",
      "25/05/18 03:52:45 INFO  DummySnapshot:95 [tableId=dae38c59-ce11-4b63-ac0e-3c6597917e9d] Created snapshot DummySnapshot(path=file:/opt/spark/warehouse/invoices_bronze/_delta_log, version=-1, metadata=Metadata(f5e82f2f-5f2b-45b2-bdd1-1b31c11e710f,null,null,Format(parquet,Map()),null,List(),Map(),Some(1747540365208)), logSegment=LogSegment(file:/opt/spark/warehouse/invoices_bronze/_delta_log,-1,List(),org.apache.spark.sql.delta.EmptyCheckpointProvider$@708cb3d6,-1), checksumOpt=None)\n",
      "25/05/18 03:52:45 INFO  DeltaLog:95 Creating initial snapshot without metadata, because the directory is empty\n",
      "25/05/18 03:52:45 INFO  DummySnapshot:95 [tableId=f5e82f2f-5f2b-45b2-bdd1-1b31c11e710f] Created snapshot DummySnapshot(path=file:/opt/spark/warehouse/invoices_bronze/_delta_log, version=-1, metadata=Metadata(8da4e8bd-6e7f-4652-866f-c6743380adc2,null,null,Format(parquet,Map()),null,List(),Map(),Some(1747540365300)), logSegment=LogSegment(file:/opt/spark/warehouse/invoices_bronze/_delta_log,-1,List(),org.apache.spark.sql.delta.EmptyCheckpointProvider$@708cb3d6,-1), checksumOpt=None)\n",
      "25/05/18 03:52:45 INFO  OptimisticTransaction:95 [tableId=8da4e8bd,txnId=92fa4f54] Updated metadata from - to Metadata(ee309d7e-debc-4830-b609-a41347a2dfcb,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"InvoiceNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CreatedTime\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"StoreID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PosID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CashierID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerCardNo\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalAmount\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"NumberOfItems\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PaymentMethod\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TaxableAmount\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CGST\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"SGST\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CESS\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryAddress\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"AddressLine\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ContactNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PinCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"State\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"InvoiceLineItems\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"ItemCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemDescription\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemPrice\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemQty\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalValue\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}}]},ArrayBuffer(),Map(),Some(1747540365360))\n",
      "25/05/18 03:52:45 INFO  OptimisticTransaction:95 [tableId=8da4e8bd,txnId=92fa4f54] Attempting to commit version 0 with 3 actions with Serializable isolation level\n",
      "25/05/18 03:52:45 INFO  OptimisticTransaction:95 Incremental commit: starting with snapshot version -1\n",
      "25/05/18 03:52:48 INFO  CodeGenerator:60 Code generated in 457.107433 ms\n",
      "25/05/18 03:52:48 INFO  SparkContext:60 Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128\n",
      "25/05/18 03:52:48 INFO  DAGScheduler:60 Job 0 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.007809 s\n",
      "25/05/18 03:52:48 INFO  DeltaLog:95 Creating a new snapshot v0 for commit version 0\n",
      "25/05/18 03:52:48 INFO  DeltaLog:95 Loading version 0.\n",
      "25/05/18 03:52:48 INFO  Snapshot:95 [tableId=8da4e8bd-6e7f-4652-866f-c6743380adc2] Created snapshot Snapshot(path=file:/opt/spark/warehouse/invoices_bronze/_delta_log, version=0, metadata=Metadata(ee309d7e-debc-4830-b609-a41347a2dfcb,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"InvoiceNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CreatedTime\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"StoreID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PosID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CashierID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerCardNo\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalAmount\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"NumberOfItems\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PaymentMethod\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TaxableAmount\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CGST\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"SGST\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CESS\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryAddress\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"AddressLine\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ContactNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PinCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"State\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"InvoiceLineItems\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"ItemCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemDescription\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemPrice\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemQty\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalValue\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}}]},ArrayBuffer(),Map(),Some(1747540365360)), logSegment=LogSegment(file:/opt/spark/warehouse/invoices_bronze/_delta_log,0,List(DeprecatedRawLocalFileStatus{path=file:/opt/spark/warehouse/invoices_bronze/_delta_log/00000000000000000000.json; isDirectory=false; length=2897; replication=1; blocksize=33554432; modification_time=1747540365415; access_time=1747540365415; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}),org.apache.spark.sql.delta.EmptyCheckpointProvider$@708cb3d6,1747540365415), checksumOpt=Some(VersionChecksum(Some(92fa4f54-1970-484a-8dd3-7c3664b26f95),0,0,None,None,1,1,None,Some(Stream()),Some(Stream()),Metadata(ee309d7e-debc-4830-b609-a41347a2dfcb,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"InvoiceNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CreatedTime\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"StoreID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PosID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CashierID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerCardNo\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalAmount\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"NumberOfItems\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PaymentMethod\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TaxableAmount\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CGST\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"SGST\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CESS\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryAddress\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"AddressLine\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ContactNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PinCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"State\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"InvoiceLineItems\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"ItemCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemDescription\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemPrice\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemQty\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalValue\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}}]},ArrayBuffer(),Map(),Some(1747540365360)),Protocol(1,2),None,None,Some(Stream()))))\n",
      "25/05/18 03:52:48 INFO  DeltaLog:95 Updated snapshot to Snapshot(path=file:/opt/spark/warehouse/invoices_bronze/_delta_log, version=0, metadata=Metadata(ee309d7e-debc-4830-b609-a41347a2dfcb,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"InvoiceNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CreatedTime\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"StoreID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PosID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CashierID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerCardNo\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalAmount\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"NumberOfItems\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PaymentMethod\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TaxableAmount\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CGST\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"SGST\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CESS\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryAddress\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"AddressLine\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ContactNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PinCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"State\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"InvoiceLineItems\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"ItemCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemDescription\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemPrice\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemQty\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalValue\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}}]},ArrayBuffer(),Map(),Some(1747540365360)), logSegment=LogSegment(file:/opt/spark/warehouse/invoices_bronze/_delta_log,0,List(DeprecatedRawLocalFileStatus{path=file:/opt/spark/warehouse/invoices_bronze/_delta_log/00000000000000000000.json; isDirectory=false; length=2897; replication=1; blocksize=33554432; modification_time=1747540365415; access_time=1747540365415; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}),org.apache.spark.sql.delta.EmptyCheckpointProvider$@708cb3d6,1747540365415), checksumOpt=Some(VersionChecksum(Some(92fa4f54-1970-484a-8dd3-7c3664b26f95),0,0,None,None,1,1,None,Some(Stream()),Some(Stream()),Metadata(ee309d7e-debc-4830-b609-a41347a2dfcb,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"InvoiceNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CreatedTime\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"StoreID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PosID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CashierID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerCardNo\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalAmount\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"NumberOfItems\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PaymentMethod\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TaxableAmount\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CGST\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"SGST\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CESS\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryAddress\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"AddressLine\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ContactNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PinCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"State\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"InvoiceLineItems\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"ItemCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemDescription\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemPrice\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemQty\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalValue\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}}]},ArrayBuffer(),Map(),Some(1747540365360)),Protocol(1,2),None,None,Some(Stream()))))\n",
      "25/05/18 03:52:48 INFO  DeltaLogFileIndex:95 Created DeltaLogFileIndex(JSON, numFilesInSegment: 1, totalFileSize: 2897)\n",
      "25/05/18 03:52:48 INFO  OptimisticTransaction:95 [tableId=8da4e8bd,txnId=92fa4f54] Committed delta #0 to file:/opt/spark/warehouse/invoices_bronze/_delta_log\n",
      "25/05/18 03:52:48 INFO  ChecksumHook:95 Writing checksum file for table path file:/opt/spark/warehouse/invoices_bronze/_delta_log version 0\n",
      "25/05/18 03:52:48 INFO  CheckpointFileManager:60 Writing atomically to file:/opt/spark/warehouse/invoices_bronze/_delta_log/00000000000000000000.crc using temp file file:/opt/spark/warehouse/invoices_bronze/_delta_log/.00000000000000000000.crc.9824ba4f-0733-4800-8819-e4212fca7fdc.tmp\n",
      "25/05/18 03:52:48 INFO  CheckpointFileManager:60 Renamed temp file file:/opt/spark/warehouse/invoices_bronze/_delta_log/.00000000000000000000.crc.9824ba4f-0733-4800-8819-e4212fca7fdc.tmp to file:/opt/spark/warehouse/invoices_bronze/_delta_log/00000000000000000000.crc\n",
      "25/05/18 03:52:48 INFO  CreateDeltaTableCommand:95 Table is path-based table: false. Update catalog with mode: Create\n",
      "25/05/18 03:52:48 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:52:48 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25/05/18 03:52:48 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_bronze\n",
      "25/05/18 03:52:48 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_bronze\t\n",
      "25/05/18 03:52:48 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:52:48 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25/05/18 03:52:48 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_bronze\n",
      "25/05/18 03:52:48 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_bronze\t\n",
      "25/05/18 03:52:48 WARN  HiveExternalCatalog:72 Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`default`.`invoices_bronze` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "25/05/18 03:52:48 INFO  SQLStdHiveAccessController:95 Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=61963330-953d-450c-a0e1-59fc5dc1a731, clientType=HIVECLI]\n",
      "25/05/18 03:52:48 WARN  SessionState:907 METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/05/18 03:52:48 INFO  metastore:313 Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook\n",
      "25/05/18 03:52:48 INFO  HiveMetaStore:781 0: Cleaning up thread local RawStore...\n",
      "25/05/18 03:52:48 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=Cleaning up thread local RawStore...\t\n",
      "25/05/18 03:52:48 INFO  HiveMetaStore:781 0: Done cleaning up thread local RawStore\n",
      "25/05/18 03:52:48 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=Done cleaning up thread local RawStore\t\n",
      "25/05/18 03:52:48 INFO  HiveMetaStore:781 0: create_table: Table(tableName:invoices_bronze, dbName:default, owner:root, createTime:1747540365, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:col, type:array<string>, comment:from deserializer)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{path=file:/opt/spark/warehouse/invoices_bronze, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema={\"type\":\"struct\",\"fields\":[]}, spark.sql.partitionProvider=catalog, spark.sql.sources.provider=delta, spark.sql.create.version=3.5.5}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{root=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:root, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))\n",
      "25/05/18 03:52:48 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=create_table: Table(tableName:invoices_bronze, dbName:default, owner:root, createTime:1747540365, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:col, type:array<string>, comment:from deserializer)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{path=file:/opt/spark/warehouse/invoices_bronze, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema={\"type\":\"struct\",\"fields\":[]}, spark.sql.partitionProvider=catalog, spark.sql.sources.provider=delta, spark.sql.create.version=3.5.5}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{root=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:root, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))\t\n",
      "25/05/18 03:52:48 WARN  HiveConf:4122 HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "25/05/18 03:52:48 WARN  HiveConf:4122 HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/05/18 03:52:48 WARN  HiveConf:4122 HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/05/18 03:52:48 INFO  HiveMetaStore:614 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore\n",
      "25/05/18 03:52:48 INFO  ObjectStore:403 ObjectStore, initialize called\n",
      "25/05/18 03:52:48 INFO  MetaStoreDirectSql:146 Using direct SQL, underlying DB is DERBY\n",
      "25/05/18 03:52:48 INFO  ObjectStore:317 Initialized ObjectStore\n",
      "25/05/18 03:52:48 INFO  log:242 Updating table stats fast for invoices_bronze\n",
      "25/05/18 03:52:48 INFO  log:244 Updated size of table invoices_bronze to 0\n",
      "25/05/18 03:52:48 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:52:48 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25/05/18 03:52:48 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_bronze\n",
      "25/05/18 03:52:48 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_bronze\t\n",
      "25/05/18 03:52:49 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_bronze\n",
      "25/05/18 03:52:49 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_bronze\t\n",
      "25/05/18 03:52:49 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:52:49 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25/05/18 03:52:49 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_bronze\n",
      "25/05/18 03:52:49 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_bronze\t\n",
      "25/05/18 03:52:49 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_bronze\n",
      "25/05/18 03:52:49 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_bronze\t\n",
      "25/05/18 03:52:49 INFO  StateStoreCoordinatorRef:60 Registered StateStoreCoordinator endpoint\n",
      "25/05/18 03:52:49 INFO  ResolveWriteToStream:60 Checkpoint root /opt/spark/datasets/checkpoint/invoices/bronze resolved to file:/opt/spark/datasets/checkpoint/invoices/bronze.\n",
      "25/05/18 03:52:49 WARN  ResolveWriteToStream:72 spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/18 03:52:49 INFO  CheckpointFileManager:60 Writing atomically to file:/opt/spark/datasets/checkpoint/invoices/bronze/metadata using temp file file:/opt/spark/datasets/checkpoint/invoices/bronze/.metadata.401b311e-c328-4b20-8d4b-c1e48efc3deb.tmp\n",
      "25/05/18 03:52:49 INFO  CheckpointFileManager:60 Renamed temp file file:/opt/spark/datasets/checkpoint/invoices/bronze/.metadata.401b311e-c328-4b20-8d4b-c1e48efc3deb.tmp to file:/opt/spark/datasets/checkpoint/invoices/bronze/metadata\n",
      "25/05/18 03:52:49 INFO  MicroBatchExecution:60 Starting ingestion_bronze [id = 83a64f49-3f9b-4e05-9cdb-ee5c08becb51, runId = fe3b5e89-9f87-4f30-91ff-8c7c4a51b2fe]. Use file:/opt/spark/datasets/checkpoint/invoices/bronze to store the query checkpoint.\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !yro110\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: r\n",
      "u\n",
      "PythonUtils\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.api.python.PythonUtils\n",
      "toSeq\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: i\n",
      "java.util.ArrayList\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ylo111\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o111\n",
      "add\n",
      "sInvoiceNumber\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o111\n",
      "add\n",
      "sCreatedTime\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o111\n",
      "add\n",
      "sStoreID\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o111\n",
      "add\n",
      "sPosID\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o111\n",
      "add\n",
      "sCustomerType\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o111\n",
      "add\n",
      "sPaymentMethod\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o111\n",
      "add\n",
      "sDeliveryType\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o111\n",
      "add\n",
      "sDeliveryAddress.City\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o111\n",
      "add\n",
      "sDeliveryAddress.PinCode\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o111\n",
      "add\n",
      "sDeliveryAddress.State\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o111\n",
      "add\n",
      "sexplode(InvoiceLineItems) AS LineItem\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.api.python.PythonUtils\n",
      "toSeq\n",
      "ro111\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !yro112\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o104\n",
      "selectExpr\n",
      "ro112\n",
      "e\n",
      "\n",
      "25/05/18 03:52:49 INFO  FileStreamSourceLog:60 Set the compact interval to 10 [defaultCompactInterval: 10]\n",
      "25/05/18 03:52:49 INFO  FileStreamSourceLog:60 BatchIds found from listing: \n",
      "25/05/18 03:52:49 INFO  FileStreamSourceLog:60 BatchIds found from listing: \n",
      "25/05/18 03:52:49 INFO  FileStreamSource:60 maxFilesPerBatch = None, maxFileAgeMs = 604800000\n",
      "25/05/18 03:52:49 INFO  MicroBatchExecution:60 Using Source [FileStreamSource[file:/opt/spark/datasets/invoices/*.json]] from DataSourceV1 named 'FileSource[/opt/spark/datasets/invoices/*.json]' [DataSource(org.apache.spark.sql.SparkSession@2e0afea5,json,List(),Some(StructType(StructField(InvoiceNumber,StringType,true),StructField(CreatedTime,LongType,true),StructField(StoreID,StringType,true),StructField(PosID,StringType,true),StructField(CashierID,StringType,true),StructField(CustomerType,StringType,true),StructField(CustomerCardNo,StringType,true),StructField(TotalAmount,DoubleType,true),StructField(NumberOfItems,LongType,true),StructField(PaymentMethod,StringType,true),StructField(TaxableAmount,DoubleType,true),StructField(CGST,DoubleType,true),StructField(SGST,DoubleType,true),StructField(CESS,DoubleType,true),StructField(DeliveryType,StringType,true),StructField(DeliveryAddress,StructType(StructField(AddressLine,StringType,true),StructField(City,StringType,true),StructField(ContactNumber,StringType,true),StructField(PinCode,StringType,true),StructField(State,StringType,true)),true),StructField(InvoiceLineItems,ArrayType(StructType(StructField(ItemCode,StringType,true),StructField(ItemDescription,StringType,true),StructField(ItemPrice,DoubleType,true),StructField(ItemQty,LongType,true),StructField(TotalValue,DoubleType,true)),true),true))),List(),None,Map(cleanSource -> archive, sourceArchiveDir -> /opt/spark/datasets/archive/invoices, path -> /opt/spark/datasets/invoices/*.json),None)]\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !yro113\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: r\n",
      "u\n",
      "functions\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ycorg.apache.spark.sql.functions\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.functions\n",
      "expr\n",
      "e\n",
      "\n",
      "25/05/18 03:52:49 INFO  OffsetSeqLog:60 BatchIds found from listing: \n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.functions\n",
      "expr\n",
      "sLineItem.ItemCode\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !yro114\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o113\n",
      "withColumn\n",
      "sItemCode\n",
      "ro114\n",
      "e\n",
      "\n",
      "25/05/18 03:52:49 INFO  OffsetSeqLog:60 BatchIds found from listing: \n",
      "25/05/18 03:52:49 INFO  MicroBatchExecution:60 Starting new streaming query.\n",
      "25/05/18 03:52:49 INFO  MicroBatchExecution:60 Stream started from {}\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !yro115\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: r\n",
      "u\n",
      "functions\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ycorg.apache.spark.sql.functions\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.functions\n",
      "expr\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.functions\n",
      "expr\n",
      "sLineItem.ItemDescription\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !yro116\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o115\n",
      "withColumn\n",
      "sItemDescription\n",
      "ro116\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !yro117\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: r\n",
      "u\n",
      "functions\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ycorg.apache.spark.sql.functions\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.functions\n",
      "expr\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.functions\n",
      "expr\n",
      "sLineItem.ItemPrice\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !yro118\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o117\n",
      "withColumn\n",
      "sItemPrice\n",
      "ro118\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !yro119\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: r\n",
      "u\n",
      "functions\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ycorg.apache.spark.sql.functions\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.functions\n",
      "expr\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.functions\n",
      "expr\n",
      "sLineItem.ItemQty\n",
      "e\n",
      "\n",
      "25/05/18 03:52:49 INFO  InMemoryFileIndex:60 It took 22 ms to list leaf files for 4 paths.\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !yro120\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o119\n",
      "withColumn\n",
      "sItemQty\n",
      "ro120\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !yro121\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: r\n",
      "u\n",
      "functions\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ycorg.apache.spark.sql.functions\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.functions\n",
      "expr\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.functions\n",
      "expr\n",
      "sLineItem.TotalValue\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !yro122\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o121\n",
      "withColumn\n",
      "sTotalValue\n",
      "ro122\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !yro123\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: r\n",
      "u\n",
      "PythonUtils\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.api.python.PythonUtils\n",
      "toSeq\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: i\n",
      "java.util.ArrayList\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ylo124\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o124\n",
      "add\n",
      "sLineItem\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.api.python.PythonUtils\n",
      "toSeq\n",
      "ro124\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !yro125\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o123\n",
      "drop\n",
      "ro125\n",
      "e\n",
      "\n",
      "25/05/18 03:52:49 INFO  CheckpointFileManager:60 Writing atomically to file:/opt/spark/datasets/checkpoint/invoices/bronze/sources/0/0 using temp file file:/opt/spark/datasets/checkpoint/invoices/bronze/sources/0/.0.8ebd6958-227e-4205-8b9a-2b584c426573.tmp\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !yro126\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o126\n",
      "writeStream\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !yro127\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o127\n",
      "queryName\n",
      "singestion_silver\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !yro128\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o128\n",
      "format\n",
      "sdelta\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !yro129\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o129\n",
      "option\n",
      "scheckpointLocation\n",
      "s/opt/spark/datasets/checkpoint/invoices/silver\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !yro130\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o130\n",
      "outputMode\n",
      "sappend\n",
      "e\n",
      "\n",
      "25-05-18 03:52:49 - DEBUG - Answer received: !yro131\n",
      "25-05-18 03:52:49 - DEBUG - Command to send: c\n",
      "o131\n",
      "toTable\n",
      "sinvoices_silver\n",
      "e\n",
      "\n",
      "25/05/18 03:52:49 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:52:49 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25/05/18 03:52:49 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_silver\n",
      "25/05/18 03:52:49 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_silver\t\n",
      "25/05/18 03:52:49 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:52:49 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25/05/18 03:52:49 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_silver\n",
      "25/05/18 03:52:49 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_silver\t\n",
      "25/05/18 03:52:49 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_silver\n",
      "25/05/18 03:52:49 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_silver\t\n",
      "25/05/18 03:52:49 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:52:49 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25/05/18 03:52:49 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:52:49 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25/05/18 03:52:49 INFO  DelegatingLogStore:95 LogStore LogStoreAdapter(io.delta.storage.HDFSLogStore) is used for scheme file\n",
      "25/05/18 03:52:49 INFO  DeltaLog:95 Creating initial snapshot without metadata, because the directory is empty\n",
      "25/05/18 03:52:49 INFO  DummySnapshot:95 [tableId=c28b53bf-a2c6-432f-897d-67feae18656a] Created snapshot DummySnapshot(path=file:/opt/spark/warehouse/invoices_silver/_delta_log, version=-1, metadata=Metadata(1430842e-c11f-4cae-bec8-6231254c34d3,null,null,Format(parquet,Map()),null,List(),Map(),Some(1747540369681)), logSegment=LogSegment(file:/opt/spark/warehouse/invoices_silver/_delta_log,-1,List(),org.apache.spark.sql.delta.EmptyCheckpointProvider$@708cb3d6,-1), checksumOpt=None)\n",
      "25/05/18 03:52:49 INFO  DeltaLog:95 Creating initial snapshot without metadata, because the directory is empty\n",
      "25/05/18 03:52:49 INFO  DummySnapshot:95 [tableId=1430842e-c11f-4cae-bec8-6231254c34d3] Created snapshot DummySnapshot(path=file:/opt/spark/warehouse/invoices_silver/_delta_log, version=-1, metadata=Metadata(5854e7d9-58c9-4f32-bdcd-20543e674ab2,null,null,Format(parquet,Map()),null,List(),Map(),Some(1747540369688)), logSegment=LogSegment(file:/opt/spark/warehouse/invoices_silver/_delta_log,-1,List(),org.apache.spark.sql.delta.EmptyCheckpointProvider$@708cb3d6,-1), checksumOpt=None)\n",
      "25/05/18 03:52:49 INFO  OptimisticTransaction:95 [tableId=5854e7d9,txnId=b2a1dd9c] Updated metadata from - to Metadata(32fdf004-f6a4-43c3-9bb1-76c5a254b3a1,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"InvoiceNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CreatedTime\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"StoreID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PosID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PaymentMethod\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PinCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"State\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemDescription\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemPrice\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemQty\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalValue\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},ArrayBuffer(),Map(),Some(1747540369694))\n",
      "25/05/18 03:52:49 INFO  OptimisticTransaction:95 [tableId=5854e7d9,txnId=b2a1dd9c] Attempting to commit version 0 with 3 actions with Serializable isolation level\n",
      "25/05/18 03:52:49 INFO  CheckpointFileManager:60 Renamed temp file file:/opt/spark/datasets/checkpoint/invoices/bronze/sources/0/.0.8ebd6958-227e-4205-8b9a-2b584c426573.tmp to file:/opt/spark/datasets/checkpoint/invoices/bronze/sources/0/0\n",
      "25/05/18 03:52:49 INFO  FileStreamSource:60 Log offset set to 0 with 4 new files\n",
      "25/05/18 03:52:49 INFO  CheckpointFileManager:60 Writing atomically to file:/opt/spark/datasets/checkpoint/invoices/bronze/offsets/0 using temp file file:/opt/spark/datasets/checkpoint/invoices/bronze/offsets/.0.1b3ad771-71e6-4a1d-922a-52e10cc75351.tmp\n",
      "25/05/18 03:52:49 INFO  OptimisticTransaction:95 Incremental commit: starting with snapshot version -1\n",
      "25/05/18 03:52:49 INFO  CheckpointFileManager:60 Renamed temp file file:/opt/spark/datasets/checkpoint/invoices/bronze/offsets/.0.1b3ad771-71e6-4a1d-922a-52e10cc75351.tmp to file:/opt/spark/datasets/checkpoint/invoices/bronze/offsets/0\n",
      "25/05/18 03:52:49 INFO  MicroBatchExecution:60 Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1747540369767,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "25/05/18 03:52:49 INFO  FileStreamSource:60 Processing 4 files from 0:0\n",
      "25/05/18 03:52:50 INFO  InMemoryFileIndex:60 It took 22 ms to list leaf files for 4 paths.\n",
      "25/05/18 03:52:50 INFO  SparkContext:60 Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128\n",
      "25/05/18 03:52:50 INFO  DAGScheduler:60 Job 1 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.000371 s\n",
      "25/05/18 03:52:50 INFO  DeltaLog:95 Creating a new snapshot v0 for commit version 0\n",
      "25/05/18 03:52:50 INFO  DeltaLog:95 Loading version 0.\n",
      "25/05/18 03:52:50 INFO  Snapshot:95 [tableId=5854e7d9-58c9-4f32-bdcd-20543e674ab2] Created snapshot Snapshot(path=file:/opt/spark/warehouse/invoices_silver/_delta_log, version=0, metadata=Metadata(32fdf004-f6a4-43c3-9bb1-76c5a254b3a1,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"InvoiceNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CreatedTime\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"StoreID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PosID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PaymentMethod\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PinCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"State\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemDescription\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemPrice\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemQty\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalValue\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},ArrayBuffer(),Map(),Some(1747540369694)), logSegment=LogSegment(file:/opt/spark/warehouse/invoices_silver/_delta_log,0,List(DeprecatedRawLocalFileStatus{path=file:/opt/spark/warehouse/invoices_silver/_delta_log/00000000000000000000.json; isDirectory=false; length=1825; replication=1; blocksize=33554432; modification_time=1747540369347; access_time=1747540369347; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}),org.apache.spark.sql.delta.EmptyCheckpointProvider$@708cb3d6,1747540369347), checksumOpt=Some(VersionChecksum(Some(b2a1dd9c-2799-485c-8e22-c703b61e4015),0,0,None,None,1,1,None,Some(Stream()),Some(Stream()),Metadata(32fdf004-f6a4-43c3-9bb1-76c5a254b3a1,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"InvoiceNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CreatedTime\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"StoreID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PosID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PaymentMethod\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PinCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"State\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemDescription\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemPrice\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemQty\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalValue\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},ArrayBuffer(),Map(),Some(1747540369694)),Protocol(1,2),None,None,Some(Stream()))))\n",
      "25/05/18 03:52:50 INFO  DeltaLog:95 Updated snapshot to Snapshot(path=file:/opt/spark/warehouse/invoices_silver/_delta_log, version=0, metadata=Metadata(32fdf004-f6a4-43c3-9bb1-76c5a254b3a1,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"InvoiceNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CreatedTime\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"StoreID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PosID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PaymentMethod\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PinCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"State\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemDescription\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemPrice\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemQty\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalValue\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},ArrayBuffer(),Map(),Some(1747540369694)), logSegment=LogSegment(file:/opt/spark/warehouse/invoices_silver/_delta_log,0,List(DeprecatedRawLocalFileStatus{path=file:/opt/spark/warehouse/invoices_silver/_delta_log/00000000000000000000.json; isDirectory=false; length=1825; replication=1; blocksize=33554432; modification_time=1747540369347; access_time=1747540369347; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}),org.apache.spark.sql.delta.EmptyCheckpointProvider$@708cb3d6,1747540369347), checksumOpt=Some(VersionChecksum(Some(b2a1dd9c-2799-485c-8e22-c703b61e4015),0,0,None,None,1,1,None,Some(Stream()),Some(Stream()),Metadata(32fdf004-f6a4-43c3-9bb1-76c5a254b3a1,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"InvoiceNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CreatedTime\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"StoreID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PosID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PaymentMethod\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PinCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"State\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemDescription\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemPrice\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemQty\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalValue\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},ArrayBuffer(),Map(),Some(1747540369694)),Protocol(1,2),None,None,Some(Stream()))))\n",
      "25/05/18 03:52:50 INFO  DeltaLogFileIndex:95 Created DeltaLogFileIndex(JSON, numFilesInSegment: 1, totalFileSize: 1825)\n",
      "25/05/18 03:52:50 INFO  OptimisticTransaction:95 [tableId=5854e7d9,txnId=b2a1dd9c] Committed delta #0 to file:/opt/spark/warehouse/invoices_silver/_delta_log\n",
      "25/05/18 03:52:50 INFO  ChecksumHook:95 Writing checksum file for table path file:/opt/spark/warehouse/invoices_silver/_delta_log version 0\n",
      "25/05/18 03:52:50 INFO  CheckpointFileManager:60 Writing atomically to file:/opt/spark/warehouse/invoices_silver/_delta_log/00000000000000000000.crc using temp file file:/opt/spark/warehouse/invoices_silver/_delta_log/.00000000000000000000.crc.e05876b4-04ac-4c1b-a46f-b8dd60871847.tmp\n",
      "25-05-18 03:52:50 - DEBUG - Command to send: m\n",
      "d\n",
      "o111\n",
      "e\n",
      "\n",
      "25-05-18 03:52:50 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:52:50 - DEBUG - Command to send: m\n",
      "d\n",
      "o124\n",
      "e\n",
      "\n",
      "25-05-18 03:52:50 - DEBUG - Answer received: !yv\n",
      "25/05/18 03:52:50 INFO  FileSourceStrategy:60 Pushed Filters: \n",
      "25/05/18 03:52:50 INFO  FileSourceStrategy:60 Post-Scan Filters: \n",
      "25/05/18 03:52:50 INFO  FileSourceStrategy:60 Pushed Filters: \n",
      "25/05/18 03:52:50 INFO  FileSourceStrategy:60 Post-Scan Filters: \n",
      "25/05/18 03:52:50 INFO  CheckpointFileManager:60 Renamed temp file file:/opt/spark/warehouse/invoices_silver/_delta_log/.00000000000000000000.crc.e05876b4-04ac-4c1b-a46f-b8dd60871847.tmp to file:/opt/spark/warehouse/invoices_silver/_delta_log/00000000000000000000.crc\n",
      "25/05/18 03:52:50 INFO  CreateDeltaTableCommand:95 Table is path-based table: false. Update catalog with mode: Create\n",
      "25/05/18 03:52:50 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:52:50 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25/05/18 03:52:50 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_silver\n",
      "25/05/18 03:52:50 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_silver\t\n",
      "25/05/18 03:52:50 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:52:50 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25/05/18 03:52:50 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_silver\n",
      "25/05/18 03:52:50 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_silver\t\n",
      "25/05/18 03:52:50 WARN  HiveExternalCatalog:72 Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`default`.`invoices_silver` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "25/05/18 03:52:50 INFO  HiveMetaStore:781 0: create_table: Table(tableName:invoices_silver, dbName:default, owner:root, createTime:1747540369, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:col, type:array<string>, comment:from deserializer)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{path=file:/opt/spark/warehouse/invoices_silver, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema={\"type\":\"struct\",\"fields\":[]}, spark.sql.partitionProvider=catalog, spark.sql.sources.provider=delta, spark.sql.create.version=3.5.5}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{root=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:root, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))\n",
      "25/05/18 03:52:50 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=create_table: Table(tableName:invoices_silver, dbName:default, owner:root, createTime:1747540369, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:col, type:array<string>, comment:from deserializer)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{path=file:/opt/spark/warehouse/invoices_silver, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema={\"type\":\"struct\",\"fields\":[]}, spark.sql.partitionProvider=catalog, spark.sql.sources.provider=delta, spark.sql.create.version=3.5.5}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{root=[PrivilegeGrantInfo(privilege:INSERT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:SELECT, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:UPDATE, createTime:-1, grantor:root, grantorType:USER, grantOption:true), PrivilegeGrantInfo(privilege:DELETE, createTime:-1, grantor:root, grantorType:USER, grantOption:true)]}, groupPrivileges:null, rolePrivileges:null))\t\n",
      "25/05/18 03:52:50 INFO  log:242 Updating table stats fast for invoices_silver\n",
      "25/05/18 03:52:50 INFO  log:244 Updated size of table invoices_silver to 0\n",
      "25/05/18 03:52:50 INFO  Snapshot:95 DELTA: Compute snapshot for version: 0\n",
      "25/05/18 03:52:50 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:52:50 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25/05/18 03:52:50 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_silver\n",
      "25/05/18 03:52:50 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_silver\t\n",
      "25/05/18 03:52:50 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_silver\n",
      "25/05/18 03:52:50 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_silver\t\n",
      "25/05/18 03:52:50 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:52:50 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25/05/18 03:52:50 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_silver\n",
      "25/05/18 03:52:50 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_silver\t\n",
      "25/05/18 03:52:50 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_silver\n",
      "25/05/18 03:52:50 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_silver\t\n",
      "25/05/18 03:52:50 INFO  MemoryStore:60 Block broadcast_0 stored as values in memory (estimated size 203.5 KiB, free 434.2 MiB)\n",
      "25/05/18 03:52:50 INFO  ResolveWriteToStream:60 Checkpoint root /opt/spark/datasets/checkpoint/invoices/silver resolved to file:/opt/spark/datasets/checkpoint/invoices/silver.\n",
      "25/05/18 03:52:50 WARN  ResolveWriteToStream:72 spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/18 03:52:50 INFO  CheckpointFileManager:60 Writing atomically to file:/opt/spark/datasets/checkpoint/invoices/silver/metadata using temp file file:/opt/spark/datasets/checkpoint/invoices/silver/.metadata.bf3c3108-a100-4525-ac2f-c78abb732a32.tmp\n",
      "25/05/18 03:52:50 INFO  CheckpointFileManager:60 Renamed temp file file:/opt/spark/datasets/checkpoint/invoices/silver/.metadata.bf3c3108-a100-4525-ac2f-c78abb732a32.tmp to file:/opt/spark/datasets/checkpoint/invoices/silver/metadata\n",
      "25/05/18 03:52:50 INFO  MicroBatchExecution:60 Starting ingestion_silver [id = 19d65685-e892-4d3b-bba8-696452481d47, runId = 964e2622-2dce-493b-8690-71950de39ea4]. Use file:/opt/spark/datasets/checkpoint/invoices/silver to store the query checkpoint.\n",
      "25-05-18 03:52:50 - DEBUG - Answer received: !yro132\n",
      "25/05/18 03:52:50 INFO  FileStreamSourceLog:60 Set the compact interval to 10 [defaultCompactInterval: 10]\n",
      "25/05/18 03:52:50 INFO  FileStreamSourceLog:60 BatchIds found from listing: \n",
      "25/05/18 03:52:50 INFO  FileStreamSourceLog:60 BatchIds found from listing: \n",
      "25/05/18 03:52:50 INFO  FileStreamSource:60 maxFilesPerBatch = None, maxFileAgeMs = 604800000\n",
      "25/05/18 03:52:50 INFO  MicroBatchExecution:60 Using Source [FileStreamSource[file:/opt/spark/datasets/invoices/*.json]] from DataSourceV1 named 'FileSource[/opt/spark/datasets/invoices/*.json]' [DataSource(org.apache.spark.sql.SparkSession@2e0afea5,json,List(),Some(StructType(StructField(InvoiceNumber,StringType,true),StructField(CreatedTime,LongType,true),StructField(StoreID,StringType,true),StructField(PosID,StringType,true),StructField(CashierID,StringType,true),StructField(CustomerType,StringType,true),StructField(CustomerCardNo,StringType,true),StructField(TotalAmount,DoubleType,true),StructField(NumberOfItems,LongType,true),StructField(PaymentMethod,StringType,true),StructField(TaxableAmount,DoubleType,true),StructField(CGST,DoubleType,true),StructField(SGST,DoubleType,true),StructField(CESS,DoubleType,true),StructField(DeliveryType,StringType,true),StructField(DeliveryAddress,StructType(StructField(AddressLine,StringType,true),StructField(City,StringType,true),StructField(ContactNumber,StringType,true),StructField(PinCode,StringType,true),StructField(State,StringType,true)),true),StructField(InvoiceLineItems,ArrayType(StructType(StructField(ItemCode,StringType,true),StructField(ItemDescription,StringType,true),StructField(ItemPrice,DoubleType,true),StructField(ItemQty,LongType,true),StructField(TotalValue,DoubleType,true)),true),true))),List(),None,Map(cleanSource -> archive, sourceArchiveDir -> /opt/spark/datasets/archive/invoices, path -> /opt/spark/datasets/invoices/*.json),None)]\n",
      "25/05/18 03:52:50 INFO  OffsetSeqLog:60 BatchIds found from listing: \n",
      "25/05/18 03:52:50 INFO  MemoryStore:60 Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.4 KiB, free 434.2 MiB)\n",
      "25/05/18 03:52:50 INFO  OffsetSeqLog:60 BatchIds found from listing: \n",
      "25/05/18 03:52:50 INFO  MicroBatchExecution:60 Starting new streaming query.\n",
      "25/05/18 03:52:50 INFO  MicroBatchExecution:60 Stream started from {}\n",
      "25/05/18 03:52:50 INFO  BlockManagerInfo:60 Added broadcast_0_piece0 in memory on 7db080dfe9d2:43279 (size: 35.4 KiB, free: 434.4 MiB)\n",
      "25/05/18 03:52:50 INFO  SparkContext:60 Created broadcast 0 from toTable at NativeMethodAccessorImpl.java:0\n",
      "25/05/18 03:52:50 INFO  InMemoryFileIndex:60 It took 22 ms to list leaf files for 4 paths.\n",
      "25/05/18 03:52:50 INFO  CheckpointFileManager:60 Writing atomically to file:/opt/spark/datasets/checkpoint/invoices/silver/sources/0/0 using temp file file:/opt/spark/datasets/checkpoint/invoices/silver/sources/0/.0.37ad327d-114f-4caa-a30f-353b1c7803e4.tmp\n",
      "25/05/18 03:52:49 INFO  CheckpointFileManager:60 Renamed temp file file:/opt/spark/datasets/checkpoint/invoices/silver/sources/0/.0.37ad327d-114f-4caa-a30f-353b1c7803e4.tmp to file:/opt/spark/datasets/checkpoint/invoices/silver/sources/0/0\n",
      "25/05/18 03:52:49 INFO  FileStreamSource:60 Log offset set to 0 with 4 new files\n",
      "25/05/18 03:52:49 INFO  CheckpointFileManager:60 Writing atomically to file:/opt/spark/datasets/checkpoint/invoices/silver/offsets/0 using temp file file:/opt/spark/datasets/checkpoint/invoices/silver/offsets/.0.f253ce5c-ccf2-496e-9114-7f1dda9da19c.tmp\n",
      "25/05/18 03:52:49 INFO  DataSourceStrategy:60 Pruning directories with: \n",
      "25/05/18 03:52:49 INFO  FileSourceStrategy:60 Pushed Filters: \n",
      "25/05/18 03:52:49 INFO  FileSourceStrategy:60 Post-Scan Filters: \n",
      "25/05/18 03:52:49 WARN  SparkStringUtils:72 Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/05/18 03:52:50 INFO  CheckpointFileManager:60 Renamed temp file file:/opt/spark/datasets/checkpoint/invoices/silver/offsets/.0.f253ce5c-ccf2-496e-9114-7f1dda9da19c.tmp to file:/opt/spark/datasets/checkpoint/invoices/silver/offsets/0\n",
      "25/05/18 03:52:50 INFO  MicroBatchExecution:60 Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1747540369851,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
      "25/05/18 03:52:50 INFO  FileStreamSource:60 Processing 4 files from 0:0\n",
      "25/05/18 03:52:50 INFO  InMemoryFileIndex:60 It took 24 ms to list leaf files for 4 paths.\n",
      "25/05/18 03:52:50 INFO  FileSourceStrategy:60 Pushed Filters: IsNotNull(InvoiceLineItems)\n",
      "25/05/18 03:52:50 INFO  FileSourceStrategy:60 Post-Scan Filters: (size(InvoiceLineItems#661, true) > 0),isnotnull(InvoiceLineItems#661)\n",
      "25/05/18 03:52:50 INFO  FileSourceStrategy:60 Pushed Filters: IsNotNull(InvoiceLineItems)\n",
      "25/05/18 03:52:50 INFO  FileSourceStrategy:60 Post-Scan Filters: (size(InvoiceLineItems#661, true) > 0),isnotnull(InvoiceLineItems#661)\n",
      "25/05/18 03:52:50 INFO  Snapshot:95 DELTA: Compute snapshot for version: 0\n",
      "25/05/18 03:52:50 INFO  MemoryStore:60 Block broadcast_1 stored as values in memory (estimated size 203.5 KiB, free 434.0 MiB)\n",
      "25/05/18 03:52:50 INFO  MemoryStore:60 Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.4 KiB, free 433.9 MiB)\n",
      "25/05/18 03:52:50 INFO  BlockManagerInfo:60 Added broadcast_1_piece0 in memory on 7db080dfe9d2:43279 (size: 35.4 KiB, free: 434.3 MiB)\n",
      "25/05/18 03:52:50 INFO  SparkContext:60 Created broadcast 1 from toTable at NativeMethodAccessorImpl.java:0\n",
      "25/05/18 03:52:50 INFO  CodeGenerator:60 Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.serializefromobject_doConsume_0$ is 25072 bytes\n",
      "25/05/18 03:52:50 INFO  CodeGenerator:60 Code generated in 387.392766 ms\n",
      "25/05/18 03:52:50 INFO  DataSourceStrategy:60 Pruning directories with: \n",
      "25/05/18 03:52:50 INFO  FileSourceStrategy:60 Pushed Filters: \n",
      "25/05/18 03:52:50 INFO  FileSourceStrategy:60 Post-Scan Filters: \n",
      "25/05/18 03:52:50 INFO  CodeGenerator:60 Code generated in 110.503047 ms\n",
      "25/05/18 03:52:50 INFO  CodeGenerator:60 Code generated in 79.79696 ms\n",
      "25/05/18 03:52:50 INFO  MemoryStore:60 Block broadcast_3 stored as values in memory (estimated size 203.8 KiB, free 433.7 MiB)\n",
      "25/05/18 03:52:50 INFO  MemoryStore:60 Block broadcast_2 stored as values in memory (estimated size 203.8 KiB, free 433.5 MiB)\n",
      "25/05/18 03:52:50 INFO  MemoryStore:60 Block broadcast_3_piece0 stored as bytes in memory (estimated size 35.4 KiB, free 433.5 MiB)\n",
      "25/05/18 03:52:50 INFO  BlockManagerInfo:60 Added broadcast_3_piece0 in memory on 7db080dfe9d2:43279 (size: 35.4 KiB, free: 434.3 MiB)\n",
      "25/05/18 03:52:50 INFO  SparkContext:60 Created broadcast 3 from toTable at NativeMethodAccessorImpl.java:0\n",
      "25/05/18 03:52:50 INFO  MemoryStore:60 Block broadcast_2_piece0 stored as bytes in memory (estimated size 35.4 KiB, free 433.5 MiB)\n",
      "25/05/18 03:52:50 INFO  BlockManagerInfo:60 Added broadcast_2_piece0 in memory on 7db080dfe9d2:43279 (size: 35.4 KiB, free: 434.3 MiB)\n",
      "25/05/18 03:52:50 INFO  SparkContext:60 Created broadcast 2 from toTable at NativeMethodAccessorImpl.java:0\n",
      "25/05/18 03:52:50 INFO  FileSourceScanExec:60 Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/05/18 03:52:50 INFO  FileSourceScanExec:60 Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/05/18 03:52:51 INFO  CodeGenerator:60 Code generated in 74.93856 ms\n",
      "25/05/18 03:52:51 INFO  SparkContext:60 Starting job: toTable at NativeMethodAccessorImpl.java:0\n",
      "25/05/18 03:52:51 INFO  SparkContext:60 Starting job: toTable at NativeMethodAccessorImpl.java:0\n",
      "25/05/18 03:52:51 INFO  DAGScheduler:60 Registering RDD 15 (toTable at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "25/05/18 03:52:51 INFO  DAGScheduler:60 Registering RDD 34 (toTable at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
      "25/05/18 03:52:51 INFO  DAGScheduler:60 Got job 3 (toTable at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/18 03:52:51 INFO  DAGScheduler:60 Final stage: ResultStage 2 (toTable at NativeMethodAccessorImpl.java:0)\n",
      "25/05/18 03:52:51 INFO  DAGScheduler:60 Parents of final stage: List(ShuffleMapStage 1)\n",
      "25/05/18 03:52:51 INFO  DAGScheduler:60 Missing parents: List(ShuffleMapStage 1)\n",
      "25/05/18 03:52:51 INFO  DAGScheduler:60 Submitting ShuffleMapStage 0 (MapPartitionsRDD[15] at toTable at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/18 03:52:51 INFO  MemoryStore:60 Block broadcast_4 stored as values in memory (estimated size 106.0 KiB, free 433.4 MiB)\n",
      "25/05/18 03:52:51 INFO  MemoryStore:60 Block broadcast_4_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 433.3 MiB)\n",
      "25/05/18 03:52:51 INFO  BlockManagerInfo:60 Added broadcast_4_piece0 in memory on 7db080dfe9d2:43279 (size: 32.7 KiB, free: 434.2 MiB)\n",
      "25/05/18 03:52:51 INFO  SparkContext:60 Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/18 03:52:51 INFO  DAGScheduler:60 Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[15] at toTable at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/18 03:52:51 INFO  TaskSchedulerImpl:60 Adding task set 0.0 with 1 tasks resource profile 0\n",
      "25/05/18 03:52:51 INFO  DAGScheduler:60 Registering RDD 14 (toTable at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "25/05/18 03:52:51 INFO  DAGScheduler:60 Registering RDD 35 (toTable at NativeMethodAccessorImpl.java:0) as input to shuffle 3\n",
      "25/05/18 03:52:51 INFO  DAGScheduler:60 Got job 2 (toTable at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/18 03:52:51 INFO  DAGScheduler:60 Final stage: ResultStage 5 (toTable at NativeMethodAccessorImpl.java:0)\n",
      "25/05/18 03:52:51 INFO  DAGScheduler:60 Parents of final stage: List(ShuffleMapStage 4)\n",
      "25/05/18 03:52:51 INFO  DAGScheduler:60 Missing parents: List(ShuffleMapStage 4)\n",
      "25/05/18 03:52:51 INFO  DAGScheduler:60 Submitting ShuffleMapStage 3 (MapPartitionsRDD[14] at toTable at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/18 03:52:51 INFO  MemoryStore:60 Block broadcast_5 stored as values in memory (estimated size 106.0 KiB, free 433.2 MiB)\n",
      "25/05/18 03:52:51 INFO  MemoryStore:60 Block broadcast_5_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 433.2 MiB)\n",
      "25/05/18 03:52:51 INFO  BlockManagerInfo:60 Added broadcast_5_piece0 in memory on 7db080dfe9d2:43279 (size: 32.7 KiB, free: 434.2 MiB)\n",
      "25/05/18 03:52:51 INFO  SparkContext:60 Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/18 03:52:51 INFO  DAGScheduler:60 Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[14] at toTable at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/18 03:52:51 INFO  TaskSchedulerImpl:60 Adding task set 3.0 with 1 tasks resource profile 0\n",
      "25/05/18 03:53:06 WARN  TaskSchedulerImpl:72 Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/05/18 03:53:21 WARN  TaskSchedulerImpl:72 Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/05/18 03:53:36 WARN  TaskSchedulerImpl:72 Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/05/18 03:53:51 WARN  TaskSchedulerImpl:72 Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/05/18 03:54:06 WARN  TaskSchedulerImpl:72 Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/05/18 03:54:21 WARN  TaskSchedulerImpl:72 Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/05/18 03:54:36 WARN  TaskSchedulerImpl:72 Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/05/18 03:54:51 WARN  TaskSchedulerImpl:72 Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/05/18 03:55:06 WARN  TaskSchedulerImpl:72 Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/05/18 03:55:21 WARN  TaskSchedulerImpl:72 Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/05/18 03:55:36 WARN  TaskSchedulerImpl:72 Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    table_name = \"invoice_line_items\"\n",
    "    schema = \"\"\"\n",
    "        InvoiceNumber string,\n",
    "        CreatedTime bigint,\n",
    "        StoreID string,\n",
    "        PosID string,\n",
    "        CashierID string,\n",
    "        CustomerType string,\n",
    "        CustomerCardNo string,\n",
    "        TotalAmount double,\n",
    "        NumberOfItems bigint,\n",
    "        PaymentMethod string,\n",
    "        TaxableAmount double,\n",
    "        CGST double,\n",
    "        SGST double,\n",
    "        CESS double,\n",
    "        DeliveryType string,\n",
    "        DeliveryAddress struct<\n",
    "            AddressLine string,\n",
    "            City string,\n",
    "            ContactNumber string,\n",
    "            PinCode string,\n",
    "            State string\n",
    "        >,\n",
    "        InvoiceLineItems array<\n",
    "            struct<\n",
    "                ItemCode string,\n",
    "                ItemDescription string,\n",
    "                ItemPrice double,\n",
    "                ItemQty bigint,\n",
    "                TotalValue double\n",
    "            >\n",
    "        >\n",
    "    \"\"\"\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"InvoicesStreamMedallion\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    invoices_bronze = InvoiceStreamBronze(spark)\n",
    "    invoices_df = invoices_bronze.read_invoices(\n",
    "        format=\"json\",\n",
    "        path=\"/opt/spark/datasets/invoices/*.json\",\n",
    "        schema=schema,\n",
    "        clean_source=\"archive\",\n",
    "        archive_dir=\"/opt/spark/datasets/archive/invoices\"\n",
    "    )\n",
    "    squery_bronze = invoices_bronze.write_invoices(\n",
    "        df=invoices_df,\n",
    "        format=\"delta\",\n",
    "        checkpoint_location=\"/opt/spark/datasets/checkpoint/invoices/bronze\",\n",
    "        output_mode=\"append\",\n",
    "        table=\"invoices_bronze\",\n",
    "        query_name=\"ingestion_bronze\"\n",
    "    )\n",
    "\n",
    "    invoices_silver = InvoiceStreamSilver(spark)\n",
    "    exploded_df = invoices_silver.explode_invoices(invoices_df)\n",
    "    flatten_df = invoices_silver.flatten_invoices(exploded_df)\n",
    "    squery_silver = invoices_silver.write_invoices(\n",
    "        df=flatten_df,\n",
    "        format=\"delta\",\n",
    "        checkpoint_location=\"/opt/spark/datasets/checkpoint/invoices/silver\",\n",
    "        output_mode=\"append\",\n",
    "        table=\"invoices_silver\",\n",
    "        query_name=\"ingestion_silver\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c34c4-6016-442d-8a97-114e9604e273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-05-18 03:56:13 - DEBUG - Command to send: c\n",
      "o93\n",
      "read\n",
      "e\n",
      "\n",
      "25-05-18 03:56:13 - DEBUG - Answer received: !yro141\n",
      "25-05-18 03:56:13 - DEBUG - Command to send: c\n",
      "o141\n",
      "table\n",
      "sinvoices_bronze\n",
      "e\n",
      "\n",
      "25/05/18 03:56:13 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:56:13 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25/05/18 03:56:13 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_bronze\n",
      "25/05/18 03:56:13 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_bronze\t\n",
      "25/05/18 03:56:13 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_bronze\n",
      "25/05/18 03:56:13 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_bronze\t\n",
      "25-05-18 03:56:13 - DEBUG - Answer received: !yro142\n",
      "25-05-18 03:56:13 - DEBUG - Command to send: c\n",
      "o142\n",
      "showString\n",
      "i20\n",
      "i20\n",
      "bFalse\n",
      "e\n",
      "\n",
      "25/05/18 03:56:13 INFO  PrepareDeltaScan:95 DELTA: Filtering files for query\n",
      "25/05/18 03:56:21 WARN  TaskSchedulerImpl:72 Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"invoices_bronze\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f75878-ab74-4106-9acc-94f784df2de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
