{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02cf1f22-d484-4897-a5af-0a6e7df6aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split, trim, lower\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "import logging\n",
    "from os.path import abspath\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2496d8f7-17c5-4ace-a768-6dc32c4067f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%y-%m-%d %H:%M:%S\",\n",
    "    level=logging.DEBUG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a8f4fb7-5957-47e8-89f6-38a79d7708f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWordCount():\n",
    "    def __init__(\n",
    "        self,\n",
    "        spark: SparkSession\n",
    "    ):\n",
    "        self.spark = spark\n",
    "        logging.info(\"Initiated SparkSession.\")\n",
    "\n",
    "    def read_text(\n",
    "        self,\n",
    "        path: str,\n",
    "        format: str = \"text\",\n",
    "        line_sep: str = \".\"\n",
    "    ) -> DataFrame:\n",
    "        lines = (\n",
    "            self.spark.read\n",
    "            .format(format)\n",
    "            .option(\"lineSep\", line_sep)\n",
    "            .load(path)\n",
    "        )\n",
    "        raw_sdf = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "        logging.info(f\"Read Text Files as DataFrame, {raw_sdf.count()} rows, {len(raw_sdf.columns)} columns, schema: {raw_sdf.schema}\")\n",
    "        return raw_sdf\n",
    "\n",
    "    def process_text(\n",
    "        self,\n",
    "        raw_sdf: DataFrame\n",
    "    ) -> DataFrame:\n",
    "        processed_sdf = (\n",
    "            raw_sdf.select(lower(trim(raw_sdf.word)).alias(\"word\"))\n",
    "            .where(\"word is not null\")\n",
    "            .where(\"word rlike '[a-z]'\")\n",
    "        )\n",
    "        logging.info(f\"Processed DataFrame, {processed_sdf.count()} rows, {len(processed_sdf.columns)} columns, schema: {processed_sdf.schema}\")\n",
    "        return processed_sdf\n",
    "\n",
    "    def count_words(\n",
    "        self,\n",
    "        processed_sdf: DataFrame\n",
    "    ) -> DataFrame:\n",
    "        sdf = processed_sdf.groupBy(\"word\").count()\n",
    "        logging.info(f\"Grouped DataFrame by word, {sdf.count()} rows, {len(sdf.columns)} columns, schema: {sdf.schema}\")\n",
    "        return sdf\n",
    "\n",
    "    def write_table(\n",
    "        self,\n",
    "        sdf: DataFrame,\n",
    "        format: str,\n",
    "        mode: str,\n",
    "        table_name: str\n",
    "    ):\n",
    "        (\n",
    "            sdf.write\n",
    "            .format(format)\n",
    "            .mode(mode)\n",
    "            .saveAsTable(table_name)\n",
    "        )\n",
    "        logging.info(f\"Wrote {sdf.count()} rows, {len(sdf.columns)} columns, schema: {sdf.schema}  to table: {table_name}, format: {format}, mode: {mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89bbc940-cad8-4a66-99c1-e66c0c89ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamWordCount():\n",
    "    def __init__(\n",
    "        self,\n",
    "        spark: SparkSession\n",
    "    ):\n",
    "        self.spark = spark\n",
    "\n",
    "    def read_text(\n",
    "        self,\n",
    "        path: str,\n",
    "        format: str = \"text\",\n",
    "        line_sep: str = \".\"\n",
    "    ):\n",
    "        lines = (\n",
    "            self.spark.readStream\n",
    "            .format(format)\n",
    "            .option(\"lineSep\", line_sep)\n",
    "            .load(path)\n",
    "        )\n",
    "        raw_sdf = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "        logging.info(f\"Read Text Files Streaming as DataFrame, {raw_sdf.count()} rows, {len(raw_sdf.columns)} columns, schema: {raw_sdf.schema}\")\n",
    "        return raw_sdf\n",
    "\n",
    "    def process_text(\n",
    "        self,\n",
    "        raw_sdf: DataFrame\n",
    "    ) -> DataFrame:\n",
    "        processed_sdf = (\n",
    "            raw_sdf.select(lower(trim(raw_sdf.word)).alias(\"word\"))\n",
    "            .where(\"word is not null\")\n",
    "            .where(\"word rlike '[a-z]'\")\n",
    "        )\n",
    "        logging.info(f\"Processed Streaming DataFrame, {processed_sdf.count()} rows, {len(processed_sdf.columns)} columns, schema: {processed_sdf.schema}\")\n",
    "        return processed_sdf\n",
    "\n",
    "    def count_words(\n",
    "        self,\n",
    "        processed_sdf: DataFrame\n",
    "    ) -> DataFrame:\n",
    "        sdf = processed_sdf.groupBy(\"word\").count()\n",
    "        logging.info(f\"Grouped Streaming DataFrame by word, {sdf.count()} rows, {len(sdf.columns)} columns, schema: {sdf.schema}\")\n",
    "        return sdf\n",
    "\n",
    "    def write_table(\n",
    "        self,\n",
    "        sdf: DataFrame,\n",
    "        format: str,\n",
    "        output_mode: str,\n",
    "        table_name: str,\n",
    "        checkpoint_location: str\n",
    "    ):\n",
    "        squery = (\n",
    "            sdf.writeStream\n",
    "            .format(format)\n",
    "            .option(\"truncate\", value=False)\n",
    "            .option(\"checkpointLocation\", checkpoint_location)\n",
    "            .outputMode(output_mode)\n",
    "            # .toTable(table_name)\n",
    "            .start()\n",
    "            .awaitTermination()\n",
    "        )\n",
    "        logging.info(f\"Wrote streaming {sdf.count()} rows, {len(sdf.columns)} columns, schema: {sdf.schema}  to table: {table_name}, format: {format}, outputMode: {output_mode}\")\n",
    "        return squery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "910c4581-6783-4e11-aedb-82c94ccb0159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-04-09 00:43:48 - INFO - spark-warehouse: /home/jovyan/work/spark-warehouse\n"
     ]
    }
   ],
   "source": [
    "table_name = \"word_count_table\"\n",
    "warehouse_location = abspath(\"spark-warehouse\")\n",
    "logging.info(f\"spark-warehouse: {warehouse_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "132df697-58c7-428c-82eb-caff60d9ed50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-04-09 00:43:54 - DEBUG - GatewayClient.address is deprecated and will be removed in version 1.0. Use GatewayParameters instead.\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: A\n",
      "6c5758cc5d7e92d86ffaf7aebdb12f51627d8f2233ef0519e7306fa8858c8bb6\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.SparkConf\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.api.java.*\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.api.python.*\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.ml.python.*\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.mllib.api.python.*\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.resource.*\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.sql.*\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.sql.api.python.*\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: j\n",
      "i\n",
      "rj\n",
      "org.apache.spark.sql.hive.*\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: j\n",
      "i\n",
      "rj\n",
      "scala.Tuple2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: r\n",
      "u\n",
      "SparkConf\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ycorg.apache.spark.SparkConf\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: i\n",
      "org.apache.spark.SparkConf\n",
      "bTrue\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro0\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.app.name\n",
      "sstreaming_word_count\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro1\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.sql.warehouse.dir\n",
      "s/home/jovyan/work/spark-warehouse\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro2\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.driver.cores\n",
      "s2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro3\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.driver.memory\n",
      "s2g\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro4\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.executor.memory\n",
      "s1g\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro5\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.submit.deployMode\n",
      "sclient\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro6\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.log.level\n",
      "sALL\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro7\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.sql.catalogImplementation\n",
      "shive\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro8\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o0\n",
      "contains\n",
      "sspark.serializer.objectStreamReset\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ybfalse\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.serializer.objectStreamReset\n",
      "s100\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro9\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o0\n",
      "contains\n",
      "sspark.rdd.compress\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ybfalse\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o0\n",
      "set\n",
      "sspark.rdd.compress\n",
      "sTrue\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro10\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o0\n",
      "contains\n",
      "sspark.master\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ybtrue\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o0\n",
      "contains\n",
      "sspark.app.name\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ybtrue\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o0\n",
      "contains\n",
      "sspark.master\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ybtrue\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o0\n",
      "get\n",
      "sspark.master\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark://spark-master:7077\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o0\n",
      "contains\n",
      "sspark.app.name\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ybtrue\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o0\n",
      "get\n",
      "sspark.app.name\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysstreaming_word_count\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o0\n",
      "contains\n",
      "sspark.home\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ybfalse\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o0\n",
      "getAll\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yto11\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "e\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yi21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "g\n",
      "o11\n",
      "i0\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro12\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o12\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark.eventLog.enabled\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o12\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ystrue\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "e\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yi21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "g\n",
      "o11\n",
      "i1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro13\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o13\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark.driver.cores\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o13\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ys2\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "e\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yi21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "g\n",
      "o11\n",
      "i2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro14\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o14\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark.history.fs.logDirectory\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o14\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ys/usr/local/spark/spark-events\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "e\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yi21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "g\n",
      "o11\n",
      "i3\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro15\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o15\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark.app.submitTime\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o15\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ys1744159434314\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "e\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yi21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "g\n",
      "o11\n",
      "i4\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro16\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o16\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark.log.level\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o16\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysALL\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "e\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yi21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "g\n",
      "o11\n",
      "i5\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro17\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o17\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark.eventLog.dir\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o17\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ys/usr/local/spark/spark-events\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "e\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yi21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "g\n",
      "o11\n",
      "i6\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro18\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o18\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark.sql.catalogImplementation\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o18\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yshive\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "e\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yi21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "g\n",
      "o11\n",
      "i7\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro19\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o19\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark.sql.extensions\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o19\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysio.delta.sql.DeltaSparkSessionExtension\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "e\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yi21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "g\n",
      "o11\n",
      "i8\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro20\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o20\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark.executor.memory\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o20\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ys1g\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "e\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yi21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "g\n",
      "o11\n",
      "i9\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o21\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark.rdd.compress\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o21\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysTrue\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "e\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yi21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "g\n",
      "o11\n",
      "i10\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro22\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o22\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark.master\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o22\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark://spark-master:7077\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "e\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yi21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "g\n",
      "o11\n",
      "i11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro23\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o23\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark.driver.memory\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o23\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ys2g\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "e\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yi21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "g\n",
      "o11\n",
      "i12\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro24\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o24\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark.serializer.objectStreamReset\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o24\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ys100\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "e\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yi21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "g\n",
      "o11\n",
      "i13\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro25\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o25\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark.sql.warehouse.dir\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o25\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ys/home/jovyan/work/spark-warehouse\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "e\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yi21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "g\n",
      "o11\n",
      "i14\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro26\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o26\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark.app.name\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o26\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysstreaming_word_count\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "e\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yi21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "g\n",
      "o11\n",
      "i15\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro27\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o27\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark.submit.pyFiles\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o27\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ys\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "e\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yi21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "g\n",
      "o11\n",
      "i16\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro28\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o28\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark.submit.deployMode\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o28\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysclient\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "e\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yi21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "g\n",
      "o11\n",
      "i17\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro29\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o29\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark.jars\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o29\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysfile:/usr/local/spark-3.5.5-bin-hadoop3/jars/HikariCP-2.5.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/JLargeArrays-1.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/JTransforms-3.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/RoaringBitmap-0.9.45.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/ST4-4.0.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/activation-1.1.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/aircompressor-0.27.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/algebra_2.12-2.0.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/annotations-17.0.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/antlr-runtime-3.5.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/antlr4-runtime-4.9.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/aopalliance-repackaged-2.6.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/arpack-3.0.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/arpack_combined_all-0.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/arrow-format-12.0.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/arrow-memory-core-12.0.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/arrow-memory-netty-12.0.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/arrow-vector-12.0.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/audience-annotations-0.5.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/avro-1.11.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/avro-ipc-1.11.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/avro-mapred-1.11.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/aws-java-sdk-1.12.782.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/blas-3.0.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/bonecp-0.8.0.RELEASE.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/breeze-macros_2.12-2.1.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/breeze_2.12-2.1.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/cats-kernel_2.12-2.1.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/chill-java-0.10.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/chill_2.12-0.10.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-cli-1.5.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-codec-1.16.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-collections-3.2.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-collections4-4.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-compiler-3.1.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-compress-1.23.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-crypto-1.1.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-dbcp-1.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-io-2.16.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-lang-2.6.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-lang3-3.12.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-logging-1.1.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-math3-3.6.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-pool-1.5.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-text-1.10.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/compress-lzf-1.1.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/curator-client-2.13.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/curator-framework-2.13.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/curator-recipes-2.13.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/datanucleus-api-jdo-4.2.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/datanucleus-core-4.1.17.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/datanucleus-rdbms-4.1.19.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/datasketches-java-3.3.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/datasketches-memory-2.1.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/delta-spark_2.13-3.3.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/delta-storage-3.3.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/derby-10.14.2.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/flatbuffers-java-1.12.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/gson-2.2.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/guava-14.0.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hadoop-client-api-3.3.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hadoop-client-runtime-3.3.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hadoop-shaded-guava-1.1.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hadoop-yarn-server-web-proxy-3.3.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-beeline-2.3.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-cli-2.3.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-common-2.3.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-exec-2.3.9-core.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-jdbc-2.3.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-llap-common-2.3.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-metastore-2.3.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-serde-2.3.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-service-rpc-3.1.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-shims-0.23-2.3.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-shims-2.3.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-shims-common-2.3.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-shims-scheduler-2.3.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-storage-api-2.8.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hk2-api-2.6.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hk2-locator-2.6.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hk2-utils-2.6.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/httpclient-4.5.14.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/httpcore-4.4.16.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/istack-commons-runtime-3.0.8.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/ivy-2.5.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jackson-annotations-2.15.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jackson-core-2.15.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jackson-core-asl-1.9.13.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jackson-databind-2.15.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jackson-dataformat-yaml-2.15.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jackson-datatype-jsr310-2.15.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jackson-mapper-asl-1.9.13.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jackson-module-scala_2.12-2.15.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jakarta.annotation-api-1.3.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jakarta.inject-2.6.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jakarta.servlet-api-4.0.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jakarta.validation-api-2.0.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jakarta.ws.rs-api-2.1.6.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jakarta.xml.bind-api-2.3.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/janino-3.1.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/javassist-3.29.2-GA.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/javax.jdo-3.2.0-m3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/javolution-5.5.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jaxb-runtime-2.3.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jcl-over-slf4j-2.0.7.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jdo-api-3.0.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jersey-client-2.40.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jersey-common-2.40.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jersey-container-servlet-2.40.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jersey-container-servlet-core-2.40.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jersey-hk2-2.40.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jersey-server-2.40.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jline-2.14.6.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/joda-time-2.12.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jodd-core-3.5.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jpam-1.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/json-1.8.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/json4s-ast_2.12-3.7.0-M11.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/json4s-core_2.12-3.7.0-M11.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/json4s-jackson_2.12-3.7.0-M11.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/json4s-scalap_2.12-3.7.0-M11.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jsr305-3.0.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jta-1.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jul-to-slf4j-2.0.7.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kryo-shaded-4.0.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-client-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-client-api-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-httpclient-okhttp-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-admissionregistration-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-apiextensions-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-apps-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-autoscaling-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-batch-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-certificates-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-common-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-coordination-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-core-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-discovery-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-events-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-extensions-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-flowcontrol-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-gatewayapi-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-metrics-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-networking-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-node-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-policy-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-rbac-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-resource-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-scheduling-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-storageclass-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/lapack-3.0.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/leveldbjni-all-1.8.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/libfb303-0.9.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/libthrift-0.12.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/log4j-1.2-api-2.20.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/log4j-api-2.20.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/log4j-core-2.20.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/log4j-slf4j2-impl-2.20.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/logging-interceptor-3.12.12.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/lz4-java-1.8.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/mesos-1.4.3-shaded-protobuf.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/metrics-core-4.2.19.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/metrics-graphite-4.2.19.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/metrics-jmx-4.2.19.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/metrics-json-4.2.19.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/metrics-jvm-4.2.19.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/minlog-1.3.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-all-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-buffer-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-codec-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-codec-http-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-codec-http2-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-codec-socks-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-common-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-handler-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-handler-proxy-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-resolver-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-transport-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-transport-classes-epoll-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-transport-classes-kqueue-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-transport-native-epoll-4.1.96.Final-linux-aarch_64.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-transport-native-epoll-4.1.96.Final-linux-x86_64.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-transport-native-kqueue-4.1.96.Final-osx-aarch_64.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-transport-native-kqueue-4.1.96.Final-osx-x86_64.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-transport-native-unix-common-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/objenesis-3.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/okhttp-3.12.12.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/okio-1.17.6.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/opencsv-2.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/orc-core-1.9.5-shaded-protobuf.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/orc-mapreduce-1.9.5-shaded-protobuf.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/orc-shims-1.9.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/oro-2.0.8.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/osgi-resource-locator-1.0.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/paranamer-2.8.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/parquet-column-1.13.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/parquet-common-1.13.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/parquet-encoding-1.13.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/parquet-format-structures-1.13.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/parquet-hadoop-1.13.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/parquet-jackson-1.13.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/pickle-1.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/py4j-0.10.9.7.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/rocksdbjni-8.3.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/s3-2.31.16.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/scala-collection-compat_2.12-2.7.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/scala-compiler-2.12.18.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/scala-library-2.12.18.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/scala-parser-combinators_2.12-2.3.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/scala-reflect-2.12.18.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/scala-xml_2.12-2.1.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/shims-0.9.45.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/slf4j-api-2.0.7.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/snakeyaml-2.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/snakeyaml-engine-2.6.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/snappy-java-1.1.10.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-catalyst_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-common-utils_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-core_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-graphx_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-hive-thriftserver_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-hive_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-kubernetes_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-kvstore_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-launcher_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-mesos_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-mllib-local_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-mllib_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-network-common_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-network-shuffle_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-repl_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-sketch_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-sql-api_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-sql-kafka-0-10_2.13-3.5.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-sql_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-sql_2.13-3.5.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-streaming_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-tags_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-unsafe_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-yarn_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spire-macros_2.12-0.17.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spire-platform_2.12-0.17.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spire-util_2.12-0.17.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spire_2.12-0.17.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/stax-api-1.0.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/stream-2.9.6.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/super-csv-2.2.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/threeten-extra-1.7.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/tink-1.9.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/transaction-api-1.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/univocity-parsers-2.9.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/xbean-asm9-shaded-4.23.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/xz-1.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/zjsonpatch-0.3.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/zookeeper-3.6.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/zookeeper-jute-3.6.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/zstd-jni-1.5.5-4.jar\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "e\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yi21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "g\n",
      "o11\n",
      "i18\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro30\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o30\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark.ui.showConsoleProgress\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o30\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ystrue\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "e\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yi21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "g\n",
      "o11\n",
      "i19\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro31\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o31\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark.repl.local.jars\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o31\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysfile:/usr/local/spark-3.5.5-bin-hadoop3/jars/HikariCP-2.5.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/JLargeArrays-1.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/JTransforms-3.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/RoaringBitmap-0.9.45.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/ST4-4.0.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/activation-1.1.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/aircompressor-0.27.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/algebra_2.12-2.0.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/annotations-17.0.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/antlr-runtime-3.5.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/antlr4-runtime-4.9.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/aopalliance-repackaged-2.6.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/arpack-3.0.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/arpack_combined_all-0.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/arrow-format-12.0.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/arrow-memory-core-12.0.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/arrow-memory-netty-12.0.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/arrow-vector-12.0.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/audience-annotations-0.5.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/avro-1.11.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/avro-ipc-1.11.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/avro-mapred-1.11.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/aws-java-sdk-1.12.782.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/blas-3.0.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/bonecp-0.8.0.RELEASE.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/breeze-macros_2.12-2.1.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/breeze_2.12-2.1.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/cats-kernel_2.12-2.1.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/chill-java-0.10.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/chill_2.12-0.10.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-cli-1.5.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-codec-1.16.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-collections-3.2.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-collections4-4.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-compiler-3.1.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-compress-1.23.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-crypto-1.1.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-dbcp-1.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-io-2.16.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-lang-2.6.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-lang3-3.12.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-logging-1.1.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-math3-3.6.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-pool-1.5.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/commons-text-1.10.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/compress-lzf-1.1.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/curator-client-2.13.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/curator-framework-2.13.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/curator-recipes-2.13.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/datanucleus-api-jdo-4.2.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/datanucleus-core-4.1.17.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/datanucleus-rdbms-4.1.19.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/datasketches-java-3.3.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/datasketches-memory-2.1.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/delta-spark_2.13-3.3.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/delta-storage-3.3.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/derby-10.14.2.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/flatbuffers-java-1.12.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/gson-2.2.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/guava-14.0.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hadoop-client-api-3.3.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hadoop-client-runtime-3.3.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hadoop-shaded-guava-1.1.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hadoop-yarn-server-web-proxy-3.3.4.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-beeline-2.3.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-cli-2.3.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-common-2.3.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-exec-2.3.9-core.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-jdbc-2.3.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-llap-common-2.3.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-metastore-2.3.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-serde-2.3.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-service-rpc-3.1.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-shims-0.23-2.3.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-shims-2.3.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-shims-common-2.3.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-shims-scheduler-2.3.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hive-storage-api-2.8.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hk2-api-2.6.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hk2-locator-2.6.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/hk2-utils-2.6.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/httpclient-4.5.14.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/httpcore-4.4.16.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/istack-commons-runtime-3.0.8.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/ivy-2.5.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jackson-annotations-2.15.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jackson-core-2.15.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jackson-core-asl-1.9.13.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jackson-databind-2.15.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jackson-dataformat-yaml-2.15.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jackson-datatype-jsr310-2.15.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jackson-mapper-asl-1.9.13.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jackson-module-scala_2.12-2.15.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jakarta.annotation-api-1.3.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jakarta.inject-2.6.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jakarta.servlet-api-4.0.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jakarta.validation-api-2.0.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jakarta.ws.rs-api-2.1.6.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jakarta.xml.bind-api-2.3.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/janino-3.1.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/javassist-3.29.2-GA.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/javax.jdo-3.2.0-m3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/javolution-5.5.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jaxb-runtime-2.3.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jcl-over-slf4j-2.0.7.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jdo-api-3.0.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jersey-client-2.40.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jersey-common-2.40.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jersey-container-servlet-2.40.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jersey-container-servlet-core-2.40.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jersey-hk2-2.40.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jersey-server-2.40.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jline-2.14.6.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/joda-time-2.12.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jodd-core-3.5.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jpam-1.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/json-1.8.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/json4s-ast_2.12-3.7.0-M11.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/json4s-core_2.12-3.7.0-M11.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/json4s-jackson_2.12-3.7.0-M11.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/json4s-scalap_2.12-3.7.0-M11.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jsr305-3.0.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jta-1.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/jul-to-slf4j-2.0.7.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kryo-shaded-4.0.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-client-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-client-api-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-httpclient-okhttp-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-admissionregistration-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-apiextensions-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-apps-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-autoscaling-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-batch-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-certificates-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-common-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-coordination-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-core-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-discovery-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-events-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-extensions-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-flowcontrol-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-gatewayapi-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-metrics-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-networking-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-node-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-policy-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-rbac-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-resource-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-scheduling-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/kubernetes-model-storageclass-6.7.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/lapack-3.0.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/leveldbjni-all-1.8.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/libfb303-0.9.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/libthrift-0.12.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/log4j-1.2-api-2.20.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/log4j-api-2.20.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/log4j-core-2.20.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/log4j-slf4j2-impl-2.20.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/logging-interceptor-3.12.12.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/lz4-java-1.8.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/mesos-1.4.3-shaded-protobuf.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/metrics-core-4.2.19.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/metrics-graphite-4.2.19.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/metrics-jmx-4.2.19.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/metrics-json-4.2.19.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/metrics-jvm-4.2.19.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/minlog-1.3.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-all-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-buffer-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-codec-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-codec-http-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-codec-http2-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-codec-socks-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-common-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-handler-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-handler-proxy-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-resolver-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-transport-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-transport-classes-epoll-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-transport-classes-kqueue-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-transport-native-epoll-4.1.96.Final-linux-aarch_64.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-transport-native-epoll-4.1.96.Final-linux-x86_64.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-transport-native-kqueue-4.1.96.Final-osx-aarch_64.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-transport-native-kqueue-4.1.96.Final-osx-x86_64.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/netty-transport-native-unix-common-4.1.96.Final.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/objenesis-3.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/okhttp-3.12.12.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/okio-1.17.6.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/opencsv-2.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/orc-core-1.9.5-shaded-protobuf.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/orc-mapreduce-1.9.5-shaded-protobuf.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/orc-shims-1.9.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/oro-2.0.8.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/osgi-resource-locator-1.0.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/paranamer-2.8.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/parquet-column-1.13.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/parquet-common-1.13.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/parquet-encoding-1.13.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/parquet-format-structures-1.13.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/parquet-hadoop-1.13.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/parquet-jackson-1.13.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/pickle-1.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/py4j-0.10.9.7.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/rocksdbjni-8.3.2.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/s3-2.31.16.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/scala-collection-compat_2.12-2.7.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/scala-compiler-2.12.18.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/scala-library-2.12.18.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/scala-parser-combinators_2.12-2.3.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/scala-reflect-2.12.18.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/scala-xml_2.12-2.1.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/shims-0.9.45.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/slf4j-api-2.0.7.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/snakeyaml-2.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/snakeyaml-engine-2.6.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/snappy-java-1.1.10.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-catalyst_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-common-utils_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-core_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-graphx_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-hive-thriftserver_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-hive_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-kubernetes_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-kvstore_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-launcher_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-mesos_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-mllib-local_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-mllib_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-network-common_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-network-shuffle_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-repl_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-sketch_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-sql-api_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-sql-kafka-0-10_2.13-3.5.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-sql_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-sql_2.13-3.5.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-streaming_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-tags_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-unsafe_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spark-yarn_2.12-3.5.5.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spire-macros_2.12-0.17.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spire-platform_2.12-0.17.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spire-util_2.12-0.17.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/spire_2.12-0.17.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/stax-api-1.0.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/stream-2.9.6.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/super-csv-2.2.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/threeten-extra-1.7.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/tink-1.9.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/transaction-api-1.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/univocity-parsers-2.9.1.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/xbean-asm9-shaded-4.23.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/xz-1.9.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/zjsonpatch-0.3.0.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/zookeeper-3.6.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/zookeeper-jute-3.6.3.jar,file:/usr/local/spark-3.5.5-bin-hadoop3/jars/zstd-jni-1.5.5-4.jar\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "e\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yi21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "g\n",
      "o11\n",
      "i20\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yro32\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o32\n",
      "_1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysspark.sql.catalog.spark_catalog\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: c\n",
      "o32\n",
      "_2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ysorg.apache.spark.sql.delta.catalog.DeltaCatalog\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: a\n",
      "e\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !yi21\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: r\n",
      "u\n",
      "JavaSparkContext\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:54 - DEBUG - Answer received: !ycorg.apache.spark.api.java.JavaSparkContext\n",
      "25-04-09 00:43:54 - DEBUG - Command to send: i\n",
      "org.apache.spark.api.java.JavaSparkContext\n",
      "ro0\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: A\n",
      "6c5758cc5d7e92d86ffaf7aebdb12f51627d8f2233ef0519e7306fa8858c8bb6\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o1\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o3\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o4\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o5\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o6\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o7\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o8\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o9\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o10\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o12\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o13\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o14\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o15\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o16\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o17\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o18\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o19\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o20\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o21\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o22\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o23\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:55 - DEBUG - Command to send: m\n",
      "d\n",
      "o11\n",
      "e\n",
      "\n",
      "25-04-09 00:43:55 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:57 - DEBUG - Answer received: !yro33\n",
      "25-04-09 00:43:57 - DEBUG - Command to send: c\n",
      "o33\n",
      "sc\n",
      "e\n",
      "\n",
      "25-04-09 00:43:57 - DEBUG - Answer received: !yro34\n",
      "25-04-09 00:43:57 - DEBUG - Command to send: c\n",
      "o34\n",
      "conf\n",
      "e\n",
      "\n",
      "25-04-09 00:43:57 - DEBUG - Answer received: !yro35\n",
      "25-04-09 00:43:57 - DEBUG - Command to send: r\n",
      "u\n",
      "PythonAccumulatorV2\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:57 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonAccumulatorV2\n",
      "25-04-09 00:43:57 - DEBUG - Command to send: i\n",
      "org.apache.spark.api.python.PythonAccumulatorV2\n",
      "s127.0.0.1\n",
      "i50241\n",
      "s6c5758cc5d7e92d86ffaf7aebdb12f51627d8f2233ef0519e7306fa8858c8bb6\n",
      "e\n",
      "\n",
      "25-04-09 00:43:57 - DEBUG - Answer received: !yro36\n",
      "25-04-09 00:43:57 - DEBUG - Command to send: c\n",
      "o33\n",
      "sc\n",
      "e\n",
      "\n",
      "25-04-09 00:43:57 - DEBUG - Answer received: !yro37\n",
      "25-04-09 00:43:57 - DEBUG - Command to send: c\n",
      "o37\n",
      "register\n",
      "ro36\n",
      "e\n",
      "\n",
      "25-04-09 00:43:57 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:57 - DEBUG - Command to send: r\n",
      "u\n",
      "PythonUtils\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:57 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils\n",
      "25-04-09 00:43:57 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.api.python.PythonUtils\n",
      "isEncryptionEnabled\n",
      "e\n",
      "\n",
      "25-04-09 00:43:57 - DEBUG - Answer received: !ym\n",
      "25-04-09 00:43:57 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.api.python.PythonUtils\n",
      "isEncryptionEnabled\n",
      "ro33\n",
      "e\n",
      "\n",
      "25-04-09 00:43:57 - DEBUG - Answer received: !ybfalse\n",
      "25-04-09 00:43:57 - DEBUG - Command to send: r\n",
      "u\n",
      "PythonUtils\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:57 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils\n",
      "25-04-09 00:43:57 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.api.python.PythonUtils\n",
      "getPythonAuthSocketTimeout\n",
      "e\n",
      "\n",
      "25-04-09 00:43:57 - DEBUG - Answer received: !ym\n",
      "25-04-09 00:43:57 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.api.python.PythonUtils\n",
      "getPythonAuthSocketTimeout\n",
      "ro33\n",
      "e\n",
      "\n",
      "25-04-09 00:43:57 - DEBUG - Answer received: !yL15\n",
      "25-04-09 00:43:57 - DEBUG - Command to send: r\n",
      "u\n",
      "PythonUtils\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:57 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils\n",
      "25-04-09 00:43:57 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.api.python.PythonUtils\n",
      "getSparkBufferSize\n",
      "e\n",
      "\n",
      "25-04-09 00:43:57 - DEBUG - Answer received: !ym\n",
      "25-04-09 00:43:57 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.api.python.PythonUtils\n",
      "getSparkBufferSize\n",
      "ro33\n",
      "e\n",
      "\n",
      "25-04-09 00:43:57 - DEBUG - Answer received: !yi65536\n",
      "25-04-09 00:43:57 - DEBUG - Command to send: r\n",
      "u\n",
      "org\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:57 - DEBUG - Answer received: !yp\n",
      "25-04-09 00:43:57 - DEBUG - Command to send: r\n",
      "u\n",
      "org.apache\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:57 - DEBUG - Answer received: !yp\n",
      "25-04-09 00:43:57 - DEBUG - Command to send: r\n",
      "u\n",
      "org.apache.spark\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:57 - DEBUG - Answer received: !yp\n",
      "25-04-09 00:43:57 - DEBUG - Command to send: r\n",
      "u\n",
      "org.apache.spark.SparkFiles\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:57 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles\n",
      "25-04-09 00:43:57 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.SparkFiles\n",
      "getRootDirectory\n",
      "e\n",
      "\n",
      "25-04-09 00:43:57 - DEBUG - Answer received: !ym\n",
      "25-04-09 00:43:57 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.SparkFiles\n",
      "getRootDirectory\n",
      "e\n",
      "\n",
      "25-04-09 00:43:57 - DEBUG - Answer received: !ys/tmp/spark-34215e40-078e-49a2-8374-91927902ee6a/userFiles-03df07e3-8ae2-4928-87a5-94e65d062914\n",
      "25-04-09 00:43:57 - DEBUG - Command to send: c\n",
      "o35\n",
      "get\n",
      "sspark.submit.pyFiles\n",
      "s\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !ys\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: r\n",
      "u\n",
      "org\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yp\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: r\n",
      "u\n",
      "org.apache\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yp\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: r\n",
      "u\n",
      "org.apache.spark\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yp\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: r\n",
      "u\n",
      "org.apache.spark.util\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yp\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: r\n",
      "u\n",
      "org.apache.spark.util.Utils\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !ycorg.apache.spark.util.Utils\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.util.Utils\n",
      "getLocalDir\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !ym\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: c\n",
      "o33\n",
      "sc\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yro38\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: c\n",
      "o38\n",
      "conf\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yro39\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.util.Utils\n",
      "getLocalDir\n",
      "ro39\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !ys/tmp/spark-34215e40-078e-49a2-8374-91927902ee6a\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: r\n",
      "u\n",
      "org\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yp\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: r\n",
      "u\n",
      "org.apache\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yp\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: r\n",
      "u\n",
      "org.apache.spark\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yp\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: r\n",
      "u\n",
      "org.apache.spark.util\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yp\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: r\n",
      "u\n",
      "org.apache.spark.util.Utils\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !ycorg.apache.spark.util.Utils\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.util.Utils\n",
      "createTempDir\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !ym\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.util.Utils\n",
      "createTempDir\n",
      "s/tmp/spark-34215e40-078e-49a2-8374-91927902ee6a\n",
      "spyspark\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yro40\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: c\n",
      "o40\n",
      "getAbsolutePath\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !ys/tmp/spark-34215e40-078e-49a2-8374-91927902ee6a/pyspark-7ca436fb-ad28-4f66-b10c-217faf1094bb\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: c\n",
      "o35\n",
      "get\n",
      "sspark.python.profile\n",
      "sfalse\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !ysfalse\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: c\n",
      "o35\n",
      "get\n",
      "sspark.python.profile.memory\n",
      "sfalse\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !ysfalse\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: r\n",
      "u\n",
      "SparkSession\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.SparkSession\n",
      "getDefaultSession\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !ym\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.SparkSession\n",
      "getDefaultSession\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yro41\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: c\n",
      "o41\n",
      "isDefined\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !ybfalse\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: r\n",
      "u\n",
      "SparkSession\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: c\n",
      "o33\n",
      "sc\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yro42\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: i\n",
      "java.util.HashMap\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yao43\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: c\n",
      "o43\n",
      "put\n",
      "sspark.app.name\n",
      "sstreaming_word_count\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yn\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: c\n",
      "o43\n",
      "put\n",
      "sspark.sql.warehouse.dir\n",
      "s/home/jovyan/work/spark-warehouse\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yn\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: c\n",
      "o43\n",
      "put\n",
      "sspark.driver.cores\n",
      "s2\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yn\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: c\n",
      "o43\n",
      "put\n",
      "sspark.driver.memory\n",
      "s2g\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yn\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: c\n",
      "o43\n",
      "put\n",
      "sspark.executor.memory\n",
      "s1g\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yn\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: c\n",
      "o43\n",
      "put\n",
      "sspark.submit.deployMode\n",
      "sclient\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yn\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: c\n",
      "o43\n",
      "put\n",
      "sspark.log.level\n",
      "sALL\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yn\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: c\n",
      "o43\n",
      "put\n",
      "sspark.sql.catalogImplementation\n",
      "shive\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yn\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: i\n",
      "org.apache.spark.sql.SparkSession\n",
      "ro42\n",
      "ro43\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yro44\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: r\n",
      "u\n",
      "SparkSession\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.SparkSession\n",
      "setDefaultSession\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !ym\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.SparkSession\n",
      "setDefaultSession\n",
      "ro44\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: r\n",
      "u\n",
      "SparkSession\n",
      "rj\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.SparkSession\n",
      "setActiveSession\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !ym\n",
      "25-04-09 00:43:58 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.SparkSession\n",
      "setActiveSession\n",
      "ro44\n",
      "e\n",
      "\n",
      "25-04-09 00:43:58 - DEBUG - Answer received: !yv\n",
      "25-04-09 00:43:59 - DEBUG - Command to send: m\n",
      "d\n",
      "o43\n",
      "e\n",
      "\n",
      "25-04-09 00:43:59 - DEBUG - Answer received: !yv\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"streaming_word_count\")\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location)\n",
    "    # .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    # .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    # .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.13:3.3.1,org.apache.spark:spark-sql_2.12:3.5.3\")\n",
    "    .config(\"spark.driver.cores\", 2)\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.executor.memory\", \"1g\")\n",
    "    .config(\"spark.submit.deployMode\", \"client\")\n",
    "    .config(\"spark.log.level\", \"ALL\")\n",
    "    # .config(conf=conf)\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
