{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb4f7c6a-cd66-4081-aa08-e64e7106b431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/18 03:52:06 INFO  SparkContext:60 SparkContext is stopping with exitCode 0.\n",
      "25/05/18 03:52:06 INFO  SparkUI:60 Stopped Spark web UI at http://7db080dfe9d2:4040\n",
      "25/05/18 03:52:06 INFO  StandaloneSchedulerBackend:60 Shutting down all executors\n",
      "25/05/18 03:52:06 INFO  StandaloneSchedulerBackend$StandaloneDriverEndpoint:60 Asking each executor to shut down\n",
      "25/05/18 03:52:06 INFO  MapOutputTrackerMasterEndpoint:60 MapOutputTrackerMasterEndpoint stopped!\n",
      "25/05/18 03:52:06 INFO  MemoryStore:60 MemoryStore cleared\n",
      "25/05/18 03:52:06 INFO  BlockManager:60 BlockManager stopped\n",
      "25/05/18 03:52:06 INFO  BlockManagerMaster:60 BlockManagerMaster stopped\n",
      "25/05/18 03:52:06 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:60 OutputCommitCoordinator stopped!\n",
      "25/05/18 03:52:06 INFO  SparkContext:60 Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73822cca-3af0-496e-a52c-5ecdd9f7e583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split, trim, lower, expr\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "import logging\n",
    "from os.path import abspath\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, List, Tuple, Any, Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0c085da-3212-4eee-b90b-1f108d16f231",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%y-%m-%d %H:%M:%S\",\n",
    "    level=logging.DEBUG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af23095-7408-41c6-b3cc-992886c1da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvoiceStreamBronze:\n",
    "    def __init__(\n",
    "        self,\n",
    "        spark: SparkSession\n",
    "    ):\n",
    "        self.spark = spark\n",
    "\n",
    "    def read_invoices(\n",
    "        self,\n",
    "        format: str,\n",
    "        path: Union[str, Path],\n",
    "        schema: Union[str, Any],\n",
    "        clean_source: Literal[\"delete\", \"archive\"],\n",
    "        archive_dir: Optional[str],\n",
    "    ) -> DataFrame:\n",
    "        if isinstance(path, str):\n",
    "            path = Path(path).as_posix()\n",
    "        if clean_source == \"archive\":\n",
    "            return (\n",
    "                self.spark.readStream\n",
    "                .format(format)\n",
    "                .schema(schema)\n",
    "                .option(\"cleanSource\", \"archive\")\n",
    "                .option(\"sourceArchiveDir\", archive_dir)\n",
    "                .load(path)\n",
    "            )\n",
    "        return (\n",
    "            self.spark.readStream\n",
    "            .format(format)\n",
    "            .schema(schema)\n",
    "            .option(\"cleanSource\", clean_source)\n",
    "            .load(path)\n",
    "        )\n",
    "\n",
    "    def write_invoices(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        format: str,\n",
    "        checkpoint_location: str,\n",
    "        output_mode: str,\n",
    "        table: str,\n",
    "        query_name: str\n",
    "    ):\n",
    "        return (\n",
    "            df.writeStream\n",
    "            .queryName(query_name)\n",
    "            .format(format)\n",
    "            .option(\"checkpointLocation\", checkpoint_location)\n",
    "            .outputMode(output_mode)\n",
    "            .toTable(table)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5156e85-1b04-46fa-ba8f-760448ab5c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvoiceStreamSilver:\n",
    "    def __init__(\n",
    "        self,\n",
    "        spark: SparkSession\n",
    "    ):\n",
    "        self.spark = spark\n",
    "\n",
    "    def read_invoices(self, table_name: str) -> DataFrame:\n",
    "        return (\n",
    "            self.spark.readStream\n",
    "            .table(table_name)\n",
    "        )\n",
    "\n",
    "    def explode_invoices(self, df: DataFrame) -> DataFrame:\n",
    "        return (\n",
    "            df.selectExpr(\n",
    "                \"InvoiceNumber\",\n",
    "                \"CreatedTime\",\n",
    "                \"StoreID\",\n",
    "                \"PosID\",\n",
    "                \"CustomerType\",\n",
    "                \"PaymentMethod\",\n",
    "                \"DeliveryType\",\n",
    "                \"DeliveryAddress.City\",\n",
    "                \"DeliveryAddress.PinCode\",\n",
    "                \"DeliveryAddress.State\",\n",
    "                \"explode(InvoiceLineItems) AS LineItem\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def flatten_invoices(self, df: DataFrame) -> DataFrame:\n",
    "        return (\n",
    "            df.withColumn(\"ItemCode\", expr(\"LineItem.ItemCode\"))\n",
    "            .withColumn(\"ItemDescription\", expr(\"LineItem.ItemDescription\"))\n",
    "            .withColumn(\"ItemPrice\", expr(\"LineItem.ItemPrice\"))\n",
    "            .withColumn(\"ItemQty\", expr(\"LineItem.ItemQty\"))\n",
    "            .withColumn(\"TotalValue\", expr(\"LineItem.TotalValue\"))\n",
    "            .drop(\"LineItem\")\n",
    "        )\n",
    "\n",
    "    def write_invoices(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        format: str,\n",
    "        checkpoint_location: str,\n",
    "        output_mode: str,\n",
    "        table: str,\n",
    "        query_name: str,\n",
    "    ):\n",
    "        return (\n",
    "            df.writeStream\n",
    "            .queryName(query_name)\n",
    "            .format(format)\n",
    "            .option(\"checkpointLocation\", checkpoint_location)\n",
    "            .outputMode(output_mode)\n",
    "            .toTable(table)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "554f53f6-1274-4260-af37-7fdc1cf86df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-05-18 03:59:36 - DEBUG - Command to send: r\n",
      "u\n",
      "SparkSession$\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:59:36 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession$\n",
      "25-05-18 03:59:36 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.SparkSession$\n",
      "MODULE$\n",
      "e\n",
      "\n",
      "25-05-18 03:59:36 - DEBUG - Answer received: !yro52\n",
      "25-05-18 03:59:36 - DEBUG - Command to send: i\n",
      "java.util.HashMap\n",
      "e\n",
      "\n",
      "25-05-18 03:59:36 - DEBUG - Answer received: !yao53\n",
      "25-05-18 03:59:36 - DEBUG - Command to send: c\n",
      "o53\n",
      "put\n",
      "sspark.app.name\n",
      "sInvoicesStreamMedallion\n",
      "e\n",
      "\n",
      "25-05-18 03:59:36 - DEBUG - Answer received: !yn\n",
      "25-05-18 03:59:36 - DEBUG - Command to send: c\n",
      "o53\n",
      "put\n",
      "sspark.sql.catalogImplementation\n",
      "shive\n",
      "e\n",
      "\n",
      "25-05-18 03:59:36 - DEBUG - Answer received: !yn\n",
      "25-05-18 03:59:36 - DEBUG - Command to send: c\n",
      "o52\n",
      "applyModifiableSettings\n",
      "ro42\n",
      "ro53\n",
      "e\n",
      "\n",
      "25/05/18 03:59:36 INFO  SharedState:60 Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/05/18 03:59:36 INFO  SharedState:60 Warehouse path is 'file:/opt/spark/warehouse'.\n",
      "25/05/18 03:59:37 WARN  SparkSession:72 Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25-05-18 03:59:37 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:37 - DEBUG - Command to send: c\n",
      "o42\n",
      "readStream\n",
      "e\n",
      "\n",
      "25-05-18 03:59:37 - DEBUG - Answer received: !yro54\n",
      "25-05-18 03:59:37 - DEBUG - Command to send: c\n",
      "o54\n",
      "format\n",
      "sjson\n",
      "e\n",
      "\n",
      "25-05-18 03:59:37 - DEBUG - Answer received: !yro55\n",
      "25-05-18 03:59:37 - DEBUG - Command to send: r\n",
      "u\n",
      "SparkSession\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:59:37 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession\n",
      "25-05-18 03:59:37 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.SparkSession\n",
      "getActiveSession\n",
      "e\n",
      "\n",
      "25-05-18 03:59:37 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:59:37 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.SparkSession\n",
      "getActiveSession\n",
      "e\n",
      "\n",
      "25-05-18 03:59:37 - DEBUG - Answer received: !yro56\n",
      "25-05-18 03:59:37 - DEBUG - Command to send: c\n",
      "o56\n",
      "isDefined\n",
      "e\n",
      "\n",
      "25-05-18 03:59:37 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:59:37 - DEBUG - Command to send: r\n",
      "u\n",
      "SparkSession\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:59:37 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession\n",
      "25-05-18 03:59:37 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.SparkSession\n",
      "getActiveSession\n",
      "e\n",
      "\n",
      "25-05-18 03:59:37 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:59:37 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.SparkSession\n",
      "getActiveSession\n",
      "e\n",
      "\n",
      "25-05-18 03:59:37 - DEBUG - Answer received: !yro57\n",
      "25-05-18 03:59:37 - DEBUG - Command to send: c\n",
      "o57\n",
      "get\n",
      "e\n",
      "\n",
      "25-05-18 03:59:37 - DEBUG - Answer received: !yro58\n",
      "25-05-18 03:59:37 - DEBUG - Command to send: r\n",
      "u\n",
      "SparkSession$\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:59:37 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession$\n",
      "25-05-18 03:59:37 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.SparkSession$\n",
      "MODULE$\n",
      "e\n",
      "\n",
      "25-05-18 03:59:37 - DEBUG - Answer received: !yro59\n",
      "25-05-18 03:59:37 - DEBUG - Command to send: i\n",
      "java.util.HashMap\n",
      "e\n",
      "\n",
      "25-05-18 03:59:37 - DEBUG - Answer received: !yao60\n",
      "25-05-18 03:59:37 - DEBUG - Command to send: c\n",
      "o59\n",
      "applyModifiableSettings\n",
      "ro58\n",
      "ro60\n",
      "e\n",
      "\n",
      "25-05-18 03:59:37 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:37 - DEBUG - Command to send: c\n",
      "o55\n",
      "schema\n",
      "s\\n        InvoiceNumber string,\\n        CreatedTime bigint,\\n        StoreID string,\\n        PosID string,\\n        CashierID string,\\n        CustomerType string,\\n        CustomerCardNo string,\\n        TotalAmount double,\\n        NumberOfItems bigint,\\n        PaymentMethod string,\\n        TaxableAmount double,\\n        CGST double,\\n        SGST double,\\n        CESS double,\\n        DeliveryType string,\\n        DeliveryAddress struct<\\n            AddressLine string,\\n            City string,\\n            ContactNumber string,\\n            PinCode string,\\n            State string\\n        >,\\n        InvoiceLineItems array<\\n            struct<\\n                ItemCode string,\\n                ItemDescription string,\\n                ItemPrice double,\\n                ItemQty bigint,\\n                TotalValue double\\n            >\\n        >\\n    \n",
      "e\n",
      "\n",
      "25-05-18 03:59:37 - DEBUG - Answer received: !yro61\n",
      "25-05-18 03:59:37 - DEBUG - Command to send: c\n",
      "o61\n",
      "option\n",
      "scleanSource\n",
      "sarchive\n",
      "e\n",
      "\n",
      "25-05-18 03:59:37 - DEBUG - Answer received: !yro62\n",
      "25-05-18 03:59:37 - DEBUG - Command to send: c\n",
      "o62\n",
      "option\n",
      "ssourceArchiveDir\n",
      "s/opt/spark/datasets/archive/invoices\n",
      "e\n",
      "\n",
      "25-05-18 03:59:37 - DEBUG - Answer received: !yro63\n",
      "25-05-18 03:59:37 - DEBUG - Command to send: c\n",
      "o63\n",
      "load\n",
      "s/opt/spark/datasets/invoices/*.json\n",
      "e\n",
      "\n",
      "25/05/18 03:59:37 INFO  InMemoryFileIndex:60 It took 51 ms to list leaf files for 4 paths.\n",
      "25-05-18 03:59:37 - DEBUG - Command to send: m\n",
      "d\n",
      "o53\n",
      "e\n",
      "\n",
      "25-05-18 03:59:37 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:37 - DEBUG - Command to send: m\n",
      "d\n",
      "o60\n",
      "e\n",
      "\n",
      "25-05-18 03:59:37 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:38 - DEBUG - Answer received: !yro64\n",
      "25-05-18 03:59:38 - DEBUG - Command to send: c\n",
      "o64\n",
      "writeStream\n",
      "e\n",
      "\n",
      "25-05-18 03:59:38 - DEBUG - Answer received: !yro65\n",
      "25-05-18 03:59:38 - DEBUG - Command to send: c\n",
      "o65\n",
      "queryName\n",
      "singestion_bronze\n",
      "e\n",
      "\n",
      "25-05-18 03:59:38 - DEBUG - Answer received: !yro66\n",
      "25-05-18 03:59:38 - DEBUG - Command to send: c\n",
      "o66\n",
      "format\n",
      "sdelta\n",
      "e\n",
      "\n",
      "25-05-18 03:59:38 - DEBUG - Answer received: !yro67\n",
      "25-05-18 03:59:38 - DEBUG - Command to send: c\n",
      "o67\n",
      "option\n",
      "scheckpointLocation\n",
      "s/opt/spark/datasets/checkpoint/invoices/bronze\n",
      "e\n",
      "\n",
      "25-05-18 03:59:38 - DEBUG - Answer received: !yro68\n",
      "25-05-18 03:59:38 - DEBUG - Command to send: c\n",
      "o68\n",
      "outputMode\n",
      "sappend\n",
      "e\n",
      "\n",
      "25-05-18 03:59:38 - DEBUG - Answer received: !yro69\n",
      "25-05-18 03:59:38 - DEBUG - Command to send: c\n",
      "o69\n",
      "toTable\n",
      "sinvoices_bronze\n",
      "e\n",
      "\n",
      "25/05/18 03:59:38 INFO  HiveUtils:60 Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.\n",
      "25/05/18 03:59:38 INFO  HiveClientImpl:60 Warehouse location for Hive client (version 2.3.9) is file:/opt/spark/warehouse\n",
      "25/05/18 03:59:38 WARN  HiveConf:4122 HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/05/18 03:59:38 WARN  HiveConf:4122 HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/05/18 03:59:38 INFO  HiveMetaStore:614 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore\n",
      "25/05/18 03:59:38 INFO  ObjectStore:403 ObjectStore, initialize called\n",
      "25/05/18 03:59:38 INFO  Persistence:77 Property hive.metastore.integral.jdo.pushdown unknown - will be ignored\n",
      "25/05/18 03:59:38 INFO  Persistence:77 Property datanucleus.cache.level2 unknown - will be ignored\n",
      "25/05/18 03:59:43 INFO  ObjectStore:526 Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\"\n",
      "25/05/18 03:59:44 INFO  MetaStoreDirectSql:146 Using direct SQL, underlying DB is DERBY\n",
      "25/05/18 03:59:44 INFO  ObjectStore:317 Initialized ObjectStore\n",
      "25/05/18 03:59:44 WARN  ObjectStore:7812 Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "25/05/18 03:59:44 WARN  ObjectStore:7900 setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.18.0.2\n",
      "25/05/18 03:59:44 INFO  HiveMetaStore:698 Added admin role in metastore\n",
      "25/05/18 03:59:44 INFO  HiveMetaStore:707 Added public role in metastore\n",
      "25/05/18 03:59:44 INFO  HiveMetaStore:747 No user is added in admin role, since config is empty\n",
      "25/05/18 03:59:44 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:59:44 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25/05/18 03:59:44 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:59:44 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25/05/18 03:59:44 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_bronze\n",
      "25/05/18 03:59:44 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_bronze\t\n",
      "25/05/18 03:59:44 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_bronze\n",
      "25/05/18 03:59:44 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_bronze\t\n",
      "25/05/18 03:59:44 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:59:44 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25/05/18 03:59:44 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_bronze\n",
      "25/05/18 03:59:44 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_bronze\t\n",
      "25/05/18 03:59:44 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_bronze\n",
      "25/05/18 03:59:44 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_bronze\t\n",
      "25/05/18 03:59:44 INFO  DelegatingLogStore:95 LogStore LogStoreAdapter(io.delta.storage.HDFSLogStore) is used for scheme file\n",
      "25/05/18 03:59:44 INFO  DeltaLog:95 Loading version 0.\n",
      "25/05/18 03:59:45 INFO  Snapshot:95 [tableId=84e0b1ac-52f0-4915-8252-274141ad51ec] Created snapshot Snapshot(path=file:/opt/spark/warehouse/invoices_bronze/_delta_log, version=0, metadata=Metadata(ee309d7e-debc-4830-b609-a41347a2dfcb,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"InvoiceNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CreatedTime\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"StoreID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PosID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CashierID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerCardNo\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalAmount\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"NumberOfItems\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PaymentMethod\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TaxableAmount\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CGST\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"SGST\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CESS\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryAddress\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"AddressLine\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ContactNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PinCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"State\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"InvoiceLineItems\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"ItemCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemDescription\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemPrice\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemQty\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalValue\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1747540365360)), logSegment=LogSegment(file:/opt/spark/warehouse/invoices_bronze/_delta_log,0,WrappedArray(DeprecatedRawLocalFileStatus{path=file:/opt/spark/warehouse/invoices_bronze/_delta_log/00000000000000000000.json; isDirectory=false; length=2897; replication=1; blocksize=33554432; modification_time=1747540365415; access_time=1747540377874; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}),org.apache.spark.sql.delta.EmptyCheckpointProvider$@7385fccf,1747540365415), checksumOpt=Some(VersionChecksum(Some(92fa4f54-1970-484a-8dd3-7c3664b26f95),0,0,None,None,1,1,None,Some(List()),Some(List()),Metadata(ee309d7e-debc-4830-b609-a41347a2dfcb,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"InvoiceNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CreatedTime\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"StoreID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PosID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CashierID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerCardNo\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalAmount\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"NumberOfItems\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PaymentMethod\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TaxableAmount\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CGST\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"SGST\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CESS\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryAddress\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"AddressLine\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ContactNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PinCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"State\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"InvoiceLineItems\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"ItemCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemDescription\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemPrice\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemQty\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalValue\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1747540365360)),Protocol(1,2),None,None,Some(List()))))\n",
      "25/05/18 03:59:45 INFO  DeltaLog:95 Updated snapshot to Snapshot(path=file:/opt/spark/warehouse/invoices_bronze/_delta_log, version=0, metadata=Metadata(ee309d7e-debc-4830-b609-a41347a2dfcb,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"InvoiceNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CreatedTime\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"StoreID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PosID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CashierID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerCardNo\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalAmount\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"NumberOfItems\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PaymentMethod\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TaxableAmount\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CGST\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"SGST\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CESS\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryAddress\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"AddressLine\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ContactNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PinCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"State\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"InvoiceLineItems\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"ItemCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemDescription\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemPrice\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemQty\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalValue\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1747540365360)), logSegment=LogSegment(file:/opt/spark/warehouse/invoices_bronze/_delta_log,0,WrappedArray(DeprecatedRawLocalFileStatus{path=file:/opt/spark/warehouse/invoices_bronze/_delta_log/00000000000000000000.json; isDirectory=false; length=2897; replication=1; blocksize=33554432; modification_time=1747540365415; access_time=1747540377874; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}),org.apache.spark.sql.delta.EmptyCheckpointProvider$@7385fccf,1747540365415), checksumOpt=Some(VersionChecksum(Some(92fa4f54-1970-484a-8dd3-7c3664b26f95),0,0,None,None,1,1,None,Some(List()),Some(List()),Metadata(ee309d7e-debc-4830-b609-a41347a2dfcb,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"InvoiceNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CreatedTime\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"StoreID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PosID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CashierID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerCardNo\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalAmount\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"NumberOfItems\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PaymentMethod\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TaxableAmount\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CGST\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"SGST\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CESS\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryAddress\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"AddressLine\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ContactNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PinCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"State\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"InvoiceLineItems\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"ItemCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemDescription\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemPrice\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemQty\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalValue\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1747540365360)),Protocol(1,2),None,None,Some(List()))))\n",
      "25/05/18 03:59:45 INFO  StateStoreCoordinatorRef:60 Registered StateStoreCoordinator endpoint\n",
      "25/05/18 03:59:45 INFO  ResolveWriteToStream:60 Checkpoint root /opt/spark/datasets/checkpoint/invoices/bronze resolved to file:/opt/spark/datasets/checkpoint/invoices/bronze.\n",
      "25/05/18 03:59:45 WARN  ResolveWriteToStream:72 spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/18 03:59:45 INFO  MicroBatchExecution:60 Starting ingestion_bronze [id = 83a64f49-3f9b-4e05-9cdb-ee5c08becb51, runId = 89ed4236-ded5-472c-ada0-7c4c27735ea4]. Use file:/opt/spark/datasets/checkpoint/invoices/bronze to store the query checkpoint.\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yro70\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: r\n",
      "u\n",
      "PythonUtils\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.api.python.PythonUtils\n",
      "toSeq\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: i\n",
      "java.util.ArrayList\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ylo71\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o71\n",
      "add\n",
      "sInvoiceNumber\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o71\n",
      "add\n",
      "sCreatedTime\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o71\n",
      "add\n",
      "sStoreID\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o71\n",
      "add\n",
      "sPosID\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o71\n",
      "add\n",
      "sCustomerType\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o71\n",
      "add\n",
      "sPaymentMethod\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o71\n",
      "add\n",
      "sDeliveryType\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o71\n",
      "add\n",
      "sDeliveryAddress.City\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o71\n",
      "add\n",
      "sDeliveryAddress.PinCode\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o71\n",
      "add\n",
      "sDeliveryAddress.State\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o71\n",
      "add\n",
      "sexplode(InvoiceLineItems) AS LineItem\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.api.python.PythonUtils\n",
      "toSeq\n",
      "ro71\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yro72\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o64\n",
      "selectExpr\n",
      "ro72\n",
      "e\n",
      "\n",
      "25/05/18 03:59:45 INFO  FileStreamSourceLog:60 Set the compact interval to 10 [defaultCompactInterval: 10]\n",
      "25/05/18 03:59:45 INFO  FileStreamSourceLog:60 BatchIds found from listing: 0\n",
      "25/05/18 03:59:45 INFO  FileStreamSourceLog:60 Getting latest batch 0\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yro73\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: r\n",
      "u\n",
      "functions\n",
      "rj\n",
      "e\n",
      "\n",
      "25/05/18 03:59:45 INFO  FileStreamSourceLog:60 BatchIds found from listing: 0\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ycorg.apache.spark.sql.functions\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.functions\n",
      "expr\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.functions\n",
      "expr\n",
      "sLineItem.ItemCode\n",
      "e\n",
      "\n",
      "25/05/18 03:59:45 INFO  FileStreamSource:60 maxFilesPerBatch = None, maxFileAgeMs = 604800000\n",
      "25/05/18 03:59:45 INFO  MicroBatchExecution:60 Using Source [FileStreamSource[file:/opt/spark/datasets/invoices/*.json]] from DataSourceV1 named 'FileSource[/opt/spark/datasets/invoices/*.json]' [DataSource(org.apache.spark.sql.SparkSession@1dbd508c,json,List(),Some(StructType(StructField(InvoiceNumber,StringType,true),StructField(CreatedTime,LongType,true),StructField(StoreID,StringType,true),StructField(PosID,StringType,true),StructField(CashierID,StringType,true),StructField(CustomerType,StringType,true),StructField(CustomerCardNo,StringType,true),StructField(TotalAmount,DoubleType,true),StructField(NumberOfItems,LongType,true),StructField(PaymentMethod,StringType,true),StructField(TaxableAmount,DoubleType,true),StructField(CGST,DoubleType,true),StructField(SGST,DoubleType,true),StructField(CESS,DoubleType,true),StructField(DeliveryType,StringType,true),StructField(DeliveryAddress,StructType(StructField(AddressLine,StringType,true),StructField(City,StringType,true),StructField(ContactNumber,StringType,true),StructField(PinCode,StringType,true),StructField(State,StringType,true)),true),StructField(InvoiceLineItems,ArrayType(StructType(StructField(ItemCode,StringType,true),StructField(ItemDescription,StringType,true),StructField(ItemPrice,DoubleType,true),StructField(ItemQty,LongType,true),StructField(TotalValue,DoubleType,true)),true),true))),List(),None,Map(cleanSource -> archive, sourceArchiveDir -> /opt/spark/datasets/archive/invoices, path -> /opt/spark/datasets/invoices/*.json),None)]\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yro74\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o73\n",
      "withColumn\n",
      "sItemCode\n",
      "ro74\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yro75\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: r\n",
      "u\n",
      "functions\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ycorg.apache.spark.sql.functions\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.functions\n",
      "expr\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.functions\n",
      "expr\n",
      "sLineItem.ItemDescription\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yro76\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o75\n",
      "withColumn\n",
      "sItemDescription\n",
      "ro76\n",
      "e\n",
      "\n",
      "25/05/18 03:59:45 INFO  OffsetSeqLog:60 BatchIds found from listing: 0\n",
      "25/05/18 03:59:45 INFO  OffsetSeqLog:60 Getting latest batch 0\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yro77\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: r\n",
      "u\n",
      "functions\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ycorg.apache.spark.sql.functions\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.functions\n",
      "expr\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.functions\n",
      "expr\n",
      "sLineItem.ItemPrice\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yro78\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o77\n",
      "withColumn\n",
      "sItemPrice\n",
      "ro78\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yro79\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: r\n",
      "u\n",
      "functions\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ycorg.apache.spark.sql.functions\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.functions\n",
      "expr\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.functions\n",
      "expr\n",
      "sLineItem.ItemQty\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yro80\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o79\n",
      "withColumn\n",
      "sItemQty\n",
      "ro80\n",
      "e\n",
      "\n",
      "25/05/18 03:59:45 INFO  OffsetSeqLog:60 BatchIds found from listing: 0\n",
      "25/05/18 03:59:45 INFO  OffsetSeqLog:60 Getting latest batch 0\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yro81\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: r\n",
      "u\n",
      "functions\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ycorg.apache.spark.sql.functions\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.functions\n",
      "expr\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.functions\n",
      "expr\n",
      "sLineItem.TotalValue\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yro82\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o81\n",
      "withColumn\n",
      "sTotalValue\n",
      "ro82\n",
      "e\n",
      "\n",
      "25/05/18 03:59:45 INFO  CommitLog:60 BatchIds found from listing: \n",
      "25/05/18 03:59:45 INFO  MicroBatchExecution:60 no commit log present\n",
      "25/05/18 03:59:45 INFO  MicroBatchExecution:60 Resuming at batch 0 with committed offsets {} and available offsets {FileStreamSource[file:/opt/spark/datasets/invoices/*.json]: {\"logOffset\":0}}\n",
      "25/05/18 03:59:45 INFO  MicroBatchExecution:60 Stream started from {}\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yro83\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: r\n",
      "u\n",
      "PythonUtils\n",
      "rj\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.api.python.PythonUtils\n",
      "toSeq\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ym\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: i\n",
      "java.util.ArrayList\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ylo84\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o84\n",
      "add\n",
      "sLineItem\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !ybtrue\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "z:org.apache.spark.api.python.PythonUtils\n",
      "toSeq\n",
      "ro84\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yro85\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o83\n",
      "drop\n",
      "ro85\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yro86\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o86\n",
      "writeStream\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yro87\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o87\n",
      "queryName\n",
      "singestion_silver\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yro88\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o88\n",
      "format\n",
      "sdelta\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o71\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yro89\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25/05/18 03:59:45 INFO  FileStreamSource:60 Processing 4 files from 0:0\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o89\n",
      "option\n",
      "scheckpointLocation\n",
      "s/opt/spark/datasets/checkpoint/invoices/silver\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o52\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yro90\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o90\n",
      "outputMode\n",
      "sappend\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o54\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yro91\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: c\n",
      "o91\n",
      "toTable\n",
      "sinvoices_silver\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o55\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25/05/18 03:59:45 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:59:45 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o56\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o57\n",
      "e\n",
      "\n",
      "25/05/18 03:59:45 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_silver\n",
      "25/05/18 03:59:45 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_silver\t\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o59\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o61\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o62\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o63\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o65\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o66\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o67\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o68\n",
      "e\n",
      "\n",
      "25/05/18 03:59:45 INFO  InMemoryFileIndex:60 It took 24 ms to list leaf files for 4 paths.\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o69\n",
      "e\n",
      "\n",
      "25/05/18 03:59:45 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_silver\n",
      "25/05/18 03:59:45 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_silver\t\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o72\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o74\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o75\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o76\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o77\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o78\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o79\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o80\n",
      "e\n",
      "\n",
      "25/05/18 03:59:45 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25/05/18 03:59:45 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25-05-18 03:59:45 - DEBUG - Command to send: m\n",
      "d\n",
      "o84\n",
      "e\n",
      "\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yv\n",
      "25/05/18 03:59:45 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_silver\n",
      "25/05/18 03:59:45 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_silver\t\n",
      "25/05/18 03:59:45 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_silver\n",
      "25/05/18 03:59:45 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_silver\t\n",
      "25/05/18 03:59:45 INFO  DelegatingLogStore:95 LogStore LogStoreAdapter(io.delta.storage.HDFSLogStore) is used for scheme file\n",
      "25/05/18 03:59:45 INFO  DeltaLog:95 Loading version 0.\n",
      "25/05/18 03:59:45 INFO  Snapshot:95 [tableId=a5afdaa4-1208-4b26-bef1-b82949904e9e] Created snapshot Snapshot(path=file:/opt/spark/warehouse/invoices_silver/_delta_log, version=0, metadata=Metadata(32fdf004-f6a4-43c3-9bb1-76c5a254b3a1,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"InvoiceNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CreatedTime\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"StoreID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PosID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PaymentMethod\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PinCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"State\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemDescription\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemPrice\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemQty\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalValue\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1747540369694)), logSegment=LogSegment(file:/opt/spark/warehouse/invoices_silver/_delta_log,0,WrappedArray(DeprecatedRawLocalFileStatus{path=file:/opt/spark/warehouse/invoices_silver/_delta_log/00000000000000000000.json; isDirectory=false; length=1825; replication=1; blocksize=33554432; modification_time=1747540369347; access_time=1747540377895; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}),org.apache.spark.sql.delta.EmptyCheckpointProvider$@7385fccf,1747540369347), checksumOpt=Some(VersionChecksum(Some(b2a1dd9c-2799-485c-8e22-c703b61e4015),0,0,None,None,1,1,None,Some(List()),Some(List()),Metadata(32fdf004-f6a4-43c3-9bb1-76c5a254b3a1,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"InvoiceNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CreatedTime\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"StoreID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PosID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PaymentMethod\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PinCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"State\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemDescription\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemPrice\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemQty\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalValue\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1747540369694)),Protocol(1,2),None,None,Some(List()))))\n",
      "25/05/18 03:59:45 INFO  DeltaLog:95 Updated snapshot to Snapshot(path=file:/opt/spark/warehouse/invoices_silver/_delta_log, version=0, metadata=Metadata(32fdf004-f6a4-43c3-9bb1-76c5a254b3a1,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"InvoiceNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CreatedTime\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"StoreID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PosID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PaymentMethod\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PinCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"State\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemDescription\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemPrice\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemQty\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalValue\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1747540369694)), logSegment=LogSegment(file:/opt/spark/warehouse/invoices_silver/_delta_log,0,WrappedArray(DeprecatedRawLocalFileStatus{path=file:/opt/spark/warehouse/invoices_silver/_delta_log/00000000000000000000.json; isDirectory=false; length=1825; replication=1; blocksize=33554432; modification_time=1747540369347; access_time=1747540377895; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}),org.apache.spark.sql.delta.EmptyCheckpointProvider$@7385fccf,1747540369347), checksumOpt=Some(VersionChecksum(Some(b2a1dd9c-2799-485c-8e22-c703b61e4015),0,0,None,None,1,1,None,Some(List()),Some(List()),Metadata(32fdf004-f6a4-43c3-9bb1-76c5a254b3a1,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"InvoiceNumber\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CreatedTime\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"StoreID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PosID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"CustomerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PaymentMethod\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"DeliveryType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"City\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"PinCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"State\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemCode\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemDescription\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemPrice\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ItemQty\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"TotalValue\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1747540369694)),Protocol(1,2),None,None,Some(List()))))\n",
      "25/05/18 03:59:45 INFO  ResolveWriteToStream:60 Checkpoint root /opt/spark/datasets/checkpoint/invoices/silver resolved to file:/opt/spark/datasets/checkpoint/invoices/silver.\n",
      "25/05/18 03:59:45 WARN  ResolveWriteToStream:72 spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/18 03:59:45 INFO  MicroBatchExecution:60 Starting ingestion_silver [id = 19d65685-e892-4d3b-bba8-696452481d47, runId = d8c6e71a-236f-42e0-91f9-49479f33b73d]. Use file:/opt/spark/datasets/checkpoint/invoices/silver to store the query checkpoint.\n",
      "25-05-18 03:59:45 - DEBUG - Answer received: !yro92\n",
      "25/05/18 03:59:45 INFO  FileStreamSourceLog:60 Set the compact interval to 10 [defaultCompactInterval: 10]\n",
      "25/05/18 03:59:45 INFO  FileStreamSourceLog:60 BatchIds found from listing: 0\n",
      "25/05/18 03:59:45 INFO  FileStreamSourceLog:60 Getting latest batch 0\n",
      "25/05/18 03:59:45 INFO  FileStreamSourceLog:60 BatchIds found from listing: 0\n",
      "25/05/18 03:59:45 INFO  FileStreamSource:60 maxFilesPerBatch = None, maxFileAgeMs = 604800000\n",
      "25/05/18 03:59:46 INFO  MicroBatchExecution:60 Using Source [FileStreamSource[file:/opt/spark/datasets/invoices/*.json]] from DataSourceV1 named 'FileSource[/opt/spark/datasets/invoices/*.json]' [DataSource(org.apache.spark.sql.SparkSession@1dbd508c,json,List(),Some(StructType(StructField(InvoiceNumber,StringType,true),StructField(CreatedTime,LongType,true),StructField(StoreID,StringType,true),StructField(PosID,StringType,true),StructField(CashierID,StringType,true),StructField(CustomerType,StringType,true),StructField(CustomerCardNo,StringType,true),StructField(TotalAmount,DoubleType,true),StructField(NumberOfItems,LongType,true),StructField(PaymentMethod,StringType,true),StructField(TaxableAmount,DoubleType,true),StructField(CGST,DoubleType,true),StructField(SGST,DoubleType,true),StructField(CESS,DoubleType,true),StructField(DeliveryType,StringType,true),StructField(DeliveryAddress,StructType(StructField(AddressLine,StringType,true),StructField(City,StringType,true),StructField(ContactNumber,StringType,true),StructField(PinCode,StringType,true),StructField(State,StringType,true)),true),StructField(InvoiceLineItems,ArrayType(StructType(StructField(ItemCode,StringType,true),StructField(ItemDescription,StringType,true),StructField(ItemPrice,DoubleType,true),StructField(ItemQty,LongType,true),StructField(TotalValue,DoubleType,true)),true),true))),List(),None,Map(cleanSource -> archive, sourceArchiveDir -> /opt/spark/datasets/archive/invoices, path -> /opt/spark/datasets/invoices/*.json),None)]\n",
      "25/05/18 03:59:46 INFO  OffsetSeqLog:60 BatchIds found from listing: 0\n",
      "25/05/18 03:59:46 INFO  OffsetSeqLog:60 Getting latest batch 0\n",
      "25/05/18 03:59:46 INFO  OffsetSeqLog:60 BatchIds found from listing: 0\n",
      "25/05/18 03:59:46 INFO  OffsetSeqLog:60 Getting latest batch 0\n",
      "25/05/18 03:59:46 INFO  FileSourceStrategy:60 Pushed Filters: \n",
      "25/05/18 03:59:46 INFO  FileSourceStrategy:60 Post-Scan Filters: \n",
      "25/05/18 03:59:46 INFO  CommitLog:60 BatchIds found from listing: \n",
      "25/05/18 03:59:46 INFO  MicroBatchExecution:60 no commit log present\n",
      "25/05/18 03:59:46 INFO  MicroBatchExecution:60 Resuming at batch 0 with committed offsets {} and available offsets {FileStreamSource[file:/opt/spark/datasets/invoices/*.json]: {\"logOffset\":0}}\n",
      "25/05/18 03:59:46 INFO  MicroBatchExecution:60 Stream started from {}\n",
      "25/05/18 03:59:46 INFO  FileStreamSource:60 Processing 4 files from 0:0\n",
      "25/05/18 03:59:46 INFO  InMemoryFileIndex:60 It took 21 ms to list leaf files for 4 paths.\n",
      "25/05/18 03:59:46 INFO  FileSourceStrategy:60 Pushed Filters: \n",
      "25/05/18 03:59:46 INFO  FileSourceStrategy:60 Post-Scan Filters: \n",
      "25/05/18 03:59:46 INFO  FileSourceStrategy:60 Pushed Filters: IsNotNull(InvoiceLineItems)\n",
      "25/05/18 03:59:46 INFO  FileSourceStrategy:60 Post-Scan Filters: (size(InvoiceLineItems#229, true) > 0),isnotnull(InvoiceLineItems#229)\n",
      "25/05/18 03:59:46 INFO  Snapshot:95 DELTA: Compute snapshot for version: 0\n",
      "25/05/18 03:59:46 INFO  FileSourceStrategy:60 Pushed Filters: IsNotNull(InvoiceLineItems)\n",
      "25/05/18 03:59:46 INFO  FileSourceStrategy:60 Post-Scan Filters: (size(InvoiceLineItems#229, true) > 0),isnotnull(InvoiceLineItems#229)\n",
      "25/05/18 03:59:46 INFO  MemoryStore:60 Block broadcast_0 stored as values in memory (estimated size 205.1 KiB, free 434.2 MiB)\n",
      "25/05/18 03:59:46 INFO  Snapshot:95 DELTA: Compute snapshot for version: 0\n",
      "25/05/18 03:59:46 INFO  MemoryStore:60 Block broadcast_1 stored as values in memory (estimated size 205.1 KiB, free 434.0 MiB)\n",
      "25/05/18 03:59:46 INFO  MemoryStore:60 Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 433.9 MiB)\n",
      "25/05/18 03:59:46 INFO  MemoryStore:60 Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 433.9 MiB)\n",
      "25/05/18 03:59:46 INFO  BlockManagerInfo:60 Added broadcast_0_piece0 in memory on 7db080dfe9d2:33277 (size: 35.7 KiB, free: 434.4 MiB)\n",
      "25/05/18 03:59:46 INFO  BlockManagerInfo:60 Added broadcast_1_piece0 in memory on 7db080dfe9d2:33277 (size: 35.7 KiB, free: 434.3 MiB)\n",
      "25/05/18 03:59:46 INFO  SparkContext:60 Created broadcast 1 from toTable at NativeMethodAccessorImpl.java:0\n",
      "25/05/18 03:59:46 INFO  SparkContext:60 Created broadcast 0 from toTable at NativeMethodAccessorImpl.java:0\n",
      "25/05/18 03:59:47 INFO  DeltaLogFileIndex:95 Created DeltaLogFileIndex(JSON, numFilesInSegment: 1, totalFileSize: 1825)\n",
      "25/05/18 03:59:47 INFO  DeltaLogFileIndex:95 Created DeltaLogFileIndex(JSON, numFilesInSegment: 1, totalFileSize: 2897)\n",
      "25/05/18 03:59:48 INFO  DataSourceStrategy:60 Pruning directories with: \n",
      "25/05/18 03:59:48 INFO  DataSourceStrategy:60 Pruning directories with: \n",
      "25/05/18 03:59:48 INFO  FileSourceStrategy:60 Pushed Filters: \n",
      "25/05/18 03:59:48 INFO  FileSourceStrategy:60 Pushed Filters: \n",
      "25/05/18 03:59:48 INFO  FileSourceStrategy:60 Post-Scan Filters: \n",
      "25/05/18 03:59:48 INFO  FileSourceStrategy:60 Post-Scan Filters: \n",
      "25/05/18 03:59:48 WARN  SparkStringUtils:72 Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/05/18 03:59:48 INFO  CodeGenerator:60 Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.serializefromobject_doConsume_0$ is 25072 bytes\n",
      "25/05/18 03:59:48 INFO  CodeGenerator:60 Code generated in 545.873392 ms\n",
      "25/05/18 03:59:49 INFO  CodeGenerator:60 Code generated in 89.351648 ms\n",
      "25/05/18 03:59:49 INFO  CodeGenerator:60 Code generated in 56.574151 ms\n",
      "25/05/18 03:59:49 INFO  MemoryStore:60 Block broadcast_2 stored as values in memory (estimated size 205.4 KiB, free 433.5 MiB)\n",
      "25/05/18 03:59:49 INFO  MemoryStore:60 Block broadcast_3 stored as values in memory (estimated size 205.4 KiB, free 433.5 MiB)\n",
      "25/05/18 03:59:49 INFO  MemoryStore:60 Block broadcast_2_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 433.5 MiB)\n",
      "25/05/18 03:59:49 INFO  MemoryStore:60 Block broadcast_3_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 433.5 MiB)\n",
      "25/05/18 03:59:49 INFO  BlockManagerInfo:60 Added broadcast_2_piece0 in memory on 7db080dfe9d2:33277 (size: 35.7 KiB, free: 434.3 MiB)\n",
      "25/05/18 03:59:49 INFO  SparkContext:60 Created broadcast 2 from toTable at NativeMethodAccessorImpl.java:0\n",
      "25/05/18 03:59:49 INFO  BlockManagerInfo:60 Added broadcast_3_piece0 in memory on 7db080dfe9d2:33277 (size: 35.7 KiB, free: 434.3 MiB)\n",
      "25/05/18 03:59:49 INFO  SparkContext:60 Created broadcast 3 from toTable at NativeMethodAccessorImpl.java:0\n",
      "25/05/18 03:59:49 INFO  FileSourceScanExec:60 Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/05/18 03:59:49 INFO  FileSourceScanExec:60 Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/05/18 03:59:49 INFO  CodeGenerator:60 Code generated in 62.772389 ms\n",
      "25/05/18 03:59:49 INFO  SparkContext:60 Starting job: toTable at NativeMethodAccessorImpl.java:0\n",
      "25/05/18 03:59:49 INFO  SparkContext:60 Starting job: toTable at NativeMethodAccessorImpl.java:0\n",
      "25/05/18 03:59:49 INFO  DAGScheduler:60 Registering RDD 7 (toTable at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "25/05/18 03:59:49 INFO  DAGScheduler:60 Registering RDD 26 (toTable at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
      "25/05/18 03:59:49 INFO  DAGScheduler:60 Got job 1 (toTable at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/18 03:59:49 INFO  DAGScheduler:60 Final stage: ResultStage 2 (toTable at NativeMethodAccessorImpl.java:0)\n",
      "25/05/18 03:59:49 INFO  DAGScheduler:60 Parents of final stage: List(ShuffleMapStage 1)\n",
      "25/05/18 03:59:49 INFO  DAGScheduler:60 Missing parents: List(ShuffleMapStage 1)\n",
      "25/05/18 03:59:49 INFO  DAGScheduler:60 Submitting ShuffleMapStage 0 (MapPartitionsRDD[7] at toTable at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/18 03:59:49 INFO  MemoryStore:60 Block broadcast_4 stored as values in memory (estimated size 106.0 KiB, free 433.4 MiB)\n",
      "25/05/18 03:59:49 INFO  MemoryStore:60 Block broadcast_4_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 433.3 MiB)\n",
      "25/05/18 03:59:49 INFO  BlockManagerInfo:60 Added broadcast_4_piece0 in memory on 7db080dfe9d2:33277 (size: 32.7 KiB, free: 434.2 MiB)\n",
      "25/05/18 03:59:49 INFO  SparkContext:60 Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/18 03:59:49 INFO  DAGScheduler:60 Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[7] at toTable at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/18 03:59:49 INFO  TaskSchedulerImpl:60 Adding task set 0.0 with 1 tasks resource profile 0\n",
      "25/05/18 03:59:49 INFO  DAGScheduler:60 Registering RDD 6 (toTable at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "25/05/18 03:59:49 INFO  DAGScheduler:60 Registering RDD 27 (toTable at NativeMethodAccessorImpl.java:0) as input to shuffle 3\n",
      "25/05/18 03:59:49 INFO  DAGScheduler:60 Got job 0 (toTable at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/18 03:59:49 INFO  DAGScheduler:60 Final stage: ResultStage 5 (toTable at NativeMethodAccessorImpl.java:0)\n",
      "25/05/18 03:59:49 INFO  DAGScheduler:60 Parents of final stage: List(ShuffleMapStage 4)\n",
      "25/05/18 03:59:49 INFO  DAGScheduler:60 Missing parents: List(ShuffleMapStage 4)\n",
      "25/05/18 03:59:49 INFO  DAGScheduler:60 Submitting ShuffleMapStage 3 (MapPartitionsRDD[6] at toTable at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/18 03:59:49 INFO  MemoryStore:60 Block broadcast_5 stored as values in memory (estimated size 106.0 KiB, free 433.2 MiB)\n",
      "25/05/18 03:59:49 INFO  MemoryStore:60 Block broadcast_5_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 433.2 MiB)\n",
      "25/05/18 03:59:49 INFO  BlockManagerInfo:60 Added broadcast_5_piece0 in memory on 7db080dfe9d2:33277 (size: 32.7 KiB, free: 434.2 MiB)\n",
      "25/05/18 03:59:49 INFO  SparkContext:60 Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/18 03:59:49 INFO  DAGScheduler:60 Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[6] at toTable at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/18 03:59:49 INFO  TaskSchedulerImpl:60 Adding task set 3.0 with 1 tasks resource profile 0\n",
      "25/05/18 04:00:04 WARN  TaskSchedulerImpl:72 Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/05/18 04:00:19 WARN  TaskSchedulerImpl:72 Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    table_name = \"invoice_line_items\"\n",
    "    schema = \"\"\"\n",
    "        InvoiceNumber string,\n",
    "        CreatedTime bigint,\n",
    "        StoreID string,\n",
    "        PosID string,\n",
    "        CashierID string,\n",
    "        CustomerType string,\n",
    "        CustomerCardNo string,\n",
    "        TotalAmount double,\n",
    "        NumberOfItems bigint,\n",
    "        PaymentMethod string,\n",
    "        TaxableAmount double,\n",
    "        CGST double,\n",
    "        SGST double,\n",
    "        CESS double,\n",
    "        DeliveryType string,\n",
    "        DeliveryAddress struct<\n",
    "            AddressLine string,\n",
    "            City string,\n",
    "            ContactNumber string,\n",
    "            PinCode string,\n",
    "            State string\n",
    "        >,\n",
    "        InvoiceLineItems array<\n",
    "            struct<\n",
    "                ItemCode string,\n",
    "                ItemDescription string,\n",
    "                ItemPrice double,\n",
    "                ItemQty bigint,\n",
    "                TotalValue double\n",
    "            >\n",
    "        >\n",
    "    \"\"\"\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"InvoicesStreamMedallion\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    invoices_bronze = InvoiceStreamBronze(spark)\n",
    "    invoices_df = invoices_bronze.read_invoices(\n",
    "        format=\"json\",\n",
    "        path=\"/opt/spark/datasets/invoices/*.json\",\n",
    "        schema=schema,\n",
    "        clean_source=\"archive\",\n",
    "        archive_dir=\"/opt/spark/datasets/archive/invoices\"\n",
    "    )\n",
    "    squery_bronze = invoices_bronze.write_invoices(\n",
    "        df=invoices_df,\n",
    "        format=\"delta\",\n",
    "        checkpoint_location=\"/opt/spark/datasets/checkpoint/invoices/bronze\",\n",
    "        output_mode=\"append\",\n",
    "        table=\"invoices_bronze\",\n",
    "        query_name=\"ingestion_bronze\"\n",
    "    )\n",
    "\n",
    "    invoices_silver = InvoiceStreamSilver(spark)\n",
    "    exploded_df = invoices_silver.explode_invoices(invoices_df)\n",
    "    flatten_df = invoices_silver.flatten_invoices(exploded_df)\n",
    "    squery_silver = invoices_silver.write_invoices(\n",
    "        df=flatten_df,\n",
    "        format=\"delta\",\n",
    "        checkpoint_location=\"/opt/spark/datasets/checkpoint/invoices/silver\",\n",
    "        output_mode=\"append\",\n",
    "        table=\"invoices_silver\",\n",
    "        query_name=\"ingestion_silver\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c34c4-6016-442d-8a97-114e9604e273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-05-18 03:56:13 - DEBUG - Command to send: c\n",
      "o93\n",
      "read\n",
      "e\n",
      "\n",
      "25-05-18 03:56:13 - DEBUG - Answer received: !yro141\n",
      "25-05-18 03:56:13 - DEBUG - Command to send: c\n",
      "o141\n",
      "table\n",
      "sinvoices_bronze\n",
      "e\n",
      "\n",
      "25/05/18 03:56:13 INFO  HiveMetaStore:781 0: get_database: default\n",
      "25/05/18 03:56:13 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "25/05/18 03:56:13 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_bronze\n",
      "25/05/18 03:56:13 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_bronze\t\n",
      "25/05/18 03:56:13 INFO  HiveMetaStore:781 0: get_table : db=default tbl=invoices_bronze\n",
      "25/05/18 03:56:13 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=invoices_bronze\t\n",
      "25-05-18 03:56:13 - DEBUG - Answer received: !yro142\n",
      "25-05-18 03:56:13 - DEBUG - Command to send: c\n",
      "o142\n",
      "showString\n",
      "i20\n",
      "i20\n",
      "bFalse\n",
      "e\n",
      "\n",
      "25/05/18 03:56:13 INFO  PrepareDeltaScan:95 DELTA: Filtering files for query\n",
      "25/05/18 03:56:21 WARN  TaskSchedulerImpl:72 Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/05/18 03:56:36 WARN  TaskSchedulerImpl:72 Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/05/18 03:56:51 WARN  TaskSchedulerImpl:72 Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "[Stage 0:>                  (0 + 0) / 1][Stage 3:>                  (0 + 0) / 1]"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"invoices_bronze\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f75878-ab74-4106-9acc-94f784df2de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
